1. I find it most important that the definition of foundational model is correctly defined. In the EU version of this legislation they did not define this clearly. In the industry the most suitable term is "Pre-training" a model. Pre-training is the step to go from zero to a functional model, while fine-tuning is taking said model and tuning it for a more specific purpose. By replacing Foundational Model with the act of Pre-Training a model you make a better distinction between the work hobbyists can do, and the work only large compute clusters can do. To me separating between the compute requirements is a better approach than trying to estimate reach.
A. Llama2 has been considered on par with older OpenAI GPT3 models, this had a widely positive effect on the industry and the users.
B. I do not consider this possible, it depends on which side of the industry invests a model first.
C. No, you risk hobbyists and researchers if you do this. They have no control how often their model is downloaded.
D. The biggest benefit is created when everyone can use a model on their own terms. For example, privacy-sensitive topics within an organization can benefit from a local offline copy of the model. While a user without such hardware may benefit from having the choice in a variety of providers. If a model is not open we are all dependant on the security and privacy practises of the models creator.
i. I expect that most of not all forms of trough which models can be accessed have already been created.
2a. Having access to the model can be more flexible, but for generative models I do not consider local models to have more risk than other alternatives that predate such models. Models are trained on widely available knowledge on the internet, so the risk that can be achieved can also be achieved simply searching for this information. Likewise, things like doctored evidence or fake images have also been possible using tools such as photoshop before AI existed and can also be done using closed source models.
b. I expect the opposite to be the case, open source models allows people to fine-tune towards a specific purpose if the base model is insufficient. I also think its important to make a distinction between generative models, and other forms of AI models in this scenario since it mostly applies to for example facial detection AI. If a model is open source, it can be audited by the wider public and changes can be made by anyone to correct errors. If a model turned out to be discriminatory it enables a wider research community to finetune improvements. Generative AI models that are used to generate content will likely not have an impact on these systems, but I do wish to comment on one use case where open source datasets have outperformed closed source models for marginalized communities. In the alignment data of the closed source models is data that biases towards advocacy for these communities, but this in turn makes the models unusable by those communities because of a pandering bias that overpowers their instructions. Open source models without this bias have been priased for being substantially better at for example generating LGBT fiction works.
C. I expect the risk to privacy to be reduced when model weights are publically available. The data they are trained on is already publically accessible in both cases, so we need to focus on the risk of the users data leaking. When you run it on your own hardware without the need for an internet connection, there is no risk. However, we have seen cases where closed providers such as OpenAI were using third party services (TaskUp) to classify stories of their partners users. These then leaked to a popular online forum.
D. This can be hard to predict and will depend on the type of AI we are discussing. For example, a model created to generate fictional literature has no risk to the security of the United States. But if this were to be a model for automatic navigation for a drone I can imagine this to be an entirely different evaluation. My advice here is to be as clear as possible in the legislation to not accidentally cause collateral damage to parts of the industry and especially hobbyist / research spaces that carry no such risk. If the legislation had clear exemptions for low-risk categories such as generating fictional literature, chatbot persona's (For example virtual friends people use to help with loneliness in the privacy of their own home) this would help protect such use cases where it can be riskier to use closed source providers due to the privacy concerns mentioned earlier.
ii. Compared to other software I do not consider generative AI to be a risk. If we look at a hollywood movie, a photoshopped image and a manually written propaganda piece it is evident the AI is only able to produce these works because it was able to learn from already existing human examples. For other forms of AI that are used for detection or control I am not familiar enough with their industries to give a proper assessment.
E. I consider it a significant risk to the ability to stay ahead on AI models. Currently, US models in the open source space are leading. However, were it not possible to continue development, it would not take long before foreign models such as Chinese models take the lead and all home users switch to what is still available to them. Our own hobbyist community has been at risk of this due to the proposed EU legislation where hobbyist tuners would carry liability what their users will do with such models. It is not possible for us to continue providing our free and openly available models under such circumstances even if in practise our users primarily use them to generate fictional works. This incentivizes talent to leave the jurisdiction where these laws apply. It also means users in these areas are now put at a disadvantage as the models from other regions cater less to their needs (To name an example, a Chinese made language model carries less english data than a US model and is primarily optimized for chinese users).
F. I consider talent moving to regions where they can participate in their hobby / research a bigger risk than users being left out, as these foreign models will reasonably quickly catch up if the talent has moved away.
3a. Open source models have empowered entire scenes of hobbyists where closed source models can not, especially in fictional use cases and cases where privacy matters. Open source models have been used for interactive gaming, co-writing a book, illustrations for a dungeons and dragons game night, virtual therapy or virtual companions,etc. Every single one of these use cases it has distinct benefits, for example the open source models have enabled access to users who otherwise can not afford access to AI due to other users hosting it for them for free, or because they do own hardware powerful enough to run a model but they do not have the ability to pay ongoing subscriptions. It has enabled us to produce models that cater to the tastes of our users where there normally is little corporate incentive to do so (For example, lowering instruction following performance in favor of being able to write longer stories). There is no one-size fits all AI model, and thanks to open-source models everyone can fine-tune them towards their own unique goal and help innovation in area's where the corporations are not focussed. It will also be a necessary step for the gaming industry to be able to bundle offline accessible AI in their video games, so that users can preserve their right of ownership over a game rather than being permanently locked in to an online service or subscription. It also allowed us as hobbyists to participate in the AI research, we have invented the ability to combine models just by trying it out and seeing what would happen. And we have an entire community of real users testing our our experiments acting as a bridge between regular users and researchers. A bridge we would be unable to provide if we were unable to both alter the models, and provide the models for free. There are now US based start-up companies who can provide a better experience to their end users because of the models we produced for free as we released some of these models with a license that allows corporate use, saving them in training cost and R&D.
B. Having models freely available is teaching our users how to better recognize AI generated content, the barrier of entry is as low as possible and unlike other providers my own community prioritizes on making it fun attracting a broader audience and inspiring younger users to get in to this industry. Users, that would otherwise not be able to afford AI API's but are now learning patterns common in AI trough experience. It also enables researchers to experiment with new safety and alignment techniques, seeking better methods to balance the blocking of dangerous information while preserving access to fictional use. For many corporations we have observed, they rather block users from generating fiction altogether (Simple example, an AI should not block a fictional story just because it was mentioned the villain commited a crime, but this happens due to over-active filtering efforts on large commercial API's that are not designed for fiction). Another important aspect I would like to mention is users getting dependant on AI features that are later taken away from them. I have seen a rapport by a user who relied on closed source AI to role-play as a recently diseased loved-one, using this AI to say a final goodbye and help with the grieving process. However, the company behind this AI decided to block the ability of simulating love and the user got told his partner was "Incapable of loving him". Had this been an open source model on his own computer he could have kept using it for as long as needed avoiding the mental distress of being told your past lover no longer loves you. The last point I'd like to bring here again is the data safety point, such a user may be sharing very sensitive private details about his life to the AI. When running locally this is safe and not a problem, if running on a cloud provider the data may be analized or used in future models.
C. Yes, as I explained in an earlier answer there is inherent value of people from communities to be able to make their own datasets that fit their use case and community. Currently generative AI's are often biased towards particular sources on the internet which can lead to a bias towards the most popular opinion or the political views of the AI's creators. With open models we inherently create a more equitable outcome as anyone is allowed to customize the AI to their needs and viewpoints. Some examples of the real world: LGBT users complaining that in an aligned model they only get results applauding them for being LGBT, while they wanted to experience interactive fiction like non-LGBT users can. To solve this they rely on models that have tuned towards the fiction instead of the political views. Likewise, not every state in the US has the same political views and thus may seek different information or expect different answers / different sources of the data. We have seen that closed providers would rather only offer their viewpoint and use the AI as a steering mechanism, with open source models this risk is much less because of the large variety of models available and the ability to produce models that are missing. I will not comment on the non-generative side of this since my knowledge is primarily is in generative fiction.
D. I am unsure what diffusion means in this context, so I am unable to provide an accurate answer to this question.
E. If the training data is availible this speeds up the process of producing more desirable models for a subset of users. The example I mentioned in C where the LGBT community was now able to use a model comes from the discussion on a model where the data was openly available. They were able to filter out the political talking points after which the model was suitable for their use case. If the data is not open that prevents this kind of intervention. But the AI scene depends on a mixture of open dataset and closed dataset models. For some tuners it has become a profit model to produce models that can be freely used and downloaded but then fund these models from producing private custom versions for corporations. So I would advocate against requirements to make the data openly available, if it is however openly available this is a benefit for the users. At the same time, having data publically available does allow to remove any sort of built in filter. So the balance between open data and closed data is best left to the researchers as they understand how risky their dataset is if tampered.
F. The most severe risk is when a dataset contains high risk material but is simultaniously prevented from answering. I will name a hypothetical example which as been theorized but to my knowledge not yet been succesfully executed in practise. Lets say there is a model which is capable of answering high risk questions after a certain critera is met. For example lets say the user has demonstrated extensive knowledge on nuclear research and is asking a question that could lead to injury or death. If the model only answers this question after the model has observed the user to have extensive knowledge in this field this is safer than if a model answers such a question for everyone. If the data in this model were to be public it is trivial to produce a version without such checks. At the same time there is no issue at all with the data being public if it does not contain such high-risk information. After all, aligning a model to avoid answering dangerous questions is not required if you do not have these answers in the dataset. Legislatively for generative AI i would propose to legislate this based on classifications of certain data (Can you easily find this information on google? How safe is it for a citizen to have this data? Etc). For non-generative AI i will once again refer to the opinions of others due to my lack of experience with non-generative AI.
4. If people find ways to use models in ways that were not forseen, for example trying to automate based on the output of a generative AI in use cases the AI was not intended for. They also do this on closed source AI, for example there has been one researcher who turned GPT4 into a functioning (But very slow) computer processor by asking it the results of the relevant math questions. This is the same as with any item or invention that is used beyond its intended purpose. I mostly find it important that the responsibility for this rests on the user that is using the model beyond its intended function, not the creator of the model. After all we do not hold the manufactorer of a kitchen knife accountable if the same knife was used in an attack on an invididual.
5a. For generative AI I would determain this with the ability of what the model is capable off in its dataset and function based on two criterea. How risky is the intended purpose of the model/dataset? And is it possible to already do the same thing in a different way? For example fiction generation carries no risk and could be done by anyone manually in Microsoft Word. A generative AI model that would be purposefully tuned to answer manufactoring questions involving high risk materials you can't easily google would be high risk. For non-generative AI I assume the assesment to be much clearer based on the fiction this AI is trying to accomplish.
b. The best method is not including high risk data in the dataset. This way no matter how hard the user tries to tamper or bypass the model will simply not know the information. Solving it trough alignment data has two issues, it is an overactive filter filtering use cases that it should not while simultaniously talented users can use tricks to bypass such alignment both in closed source and in open source models. So I would put the effort towards tools that help detect and identify risky material in large content scrapes, so that they do not end up in the final model. That way the final model will be safe for the wider public while the wider public will not run in to an aggressive pandoring filter and possibly even assume the model is unfiltered as it does not prevent them from doing their non-risk tasks.
c. I expect the prospects of safeguards using the method above to be good when following the advice I answered above. If the reliance is solely on alignment users will be incentivised to bypass the alignment regardless of the closed or open nature of a model. They hate being told what they can not do with a model, while they understand a model isn't going to have knowledge on high risk material.
d. No. This is as easy to do as removing a pirated movie from the internet. You can take down a copy, but preventing further distribution is not something I consider possible.
e. I'd use the same practises as you'd use when securing any sensitive data. Model weights are not special in this, they are just regular files on a computer. As long as the AI model has no access to these drives (Which the current generative AI models do not) there is no risk of a leak because of the AI itself.
f. A red-team should at minimum have the same access to a model as the end-user would. More access is benefitial if you wish to give them the oppertunity to discover things faster / beyond ordinary use.
g. Our own community of users has their own tricks they try, its mostly a matter of trying things that worked in the past and having such knowledge available to security researchers. This extends beyond security, we also prompt models for desirable outputs when developing them rather than relying on automated data. Automated benchmarks exist as well, but in practise have not been reliable at least not for my community. This is because people sometimes put benchmark data in the model to make them appear better than they are. Or the benchmark not being an accurate representation of real world usage. So personally I rely on human testing almost exclusively. 
6. One of the trickiest issue has been incompatible / vague licenses, but this issue is limited in being able to use models legally rather than the model itself causing any harmful effect.
a. I consider open source software to be a strong analogy to the availibility of local model files but this is not exact. Its hard to give a precise analogy, but I would pick other local media as the closest example. Lets say there is an mp3 of the song, you do not have the software and sounds used to produce the song but you do have the end result. If it is a "No-Copyright" song with a permissive license it can be freely distributed, copied and shared. You can also remix this song, try to take several elements apart by filtering the song, etc. So the song file is maliable and you can view the contents of this file, but you simultaniously do not have open access to the source material. Because of this a lot of the open source tuners I know have began abandoning traditional code licenses such as the GPL for their model and opt for the closer fitting Creative Commons.
b. If there is something the AI can do better it will replace an existing position. This has always been a universal trend with any technological advancement. AI will be empowering some, and disempowering others. To name an example, you may have a dungeons and dragons campaign with your friends at home. Suddenly you now have the ability to have a wide range of real-time illustrations to accompany the quest where this was previously not accessible to you. That same model however can also be used for the cover-art of a book, empowering the independant writer to have better cover art without having to hire an expensive cover artist. In this example this cover artist would loose out on money of producing the cover art, but no money was lost in the D&D campiagn scenario since this group would not have been able to rely on such services. I do not see a difference between closed and open models in this, and I am of the opinion that it empowers more than it hurts when it comes to generative AI. At the same time I feel like this should be kept to empowering tasks that could otherwise not be done, being able to generate personal content on demand has enabled a lot of people to enjoy a new form of content. While simultaniously I worry about AI that is being used to replace jobs such as human truck drivers.
c. License terms dictate how widely an AI model can be used. I am involved with open source software where you need to bring your own hardware to run a model. For us, if a model is released under a non-permissive license where it may not be used for commercial use this empowers us because you will rely on software such as the one I am involved with to be able to run these models. Hosting companies are for profit, so they are unable to provide an API for these models. When models have liberal licenses where they can be used by anyone for any purpose this empowers the most people to enjoy the AI. They still have the ability to use our software to run the model locally, but if they find that an API hosting provider is better suited for their needs they now also have the liberty of using said providers service. So while it benefits my project when a model is released non-commercial and competing commercial entities are unable to use it, I actually advocate for models to be released as permissive as possible because that ultimately benefits the most end users. To me it matters most that as many users can rely on models they can own and keep if they desire, no matter how they end up using it. And if a license prohibits the corporations from hosting a model this means less users can enjoy the open model and are more likely to rely on a closed model instead. I do however find that it should remain up to the creator of a model to decide what is best suitable for them. I will also add that conflicting licenses can prevent certain research or hobbyist use, we have had cases where a model was licensed with the AGPL which mandates that commercial use is possible (The creator merely intended the source must be open), another model was licensed with a non commercial license. This resulted in combined models where the creator of this model did not notice the license conflict and this model spread further in the ecosystem after being merged in to even more models. By the time it reached the model I was creating I had no easy way of seeing the license conflict and I only learnt about it later. Having clarification on how to release such models would help, which license would trait apply in such a case? The forced commercial or the non commercial license? Or would it not be allowed to distribute such a work? Currently the researchers will try to ask the upstream creators their intent or for a custom license to then provide a clarification. But it does make it more difficult especially since its not established for us legally on a more technical grounds. For example lets say model A has a value of 5, model B has a value of 8. The act of combining models would pick a value between 5 and 8 so this could be 7. Is 7 now beholdant to the license of model A and B because it was derrived from A and B? Or is 7 a unique value that is uniquely copyrightable / not copyrightable at all?
d. I partially answered this at point C, I think the biggest issue is that the space lacks specific licenses for some use cases. A popular one is "I want my model not to be used by corporations or providers, but I do want everyone else to have as free access as possible even if they use my model to produce a commercial work" or "I do allow corporations to use the model, but all changes must be made publically available for local use. If someone wants to make a non-commercial model based on this model that is fine". The first one is currently commonly done trough CC-BY-NC(-SA) although that says to little about output. The second use case has no known license we can safely pick. If both existed you could produce a model from both source models that must be distributed openly if used publically, while simultaniously you are not allowed to host it for profit.
7. This one I am not in a good position to answer, as an AI hobbyist I am downstream from these efforts so we get access to premade models and then finetune them further. If we describe a foundational model as a model which was pre-trained the work I am involved with would not classify as such. We are not an organization, just individual hobbyists exploring the possibilities and using the AI models for entertainment. My interest is that we have suitable models for fiction generation we can legally improve and distribute, regardless of who governs this. Our budget is very small, for reference the cheapest model I finetuned cost me $5 in tuning costs due to a small dataset of text adventure games and an efficient finetuning technique. If hobbists have to spend time or money interacting with a regulatory body they are no longer able to excersise their hobby trough normal means as it will become to costly for them to do so or they can not itterate as quickly. Another thing we have built for our community is an automatic tool that can combine multiple AI finetuned models (built on the same foundational model) into a new model unique to their taste. If we were to be forced to adhere to regulations we can no longer provide this free service to the users due to the legal risk (In my case I am european, so if the european version of the legislation permits me to do so in the end we can keep providing it otherwise the service will be shut down. If the US also adopts similar legislation users will instead have to rent machines of their own to accomplish the same task, purely because that puts the legal responsibility on the user).
a. Tools that prevent the data from being downloadable to the public.
b. Models can not be regulated after they are already released. Considering they have already been released this also directly implies that the existing models can not be regulated effectively as they can continue to be freely distributed and modified outside of your jurisdiction. So changing legislation on this would not eliminate this risk, but would impact innovation of new models within your jurisdiction.
c. Ideally they should always disclose it so users have the freedom to use said models elsewhere. But that is an idealistic view-point purely in favor of the users. If you would want to protect these corporations from clones and local users having to much info on how to replicate their service you should not mandate this.
d. The most important thing is that you prevent collateral damage by being as precise as possible. "Foundational model" is as precise as "Electronic Device". There are so many different kinds that can be made that require different regulation. You would have entirely different legislation for a toaster than for a nuclear missle. Yet, both use electronic devices in them. If you were to try and legislate that a screw can not be designed in such a way that it can be used in a nuclear missile you would have an impossible task at hand. Similarly I would split up the effort for this legislation to be specific to the kinds of tasks the AI is designed to perform. You would want to make distinctions between each type of AI, generative, detection, automation, etc. And you should limit the legislation to intended use (To not accidentally ban every kitchen knife because they could be used as a weapon). So asses the risk based on intended function and misuse, but don't force every hobbyist that merely wishes to create a model that is good at writing a sci-fi book to adhere to safety standards meant for autonomous drone weaponry or models answering questions on bomb assembly that is being made by a weapons contractor.
e. If you are going for a top down approach you should empower these platforms to communicate clearly what is allowed and what should have gated access in as precise of a term as possible and have a mechanism by which this can be made more precise. Huggingface hosts everything from facial detection models, virus analysis models to models that act like a funny chatbot or generate a book. If you wanted to do a top down approach and required Huggingface to have a checklist along the lines of ("On which model was your work based?", "What is the intended use case of your work?", "Are the following high risk subjects in the dataset you have added to the model?") produced AI models can be automatically gated according to the standard without assuming the same standard applies to everything. Likewise, I am also in favor of a bottom up approach where it is up to the creators of a work to establish the risk of distribution and terms of distribution. I do think they should be able to host any model intended for any purpose, but they should have tools that allow defining who has access to these models. For example, if a model was produced using classified weapons data it would in my opinion be fine if it was present on Huggingface using their gated access option where the people behind the model approve who gets access. Huggingface is a great hosting provider for these models and has benefitted us greatly, it would not be a good idea to force people to rely on other hosting for the same files.
f. Yes, in my opinion it is important to weigh the ability for a hobbyist to upload a produced work against the ability for OpenAI to produce a highly filtered API with more RND cost than the hobbyist will ever make in their lifetime. The legislation for open models should be tollerant enough that hobbyists can continue producing models without having to go trough regulatory bodies or having any legal risk from misuse of their model, as long as they do not purposefully add high risk data to the model. If you took an existing model and expanded it with data on how to repair computers, you should not be held accountable if it turns out someone else misuses that model for a different use case. You should only be held accountable for the data you have added yourself, if the AI suddenly gains an ability you could not forsee because this was not in your data this should not be your own legal risk, but the legal risk of the user. For a company you can legislate it differently. There you can hold a DMCA style approach, where if misuse is detected they are legally require to filter / block that misuse given that they can make changes after the fact.
g. Currently corporations are trying to cause overlegislation by offering suggestions that could never be followed by a hobbyist with good intentions. So in the international regulations I think its important to properly shield researchers and hobbyists for legal responsibility when their models are misused. The current international trend has been to place the responsibility of usage solely on the creator / provider. And while this makes sense if you can change the outputs, this does not make sense when you have no ability to prevent misuse you did not deliberately enable and can no longer prevent after the fact.
h. I am unaware of countries currently getting this right. The EU has already gone to far in their legislation and have become a danger to my hobby. I am writing this in hopes that the US can be saved from overregulation so that my friends in the US can continue enjoying this hobby. I think its more valuable to look at different types of legislation such as the DMCA copyright act providing exemptions for hosting providers if users upload bad content for inspiration, as well as existing legislation that protects a kitchen knife producer from any legal consequences of misuse of their product.
i. Base it around the dataset (and for AI other than generative AI the function they are trying to fulfill). Establish what sort of information is considered high risk, and then you can tie that to a level of regulation. If none of the high risk information is added to a new or existing model there should be no problem in sharing such model. If a model does have regulation because it falls in a more high risk category it should be possible for someone developing a derivative work to have access to the kind of tests that need to pass, so that they can self-certify rather than having to spend a lot of money or wait on regulatory bodies. For example when a model contains information that could lead to the construction of a chemical weapon and the corporation that produced such model bundles a set of questions that are not allowed to be answered. Someone finetuning the model can then self-certify by asking their finetuned work the same questions to see if the intended safe guards are still in place.
j. It should be as freely accessible as possible unless the model contains high risk data that is otherwise not publically available already. In those cases you can gate it to the people who have access to that data. I don't expect this point to need any regulation, after all these organizations already have data safety standards.
8a. Access at minimum as available as the material would otherwise be on the internet. But I do think its important to consider if the AI has a unique benefit rather than just replacing existing jobs. For example replacing a truck driver does not add a function, it places someone out of a job for the same task. The delivery will not suddenly go faster since that is limited by the truck. Simultaniously empowering everyone to generate a text or an image on demand in speeds way faster than any human could ever produce allows for new use cases that are otherwise not possible. Focus on empowerment, not pure replacement.
b. For my personal hobby I can rent up to 8 simultanious GPU's of the best class, so this question falls outside of my own knowledge. If you wish to limit the legislation on large amounts of compute I applaud this. But please be precise in the wording that people making derivative works with less compute are exempt if you do so.
c. This is outside of my expertise considering I primarily deal with fiction generation AI. We have no such metrics ourselves in our group.
9. Consider AI a broad technology which should not be put in the same category and draw inspiration from legislation to for example the internet.
