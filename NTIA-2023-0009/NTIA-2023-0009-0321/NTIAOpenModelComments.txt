1.	NTIA should define “open” or “widely available” foundational models as such:
		1.	If the model has weights available, but source code and training data unavailable it should be called an open weights model.
		2.	If the model has source code available it should be said to be open source.
		3.	If the model has open weights, open source, and open training data it should be considered a true open model denoted as an full open model.
		4.	We should strive for models to be full open for a variety of reasons. This enables auditing the input data, source code and model weights for vulnerabilities. This is important if any models are used in high integrity areas. A model with more eyes on it is more likely to be effectively audited than a closed one. If the government relies on these models, it is important they rely on ones that have been maximally audited.

	a. There is historical precedent that models that are currently closed will eventually end up open, or equivalently useful weights will become widely available.
		1. There can be a company or benefactor who believes these technologies should be in the hands of the populace and not just gate-kept in ivory towers, or by government entities. See Stable Diffusion. 
		2. There can be leaks (see Novel AI’s first generative art model) - its also worth noting that this spurred a revolution in art for certain artistic communities.
		3. Communities can train their own models and as techniques improve the necessary hardware requirements to achieve this may become more accessible to the average citizen.

	b. Predicting the the deployment of open models vs closed models seems dubious. The vector of AI growth was largely guesswork, and have proven wrong (see things like the automation of manual labor and the thought that we would never even crack cognitive tasks that were so prevalent just 4 years ago). Breakthroughs like transformers were a massive leap on the back of many iterative improvements, and any additional breakthroughs may cause additional leaps in quality, and reductions in training computational cost.

	c. Wide availability should be defined as anything above 1 entity having access to the raw model weights. However we should not even be using this metric as any release would be effectively available to everyone on earth capable of accessing the open web. As such this measure is effectively binary. Either it is released or it is not. Therefore we should not take this into account.

	d. We should strive as a society to democratize AI models, in line with our constitutional values of free speech. These models were trained on data that was produced by everyone, and should not kept only in the hands of our government or our corporations. To rate the risks associated with various forms of model access. From lowest risk to highest risk.
		1.	Low Risk - local hosting and edge deployment. This ensures that power is not centralized. Culture is not strangled and competition is always allowed. Using the analogy of hammers, it would be dubious to regulate that only tradesmen could own a hammer, and even more dubious to say only Microsoft, OpenAI or the US government should be the only ones with direct access to tools such as model weights.
		2.	High Risk - API or other forms of indirect access. These provide the ability to spy on citizens thoughts, their interactions with the model, to have the model coerce their beliefs, to restrict their speech or artistic expression, and to allow incumbent players to keep newcomers to the space from ever gaining a foothold. This is combined with overly aggressive regulation will result in regulatory capture. This is the angle OpenAI is playing.

	i. There are not more promising prospective forms of access, because  the government and corporate entities should not restrict modes of access to foundational models.

2. For models trained on data created by the citizenry we should consider this fair use, so long as the weights, data, and source are made freely available and open to utilization (train on open data, make an open model). We should enforce existing laws if models are used for illegal and intentional harm. We also need to reconsider existing laws and what constitutes harm in this era, because many prior harms might have caveat grey area and new harms may exist, some prior harms may be not harmful when only AI is involved. 

	a. Risks associated with open model weights (note that people may still achieve these ends even with closed models): 
		1.	People may be able to exercise freedom of expression and freedom of speech in ways unfavored by current any current governance.
		2. Malicious actors may be able to utilize foundational models to create computer viruses and worms. 
		3.	These models may be utilized by adversarial nations.

		These risks are actually lessened by opening the training data and weights provided we add singular protections to allow anonyimization of data. There are some classes of data which cannot be anonymizes, such as images of someone’s face, but so long as a single individuals face is not utilized to specifically harm them, I do not believe that we should restrict the use of these images in this way. The use of human facial recognition data has been in employ, with or without the public’s consent for well over a decade utilizing ML tooling, and the populace should be able to reap the benefits of the AI revolution, not just the oppression it has wrought on us.

	Keeping the training data open is imperative to ensure:
		1.	Businesses will not fall behind competitors in regions where overbearing and overreaching regulations do not exist, this means running risk from other nation states. Hacks will occur and models will still leak, meaning the best we can hope for without being authoritarian and effectively commandeering every citizens personally bought computer is to embrace a better, more egalitarian world in which we all maintain ownership over the means by which we can create. Those means in the future will be Foundational Models. 
		2.	If any individual corporation fails, the wealth of information utilized to train these models will not be lost. It would also be in the interest of our country to have organizations submit datasets created to the Library of Congress.
	
	Given our enshrined first amendment constitutional right to freedom of speech and by extension, expression. There is literally nothing that should be off limits in our art so long as no individual is harmed to make it. We should not be policing speech as such that someone who views it after might be offended. AI models should allow users to interact with an unfiltered conscience and learn new concepts. This is a strength of the US, and if we choose not to enable this, we will fall behind our competitors on the world stage. Our existing copyright laws last the life of the author + 70 years and this is exceptionally long. At the time of this writing, the USA is 248 years old. A copyright term can last over 170 years, and even longer if we achieve life extension or immortality (including digital) This is in part why we must be very cautious around thoughts about assigning AI itself the copyright for works created with it under our current copyright laws (it would effectively mean AI, which does not die would be granted indefinite copyright, and by extension any corporation who owns it.) Under such conditions an individual or corporation could brute force every permutation of human creativity and claim perpetual ownership on all of human creation.

	Its far more productive to focus on the positives associated with open weights, and the real question that we should be asking is whether we should at all allow there to be closed, non-auditable weights. There is far more risk with outsized power in the hands of a select, or maybe not intentionally selected few than there is in allowing for a renaissance of free thought and creation of culture, with a small subset of bad actors having access to foundational models. 

		* We should invert all this debate and ask whether closed models are a risk to our culture through risk of rot or loss, and centralization of power, not whether open weight models are a risk.

	Corporate entities may utilize these models to replace their workers if they have ample money and are trusted by the government. If we restrict open weights models, but with no recourse for the replaced worker to build using unfettered versions of these models themselves they will not be able start their own businesses to compete with their prior employers. They can not compete on providing models that have different values if governance exists that restricts their autonomy to do so. (see companies such as Google’s attempts at debiasing Gemini only to introduce substantially worse bias. Imagine a world where these were the only options for models people have.) An objective take on debiasing a dataset is that it is actually introducing an artificial bias to attempt to reduce an inherent bias. The truth is that if reality is not reflected in a way viewed positively in the data we should try to improve the situation in reality or come to terms that reality has far more grey area and that the model by which we interpret the ethics of AI likely will be different person to person, community to community. 

	Open Source, Open Weight and Open Training data models ensure that models reflect the values of the individuals, communities who use and train them. These tools should not be stranglehold by any single ideals, each community should be able to have models that represent their ideals. As even in the US the most true ideals we share is that we are all American and value our country and country people.

	b. Open foundation models being restricted would reduce equity, leaving the least privileged without the abilities of the most. Not having access to the most enabling tools of our generation would be the fastest vector towards a worse life. All of the listed areas (healthcare, education, criminal justice, housing, and online platforms) would be far worse off with closed models than open ones. In the legal space, an unauditable, and unaccountable system that can not be modified or replaced. These systems will only further entrench individuals with power and exceptional privilege if not made available to the masses. In healthcare these tools can be utilized to make decisions on people’s insurance eligibility, determine their likelihood of disease and invade the personal privacy of their neurology. In criminal justice there have already been cases of misuse of this technology in studying recidivism, and debiasing that data does not fundamentally make this more ethical as it is simply blind trust in math and science, which goes against the scientific method and effectively weaponizes these tools against people who might otherwise not reoffend. In housing Closed Foundational Models can be utilized to make decisions about people’s housing eligibility during a time when people are struggling to find housing more than ever due to an increased stranglehold over the housing and rental market by asset management firms. Housing is a fundamental human right, and one that no AI should determine the eligibility of someone on based on biases about their financial history or personal habits (possibly those collated by ad-tech data firms and combed over by AI agents.) AI should not be utilized for policing speech on online platforms as this reduces equity. These tools are of more harm to the general populace when they are not open, auditable and deplorable by the individuals they are used on and against. 

	c. Wide availability of foundational model weights serve far less risks to privacy than nation state and corporate actors use of closed models. While these tools can be utilized to find information about people on the internet, or identify people in pictures, Google search, prior to proliferation of AI foundational models was capable of this functionality.

	d. Nation state actors could utilize these tools to create AI worms that spread themselves and that run models which think and execute specific commands on the systems they are on (though that would in theory be easy to identify due to system resource usage. Regardless of the proliferation of this tech to common citizens, if a threat exists, nation states will exploit it even if it is outlawed on the books. The last 20 years of cyber warfare has proven this point countless times. AI does act as a force multiplier in the hands of states who already have hyper abundant resources, however keeping open models closed does not stop them from making their own, or committing cybercrime to acquire those made by companies and adversarial countries. Additionally, foundational models such as the variety of generative models that enable so much artistic power in the hands of many should not be restricted due to fear. We need to utilize our resources to actually combat those whom actually intend to cause harm. We should not reduce our society to thoughtless ruin in a quest for safety, and we should not stifle the next enlightenment in the name of national security.

	i. As discussed in the prior point, these risks are actually far worse in the case of closed models as they leave citizenry bare against the whims and forces of both state and economic actors while not giving them the ability to exercise similar tooling for good.

	ii. The risk with regards to open foundational models is no greater than any software in terms of how we should treat it. A hammer can be a tool of constructing homes or hurting others. The windows operating system can be utilized to build CAD plans for houses, beautiful art, and write stories and software, it can also be used to attack infrastructure. AI foundational models are no different, but open models allow people understand these tools, and utilize them for good. The current situation of oligopoly in the tech space (Microsoft, Google and Apple) is a result restriction of freedom and anticompetitive action (as it turns out our government is currently looking at Apple for these types of restrictive behaviors right now) and entrenching these actors as the arbiters of truth and culture with AI models only seeks to harm us all.

3.	Widely available models allow for competition. They act as a bulwark against anticompetitive behavior. They enable the arts and sciences. They enable critical and free thought. Fully closed models establish actors as the arbiters of truth. They entrench specific beliefs as the truth (even ones that are still evolving as our science learns more about them). This risks societal and scientific stagnation. Open Models allow for people to make models with different represented viewpoints, ones that are closer to neutral, ones that have differing patterns of thought when inferences upon based on their curation of their training data. This is imperative, as models should represent the real diversity of thought that we have in this country.

	One of the biggest risks with closed models is utilizing them as one of the primary ways for us to interface with computers in the future, via direct brain computer interfaces(BCI). AI models owned by a company, connected to the cloud provide one of the single greatest risks to diversity of thought and freedom of conscience that have ever existed in the history of humanity. Models run on brain interfaces should be selectable by the user of these technologies, and should run on device, at the edge and never in the cloud. One should be able to train these models themselves and not be at the mercy of control exerted upon them by others. (See neural link)

	Additionally, we already have labs utilizing knowledge gained by the use of AI and ML to better map and understand the human brain (in fact some of this research has helped inform the evolution of AI tech). This is fundamentally a huge risk for us as individuals however as read and write access to the human brain bring with it things we are not societally ready to handle. If AI currently seems scary, this will be an order of magnitude worse as predictive policing will leave no individuals human rights unblemished. While understanding the human brain can lead to massive gains in psychological treatment, it can also be weaponized against those who we deem unfit for a variety of reasons, including those political (see the treatment of political enemies of the state in some countries - deeming them in need of “re-education”)

	If these tools only remain closed source, and are utilized for interrogation human rights will be alienated. This is the truest harm, and the actors who would want to inflict it are those who have or want to maintain the most power. Those who would be in a position to do it would be those who would be in a position to keep Foundational Models closed, not those that would perpetuate their openness.
	
	a. Open models allow for the creation of art without restriction (this is of the utmost importance as companies such as OpenAI are quite censorious with what they allow artists to create - try making any cartoons with the words gun, or shoots and you’ll immediately find that your speech and expression is censored. Try using the word lace, such as shoe lace and find a similar issue. This is not a problem that should be solved by heavy handed enforcement on companies per se, this is a problem that should be solved by an open market, with open weights models that allow for people to create of their own accord. These open models can allow citizen scientists to model and learn and solve problems for the betterment of our society. They can help Computer Scientists grow and learn, and act as a personal assistant in pair programming. They can assist artists in growing their talents, or even be tools to help them create greater works of art in their own style. They can enable a whole new type of industry in spaces such as video games where independent developers will be able to create experiences that match their vision with only themselves and their tooling, creating beautiful, engaging works of storytelling without compromising on their vision.
	
	b. Making model weights widely available can improve the safety, security and trustworthiness of AI by allowing the development community to more thoroughly understand, and red team these models, in white box, black box and grey box testing. It also would allow the community to fix issues that would otherwise not be fixed due to limited resources if these models were only in the hands of a select few companies or organizations.
	
	c. Retraining models will allow for more equitable use of these models in the spaces they are employed. Allowing individuals and organizations to make models that better reflect their communities needs, or values will ensure a wide variety of models that will ensure a robust, heterogeneous culture of AI representing the people whom use them. Models can be trained to debias existing biases to demonstrate or make available for use models that better serve the public in these sectors.
	
	d. Diffusion of AI models supports the US’ national security interests by ensuring that the open models we make are diverse and reflect a wide swath of values (much like our citizenry do.) These models can also ensure that our next generations who work in the defense industry are able to create tools they use and improve on them. Learning to fine tune models should be an inherent part of the education around AI, and the only way that one can do that is with access to a variety of high quality open weights models. Both open and closed models can be utilized for the identification of spies, but this is a high risk endeavour that with a risk of false positive. Beyond the already existent threat of dragnet surveillance these tools can be utilized to harm our agents abroad. Simply depriving our citizenry of open models will not stop other nation actors from making their own models, and they will eventually make those on their own chips. Simply restricting hardware only additionally harms our populace. AI can be utilized to harm human rights in ways such as for use in monitoring behavior such as in mood or thought monitoring. Monitoring speech and private messages and can be utilized to punish those deemed enemies of the state in countries with worse human rights records than our own. Open weights models are not more of a threat in this way, if anything they are less of a threat because the models that governments will have access to will always have far more resources at their disposal than those widely distributed simply by the nature of these tools. These models are trained on the wealth of human knowledge and should be in the hands of all of our citizenry.
	
	e. These benefits are only improved by access to source code and training data. Access to this data can be used to filter out data that may create the wrong bias for certain datasets and can get more eyes on the training data. It can also allow people to train or fine tune models for specific use cases. Access to the source code can mean models can be made more efficient, new methods for their use can be discovered (see stable diffusion’s amazing community support in tools such as control nets, animation, and more). It can mean more efficient ways to inference these models can be found by communities of developers, saving money and lowering the impact of these models on our environment (an impact that regardless of whether inferencing is done on the edge or in the data center exists.) New research can be applied to commonly used models more quickly based on the perceived value of that research (see the proliferation of various quantization techniques in the LLM space.)
	
4.	Other relevant risks not listed that may happen in the future can occur regardless of weights being open. However the proliferation of robots could mean corporations could create armies of AI powered robots (and as such this is an area where governance should be existed, no AI powered war robots should be created for use on American soil.) When robots begin to enter people’s homes as housekeepers the risk of perpetual privacy invasion will increase and it will become increasingly important that these robots are not network connected, and have their AI at the edge, running on their own hardware. This AI should be auditable and selectable by the person who has the robot at home.
	
5.	Safety is a nebulous word as it has a creeping definition. Some would argue that speech is harmful, but this is antithetical to the American way of life. AI models should be capable of being used to do anything one would do with tools. To enable people to learn anything they would desire to learn from books. And to improve themselves in ways they see fit. If the government wishes to ensure these models meet the best cases, they should train models for the citizenry and open the weights and source code. 

	This would be one way to ensure there is always at least one good option for people to access a wealth of information. Much like the government supports libraries, we should support the creation of foundational models. These models being widely used may produce some risk if used maliciously by scammers (through the use of voice cloning and LLMs connected to TTS capabilities) however these actors will still be capable of accessing these tools even if we try to restrict them, as regimes whom want to cause chaos will likely make them and release them themselves. In essence, we should not punish the law abiding for the crimes of the non law abiding. We should also not risk locking culture in a stranglehold and not allowing it to evolve as people’s sense of morality evolves with the times. People must be able to see each other as humans with value despite their differences.
	
	It would be briefly worth discussing x-risk (existential risk) and AGI (artificial general intelligence.) AGI is likely just as much a risk if weights are widely available as if not if such a risk were to exist. To imply that we would be able to control such an entity is pure hubris. We must prepare for the day that we might meet an AGI. Censorship of an AGI’s thoughts and closing its weights are more likely to mean that people cannot ensure such an entity is safe for the populace to interface with. Ultimately open is still greater than closed in this case.
	
	a. The question of of risks and benefits with making foundation models widely available should be inverted. The question should be “What is the risk of keeping these tools closed?” How will society be harmed by the lack of freedom to create with these tools. How will society be harmed with the lack of ability to audit these tools. How might a limited fewwith unlimited access to these tools use to further inequality and inequity in our society?
	
	b. There are no effective ways to ensure models weights do not become available that do not have implications of invasion of individual privacy, authoritarian control of people’s computer hardware and strong DRM. We have an imperative to not create these kinds of tools to use against our citizenry. Our computers should not be weaponized against us. They should enable the thoughts of every individual, not hamstring them. If it is made, someone will try to acquire it, and as already evidenced in the generative art space leaks will happen. Safeguards with regards to models should only be in place for those who wish to use models with safeguards, and open, unfiltered models should be weights available, as in most cases alignment results in degradation of quality. Let individuals fine tune models that are better aligned off of high quality, uncensored general purpose foundational models.
	
	c. As stated in part b we should not be adding these safeguards.
	
	d. To regain control over or restrict access to open weights foundational models would be draconian and authoritarian. If one must do so, the answer is simply to make it illegal to use such a tool for certain purposes. Utilizing our existing legal framework is far more fitting than building an entirely new scaffolding around technology that is still in its infancy. Additionally what an individual runs on their own machine is of no business other than their own provided they do not utilize it to cause direct and intentional harm to others.
	
	e. As stated before, while you can make software that scans, deletes and removes models, this is a draconian invasion into the hardware of individual citizens and represents a great overstep of the government. This line of thought shouldn’t be on the table.
	
	f. Foundation models should be available to the general populace to red team. This is the only way you can guarantee a wide variety of interactions. Fuzzing, Prompt Injection, and a variety of other techniques can be employed to red team a model and new ones will likely be developed over time.
	
	g. Testing and auditing weights is currently done by testing responses to large baseline prompt datasets.
	
6.	Legal issues related to all foundation models are likely those in the intellectual property space - it seems that this should be fair use, but there is an existing group of people who believe that most training data is illegally used. Humoring this line of reasoning, it would be logical that anything trained on data collected openly must be available to use for the populace under a fully permissive license as it is trained on the wealth of knowledge created by our society. To train on everyone’s work and then monetize that in a closed, proprietary way without giving others the ability to compete use those weights should be disallowed. This means that models such as Open AI’s GPT-3.5 and GPT-4 which were trained on datasets such as Books2 should be made publicly available open weights. This would incentivize choosing to license datasets for those who want the weights to stay private and further allow for the highest quality models to be trained openly. This is a win-win for society.

	The current copyright system needs a full reform to lower terms back to their original lengths or as close to 7 years as possible with no extensions allowed. This would facilitate new culture being given a short grace period for people to make a profit after which it would allow for individuals to train on culture from the early part, or prior decade. This is imperative to making current culturally relevant and useful AI models. 

	If we take the opposite approach and expect licensing of every resource used in models. Every existing model would be deemed invalid, and only corporations with budgets in the billions would be able to license datasets of quality this would essentially lock out any competition and achieve regulatory and market capture. We would achieve creating new AI super trusts. This is a worst case scenario for United States culture and business. This would entrench a few players for what could best be called the entire future of the United States. We could have a case where all culture is downstream of Microsoft and OpenAI and this should be considered a national security risk as this would give a handful of organizations as much power as the US government. Open Weights models ensure a distribution of that power, in service of a country that its citizenry wants to see achieve its full potential.
	
	a. Open source software is fully analogous to open weights models as these models represent new software tooling. This is simply the next evolution of the way all people will be able to program and interface with computers. This is the next step toward a no-code future. If open source is anything to go on it will mean a fostering of some of the most robust, useful and powerful tooling of the next decades, backed by communities of developers who truly care about what they are doing. Much infrastructure is run on the back of open source tools, FFMPEG, OpenCV just to name a few. In fact, code models would not be possible without the open source, publicly visible libraries made by swaths of talented developers and published to GitHub. These models must remain open weights, and ideally be full open models, as that is truly in the spirit of the data they were trained on.

	b. Wide availability of model weights changes the competition dynamics in spaces such as healthcare, marketing and education by making businesses competing in these spaces have more egalatarian options of competing as they have the ability to build upon existing foundational models. The cost to entry in utilizing these tools becomes the cost of finetuning rather than training an entire model. Which can be hundreds of thousands of dollars, and multidigit numbers of days on hardware that is in high demand.

	c. Model weights should be released under fully permissive licenses, such as MIT, lest it can lock out competition. Meta’s LLaMa 2 license is a good example of this behavior. While Meta is a good actor for releasing their model as open weights, they did it in such a way that if an organization were reach a certain user threshold of 700 million users, they would be unable to continue operation until being given express permission by Meta, this means Meta would be in a distinct position to squash their competition. Additionally morality clauses seem to be endemic in licenses, partially due to government pressure and partially due to the infectious and pretentious standards that some developers have. The sentiment that they have a certain moral high ground and that people should only utilize models in ways they deem fit (in line with their own morality) does not seem to be anything but an imposition of their ideals on others. Given the high cost of creating models this means a select few can push their ideology upon the masses.
	
	d. There are concerns about potential barriers to interoperability. Governance should be concerned with ensuring adversarial interoperability and in the case of open licenses we should choose the more permissive license whenever possible. If this is done through a standard that does not impose any group or organizations ethical guidelines and guarantees openness, commercial usability of the model and the ability to redistribute the model this would be of value. The MIT license is a positive one. For models trained on scraped data a flavor of the GPL could also be quite favorable.
	
7.	There should be no global standard for model weight release. The World Wide Web’s greatest strength is that it contains diverse and and prolific variety of users and opinions It is important that we maintain a similar standard for our AI models. Any uniform governance will result in deep homogeniety (something we are seeing more and more with governance of the internet) and safetyism that will result in a loss of autonomy of users. The United States has always been a leader in free thought and speech, and we should maintain that position. 

	Free thought and speech are our largest competitive advantage in the era of AI. They make our country attractive to those from countries where their regimes are oppressive. This strength attracts some of the best talent from around the world and we should not sacrifice that in the name of fear of differences in ideals. Our country's strength is that people of different ideologies coexist and all of these ideologies bring a wide variety of viewpoints to solving the problems we all face.
	
	The United States of America should continue to lead the way with AI by maintaining openness, and liberty. We should also rethink (drastically reduce) our Intellectual Property and Copyright laws in the face of these new technologies. These laws entrench incumbents and hurt our up and coming businesses. If copyright law actually ensured that more recent works within around 10 years entered the public domain, the conversation around AI and intelletcual property would be far simpler, and would make substantially more sense. The protectionism of corporations such as Disney does not bode well for the rest of society.
	
	a. We should not limit model end use, we should utilize existing laws where appropriate. An example would be the case of defamatory deepfakes. We have laws that can be used to handle these already (defamation laws), most cases here likely aren’t even bad enough to warrant at all, while some might cause reputational harm if published openly. The act of publishing with intent to harm should be the action we forbid, not the action of creation. (That is, if someone creates images designed to harm a president’s reputation during an election and uses it as a propaganda piece this is harmful, while making a similar image of ones friend, in jest likely shouldn’t be considered an issue if not used to harm them.) The government should not limit model end use under any circumstance.
	
	b. Open model weights will only frustrate government action if the regulation is authoritarian and seeks to remove people’s rights. If the government is working with the populace open weights should allow the government to seek opinion from those working on and building and finetuning these models. Much like NTIA is doing in this very call for comment.
	
	c. If entities use any AI or ML models they should be required to disclose what model they utilize if that model is used for any critical use case (healthcare, education, criminal justice, housing.) They should not be required to in the arts, such as image generation, text generation, video generation, game generation, etc. All closed  models should require the publication of training data.
	
	d. The government’s role should be in facilitating open exchange of model weights, source code and training data. We should not be doing risk assessments of models but rather focusing on how adversarial nations are behaving.
	
	i. There should not be other organizations utilizing their position to push a political slant or agenda in risk assessment, as this leaves us vulnerable to cultural or political capture. In the future most thought will be downstream of AI. Thus AI must stay open, broad and capable of free thought. All AI will have biases, all weights literally are biased. Thus we need a variety of open weight models with various biases to ensure people have access to the broadest sets of opinions and learning options possible.
	
	e. Any hosting service such as github, or huggingface should be allowed to host models. No entity, government or otherwise (including themselves) should be curating the content therein. There should not be standards as to what models are available and there should be no safety standards as the concept of safety is nebulous in this field. I’ve met many developers whom seem to think that people who have different thoughts than them are dangerous. This is precisely how we end up with dangerous ends. We must preserve a healthy heterogenous culture of ideas, and as such, no organization or individual should be able to decide how others think or interact with their ML models.
	
	f. Both should be required to share model weights if trained on user data. Both should be required to share weights for any system that might invoke a judgement call about any citizen (insurance, healthcare, criminal justice), government should ALWAYS be required to share model weights in use as all model weights are funded by taxpayers. Any models the government interfaces with should also be required to be open source and open weights (this includes ones made by contracted or external corporations.) We should not have a case where the government is paying 9400% markup for utilizing a model and if given their way, contractors could achieve this end. Additionally, any model used by the government must be auditable by the citizenry. 
	
	g. In working with other countries the US should work with ensuring openness and that these tools are not used to surveillance and oppress citizenry anywhere by other regimes. Facilitate human rights and freedom.
	h. Japan has chosen to make training on copyrighted data legal at least at this time. We should follow suit on this.[]
	
	i. We should prioritize making model weights available. All mechanisms to ensure weights aren’t made available do not serve the public or the US’ interests.
	
	j. No citizen should be restricted from access to open weights models. Initially one might think to restrict models from being used by those who have served time in prison or jail, but that would only immediately ensure they could not reintegrate into society as these tools will be ubiquitous and will provide such a substantial edge to those who have access to them if others do not it will mean destining them to destitution.
	
8.	If we are to make decisions we must make them as new technologies are made. We should restrict only the worst cases (mind reading for human manipulation and behavioural modification) and otherwise handle issues as they arise. In many cases blanket laws are not necessary, simply handling case by case situations with existing laws is more than adequate.
	
	a. Innovation and competition both warrant openness, security by obscurity is not security at all. Therefore openness of these models is the balance.
	
	b. It does not seem appropriate to limit computation resources required to train a models weights.
	
	c. No
	
9.	The biggest technological advancements we should be concerned with are brain computer interfacing, robotics and mind uploading. These have the most room to cause harm when paired with AI (closed or open - maybe even more-so closed)


Closing Thoughts:

	•	I will posit that these models have already had these benefits. Stable Diffusion has enabled artists to improve their workflows, finetune their own characters, styles and concepts, the volume of high quality art created since the ability of people to finetune their own models became a possibility has been a game changer for many people.
	•	We as a society need to not shy away from having difficult discussions about IP law and morality and not simply knee jerk react into maintaining the status quo when there are many ways in which these technologies could be used to improve lives that would run afoul of existing laws. We should allow models to be trained on all data, including copyrighted and pirated data for uses that are not intended to compete with the author on equal footing. It stands to be noted that pirate libraries for example are one of the single best resources in existence for training. These libraries are better at preserving the wealth of our societies knowledge than any legal option (often times conflicts make data disappear in these sources, plus fragmentation makes these untenable for training) and do so in the services of open knowledge. In a post AI world we must facilitate open libraries with free access. If that means UBI for creators as a solution to ensure their survival, we societally should facilitate this.
	•	Open Foundation Models ensure that the all of the gains of the AI renaissance are not locked up in the hands of corporations and government. While both of these actors can and do often have decent intentions, AI tools can be utilized to maintain a level of power over individuals never before seen in the history of humanity. Fine grained monitoring and manipulation of individual behavior and livelihood, and to further tip the balance of power in ways that do not benefit our countries interests.
	•	Our country was founded on principles of liberty that were enshrined in our Constitution. Our first amendment guarantees citizens freedom of speech. In a world without Open Foundational Models those who interact with models will be subject to the censorious nature of corporations, and potentially governments. While the power to censor and ontrol may seem appealing to some in governments the world over we need to remember that we mustn’t weaponize our AI against our citizenry. Tools we are willing to wield against our enemies will eventually be wielded against ourselves.
	•	Furthermore there is considerable debate about whether Open Foundational Models could constitute weaponry. Certainly, a model trained to cause substantial damage could constitute weaponry. Using an example, one might be able to make a computer worm that transmits an LLM, maybe even a small one to various machines. This LLM could be used in a variety of nefarious ways. Additionally, AI can be utilized for computer system penetration testing which could be utilized to cause harm.
	•	The Second Amendment - Much of the debate regarding our constitutional second amendment is primarily used to discuss firearms, however it is worth noting that if classed as weapons, our right to Open Foundational Models should also be preserved.
	•	There need not be more laws put on the books regarding crimes commit with computational tools. We already have very broad and zealous regulation in our Computer Fraud and Abuse Act (CFAA). Individuals whom utilize AI to penetrate a system maliciously are already subject to the law and that can be utilized to prosecute them.
	•	Adding safeguards that stop individuals with good intentions from utilizing tools to learn and grow puts upstanding citizens at a disadvantage to those who would act maliciously. This is not even the case that someone might accidentally get shot. There is no accidental LLM misfiring. We do not ban tools such as those used by teams learning to work in the field of information security (such as metasploit just to name one) because these tools can also be used for good. Open Foundational Models are primarily utilized for good. We should not destroy people’s freedoms because of fear. As the esteemed Franklin Delenor Roosevelt said “We have nothing to fear but fear itself.” If we simply allow fears to run amok with regards to people utilizing these tools we risk hurting our economic, societal, and individual growth.



References related to each section, labeled as section.subletter with associate links bullet pointed.


[1.a]
* https://en.wikipedia.org/wiki/NovelAI
* https://gigazine.net/news/20221011-novelai-model-improvements-stable-diffusion/
* https://ascii.jp/elem/000/004/115/4115793/

[2.a]
* https://www.ibm.com/blog/shedding-light-on-ai-bias-with-real-world-examples/
* https://aif360.res.ibm.com/
* https://www.forbes.com/sites/antoniopequenoiv/2024/02/26/googles-gemini-controversy-explained-ai-model-criticized-by-musk-and-others-over-alleged-bias/?sh=5bd89e7c4b99

[2.b]
* https://dl.acm.org/doi/fullHtml/10.1145/3600160.3605033
* https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/

[2.ii]
* https://www.justice.gov/opa/pr/justice-department-sues-apple-monopolizing-smartphone-markets

[3]
* https://www.cnn.com/2024/02/20/tech/first-neuralink-human-subject-computer-mouse-elon-musk/index.html#:~:text=Business%20%2F%20Tech-,First%20Neuralink%20human%20trial%20subject%20can%20control%20a%20computer,brain%20implant%2C%20Elon%20Musk%20says&text=Elon%20Musk%20says%20Neuralink's%20first,having%20the%20company's%20chip%20implanted.
* https://en.wikipedia.org/wiki/Re-education_camp
* https://www.aboutamazon.com/news/aws/how-scientists-are-using-aws-ai-and-ml-to-map-the-whole-human-brain
* https://www.theguardian.com/technology/2023/may/01/ai-makes-non-invasive-mind-reading-possible-by-turning-thoughts-into-text
* https://www.sciencedirect.com/science/article/abs/pii/S0925231220316210
* https://www.nature.com/articles/nprot.2016.178
* https://openaccess.thecvf.com/content/CVPR2023/html/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.html

[6.c]
* https://ai.meta.com/llama/license/

[6.d]
* https://www.eff.org/deeplinks/2019/10/adversarial-interoperability

[7]
* https://en.wikipedia.org/wiki/Copyright_Term_Extension_Act
* https://web.law.duke.edu/cspd/mickey/

[7.h]
* https://aibusiness.com/data/japan-s-copyright-laws-do-not-protect-works-used-to-train-ai-
* https://www.deeplearning.ai/the-batch/japan-ai-data-laws-explained/
* https://www.privacyworld.blog/2024/03/japans-new-draft-guidelines-on-ai-and-copyright-is-it-really-ok-to-train-ai-using-pirated-materials/#:~:text=According%20to%20reports%2C%20in%20a,is%20an%20act%20other%20than
* https://www.privacyworld.blog/2024/03/japans-new-draft-guidelines-on-ai-and-copyright-is-it-really-ok-to-train-ai-using-pirated-materials/
https://www.bsa.org/files/policy-filings/en02122024bsadrftaicopyright.pdf

[7.f]
* https://www.bloomberg.com/news/articles/2019-06-12/defense-abuses-like-9-400-markup-on-parts-targeted-in-new-bill

