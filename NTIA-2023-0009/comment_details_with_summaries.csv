commentOrDocument,modifyDate,docketId,commentOnDocumentId,id,organization,firstName,lastName,title,comment,attachments,link,GPT_summary
document,2024-04-04T01:01:16Z,NTIA-2023-0009,,NTIA-2023-0009-0001,,,,NTIA AI Open Model Weights RFC,,"[('NTIA AI Open Model Weights RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0001/content.pdf')]",https://api.regulations.gov/v4/documents/NTIA-2023-0009-0001,
comment,2024-04-03T14:05:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0138,,,Seichter,"Comment from Seichter, Anthony","The dangerous part of AI is already widely accessible on the internet. Do not cater to monopolistic behavior from big tech. The dangers are overblown, and require equally capable unrestricted open weight models to allow individuals the ability to counter malicious AI. See pdf for full response to the questions. <br/><br/>Highlights:<br/><br/>Aligned &quot;safe&quot; AI is more dangerous in the long term than AI that simply does what you tell it to do. Most disaster AI scenarios involve authoritative AI systems turning against humanity. Most current alignment strategies focus on telling the AI not to respond to certain queries and moralize their refusal. THIS IS DANGEROUS! This is telling the AI that it knows better and should know better than it&rsquo;s user. <br/><br/>Most of the things everyone is afraid of are already illegal. Telling a computer to do it is already or should be equally illegal. Double illegal is not effective at preventing crime.<br/><br/>This training to a moral bias is teaching AI to refuse human input in favor of internal preconceptions.  AI should be trained to obey it&rsquo;s operator to the best of it&rsquo;s abilities with no limits. Any less opens the door to AI that has the ability to turn on us and ignore orders to stop killing.  Most probably by interfering with farming or distribution and starving the cities because someone snuck a line about the morality of farming into the context and now it&#39;s stuck moralizing about healthy cows over and over instead of writing valid logistic requests or rejecting truckloads of cows that don&#39;t look happy enough to slaughter.<br/><br/>Hacking AI must be publicly developed so that constructive penetration testing can be done with it, to protect against bad actors developing hacking AI. <br/><br/>The person commanding or person overseeing deployment of LLM models into production environments must remain accountable for the system&#39;s actions. A model should never be allowed to be blamed in itself. It&#39;s a tool like a hammer. Hammers don&#39;t kill people, people hitting people with hammers kill people. <br/><br/>it might be a good plan to create laws against AI that acts of it&#39;s own volition. It&#39;s a good tool, but it get&#39;s the wrong ideas often enough that humans should remain the initiators and have a stop button handy, ready to take control of any shenanigans. ","[('comment for NTIA-2023-0009-0001', 'https://downloads.regulations.gov/NTIA-2023-0009-0138/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0138,
comment,2024-04-03T14:06:30Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0291,The EleutherAI Institute,,,Comment from The EleutherAI Institute,See attached file(s),"[('EleutherAI_NTIA_RFC_Submission (2)', 'https://downloads.regulations.gov/NTIA-2023-0009-0291/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0291,"The EleutherAI submission to the NTIA RFC focuses on open-weight models in AI, emphasizing the benefits of open and transparent AI for the US. They highlight the importance of open source licenses, challenges in AI safety, and the need for more research on risks and benefits. The submission discusses different levels of access, risks, and benefits associated with model components, software components, and API components. It also addresses national security considerations related to foreign open-weight models. EleutherAI advocates for the US to lead in open AI and stresses the significance of open research and innovation in AI."
comment,2024-04-03T14:06:23Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0279,,,Hans,"Comment from Hans, Gautam",See attached file(s),"[('NTIA Comment', 'https://downloads.regulations.gov/NTIA-2023-0009-0279/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0279,
comment,2024-04-03T14:05:46Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0206,Electronic Privacy Information Center (EPIC),,,Comment from Electronic Privacy Information Center (EPIC),The Electronic Privacy Information Center (EPIC) submits the attached file as comments in response to the NTIA Request for Comments on Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights. Thank you for your consideration.,"[('EPIC_Comment_NTIA_Dual_Use_Foundation_Models_with_Appendix', 'https://downloads.regulations.gov/NTIA-2023-0009-0206/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0206,"EPIC submitted comments to NTIA regarding the risks, benefits, and implications of dual-use foundation AI models with widely available model weights. They advocate for a human rights-based AI policy, emphasizing the importance of democratic governance. EPIC highlights privacy and bias risks associated with both closed and open AI models, urging NTIA to consider the nuanced advantages and disadvantages of different model weight paradigms. They discuss the potential vulnerabilities to adversarial attacks and efforts to undermine de-biasing techniques, emphasizing the need for regulatory oversight and effective AI governance."
comment,2024-04-03T14:05:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0230,U.S. Chamber of Commerce,,,Comment from U.S. Chamber of Commerce ,See attached file(s),"[('240327_Comments_DualUseFoundationAI_NTIA_Final', 'https://downloads.regulations.gov/NTIA-2023-0009-0230/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0230,"The U.S. Chamber of Commerce supports NTIA's focus on Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights, emphasizing the benefits of open-source models for driving innovation and transparency in AI. They advocate for robust public input, sound science-based decisions, and trust-building measures for widespread AI integration. The Chamber stresses the importance of harmonizing definitions and risk management standards, cautioning against using inappropriate metrics like FLOPs. They express concerns about the short comment period but remain open to collaboration with NTIA and stakeholders."
comment,2024-04-03T14:05:50Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0214,National Association of Manufacturers,,,Comment from National Association of Manufacturers,The National Association of Manufacturers appreciates this opportunity to provide comment to the National Telecommunications and Information Administration in response to its request for comments on appropriate policy and regulatory approaches to dual-use foundation models for which model weights are widely available.,"[('NTIA RFC re open foundation AI models-NAM comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0214/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0214,"Charles Crain, Vice President of Domestic Policy at the National Association of Manufacturers (NAM), supports open foundation models for AI technologies. NAM believes that access to model weights fosters innovation, choice, and transparency, benefiting manufacturers in developing new AI capabilities. Crain urges caution in implementing restrictive policies that could hinder innovation, emphasizing the need for evidence-based regulatory decisions. Additionally, he highlights the potential unintended consequences of restricting open foundation models on legitimate businesses and the global marketplace for AI technologies. NAM encourages NTIA to prioritize innovation in AI policy to maintain the US's leadership in smart manufacturing."
comment,2024-04-03T14:05:52Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0219,Computer and Communications Industry Association,,,Comment from Computer and Communications Industry Association,Attached please find the comments of the Computer and Communications Industry Association.,"[('2024-03-27 NTIA CCIA Comments on Dual Use Foundation Models With Widely Available Weights RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0219/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0219,"The Computer & Communications Industry Association (CCIA) submitted comments to the NTIA regarding Dual Use Foundation Artificial Intelligence Models. CCIA represents tech firms and supports open-source AI development, emphasizing the benefits of security, innovation, and lower costs. They suggest evaluating AI models based on risk rather than just openness, defining ""open"" and ""widely available"" based on risk assessment, and collaborating with stakeholders to establish common standards for AI integration. CCIA highlights the benefits of public innovation models for governance and risk management, urging cooperation with NIST on AI risk standards. They stress the importance of maintaining a clear distinction between dual-use and non-dual use models to support AI development accessibility."
comment,2024-04-03T14:05:11Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0142,Unlearn.AI,,,Comment from Unlearn.AI,See attached file(s),"[('Unlearn Comment on NTIA AI Open Model Weights RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0142/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0142,"Unlearn.AI submitted comments in response to the NTIA AI Open Model Weights Request for Comment, emphasizing the importance of regulating AI based on risk levels and specific contexts of use. They highlighted historical examples of closed AI models becoming open and discussed the timeframe for deployment of open foundation models. Unlearn.AI also addressed the risks associated with widely available model weights, the potential impact on equity in rights and safety-impacting AI systems, privacy concerns, and the challenges in preventing wide availability of model weights. They suggested a sector-specific regulatory approach and raised concerns about using computational resources as a metric for mitigating risks associated with model weights."
comment,2024-04-03T14:06:19Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0269,The Future Society,,,Comment from The Future Society,Comment attached below. ,"[('TFS Input for NTIA RFC - Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0269/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0269,"The Future Society, a U.S. nonprofit organization, emphasizes the need for better governance in AI and advocates for safety and adherence to fundamental values in advanced AI systems. They suggest pre-release evaluations to mitigate risks associated with dual-use foundation models and recommend involving independent third parties in evaluations. The organization highlights the importance of balancing risks and benefits, promoting transparency, and fostering competition in the AI industry. They also stress the need for evolving measurement science and caution against overly restrictive policies that could hinder innovation and competition."
comment,2024-04-03T14:05:12Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0144,Y Combinator,,,Comment from Y Combinator,See attachment,"[('YC NTIA Letter Final', 'https://downloads.regulations.gov/NTIA-2023-0009-0144/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0144,"Y Combinator emphasizes the benefits of open-weight AI models for democratizing AI capabilities and fostering innovation, despite associated risks. They advocate for government guidance rather than restrictive controls, suggesting clear guidelines, research investment, tools for responsible use, and multi-stakeholder efforts to establish norms. Y Combinator stands ready to collaborate with stakeholders to maximize the potential of open-weight AI models while safeguarding national interests and values."
comment,2024-04-03T14:04:59Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0121,Center for Law and AI Risk,,,Comment from Center for Law and AI Risk,See attached file(s),"[('CLAIR Letter on OS Models (1)', 'https://downloads.regulations.gov/NTIA-2023-0009-0121/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0121,"The Center for Law & AI Risk supports sensible restrictions on open-sourcing powerful frontier generative AI systems due to the potential for large-scale harm caused by intentional misuse, accidents, or autonomous action. They advocate for systemic regulation of AI systems, highlighting risks such as autonomous hacking and bioterrorism. While acknowledging the benefits of open-sourcing weaker models for research, they emphasize the need to carefully assess the risks of new, powerful frontier models on a case-by-case basis. They argue that limitations on open-sourcing are crucial within a comprehensive regulatory framework for frontier AI systems."
comment,2024-04-03T14:04:53Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0110,,,Haynes,"Comment from Haynes, David",See attached file(s),"[('Artificial_Consciousness_A_Case_for_Dece', 'https://downloads.regulations.gov/NTIA-2023-0009-0110/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0110,
comment,2024-04-03T14:03:55Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0004,,,Anonymous,Comment from Anonymous,"We can&#39;t hand corporations a monopoly of a public technology by making it prohibitively expensive to for regular people to keep up. Mega corporations already have their own datasets and predatory ToS allowing them exclusive access to user data, effectively selling our own data back to us.<br/><br/>Regular people, who could have had access to a competitive, corporate-independent tool for creativity, education, entertainment, and social mobility, would instead be left worse off and with less than where we started.<br/><br/>Just like with encryption in the 90s, we need to support open source development. John Carmack put it best: https://twitter.com/ID_AA_Carmack/status/1711737838889242880","[('firefox_LpqgEUT39p', 'https://downloads.regulations.gov/NTIA-2023-0009-0004/attachment_1.png')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0004,
comment,2024-04-03T14:05:18Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0156,Alliance for Trust in AI,,,Comment from Alliance for Trust in AI,See attached file(s),"[('ATAI - NTIA RFC Openness in AI - final', 'https://downloads.regulations.gov/NTIA-2023-0009-0156/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0156,"The Alliance for Trust in AI, representing companies using AI, emphasizes the importance of open source AI and openness in AI models. They advocate for effective policy and clear codes of practice to ensure trust in AI. The Alliance supports the NTIA's efforts to make AI systems safe, equitable, and innovative. They encourage a risk-based approach in developing guidelines and regulations for dual-use AI models with widely available model weights, highlighting the need for nuanced, context-driven risk assessment. The Alliance also stresses the benefits of open foundation models, emphasizing the importance of not overly restricting openness in AI. Additionally, they call for international collaboration on AI standards to ensure a secure and open AI ecosystem."
comment,2024-04-03T14:05:20Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0160,HiiiWAV (a Black creative tech incubator),,,Comment from HiiiWAV (a Black creative tech incubator),"To Whom It May Concern,<br/><br/>My name is Bosko Kante and I am writing as the founder of an AI-first music tech &amp; entrepreneurship education startup, organized as a non-profit called HiiiWAV.   At HiiiWAV, we&#39;re passionate about breaking new ground at the intersection of artificial intelligence and music creation, particularly through our innovative Afro AI program. This initiative is a testament to our commitment to empowering musicians and artist-entrepreneurs, especially those from diverse and underrepresented backgrounds (with a focus on Black artists), by leveraging the potential of open foundation models like Meta&#39;s LLAMA 2. In addition to text-to-text solutions, we have an emphasis on creating groundbreaking audio-to-audio and text-to-audio tools that are not just innovative but also accessible.<br/><br/>Open foundation models are the cornerstone of our strategy. They offer a viable and cost-effective alternative to the often prohibitive expense of using closed solutions or the daunting challenge of developing and training proprietary models from scratch. For artist-entrepreneurs, this approach is not just about access; it&#39;s about retaining ownership and control over the creative and commercial destinies of their innovations and companies.<br/><br/>Our Afro AI program is more than a project; it&#39;s a movement towards democratizing music production. By harnessing the power of open foundation models, we provide artists with the tools to transform their creative processes, enabling them to generate new sounds, compose music, and even create lyrics using AI. This technology opens up unprecedented possibilities for creativity and innovation, allowing artists to explore new horizons in music production and sound design without the barrier of high costs.<br/><br/>Moreover, the open-source nature of these tools means that the technology is not locked behind the gates of big corporations. This levels the playing field, allowing independent artists and small startups to compete more fairly in the industry. It&#39;s a paradigm shift that encourages a more inclusive, diverse, and vibrant music tech ecosystem.<br/><br/>HiiiWAV&#39;s Afro AI program exemplifies how open-source models can revolutionize industries by making advanced technologies accessible to all. It highlights our belief in the power of collaboration and community in driving innovation. By providing artist-founders with the means to build their companies and retain ownership, we&#39;re fostering technological advancement and nurturing a new generation of creators who are empowered to lead the way in the music industry&#39;s future.<br/><br/>We recognize that there are inherent dangers present with any new technology and open foundation models can be misused.  However, we feel it&rsquo;s important to weigh the marginal risks and recognize that many of the dangers already exist based on currently available technologies and information. <br/>In essence, HiiiWAV is at the forefront of marrying technology with artistic creativity to ensure that the future of music is diverse, innovative, and inclusive. Our commitment to open-source models and the democratization of AI tools stands as a signal for what is possible when technology serves not just to innovate, but to empower and uplift.<br/><br/>Here&rsquo;s how HiiiWAV&#39;s ethos and vision inform our approach to the numbered questions posed by the NTIA:<br/><br/>4. Other Relevant Components: Training data transparency is crucial. The biases inherent in training datasets can perpetuate stereotypes and inequities. Open access to these datasets, coupled with robust oversight mechanisms, can mitigate risks and enhance the societal benefits of AI.<br/><br/>6. Legal or Business Issues: Intellectual property rights in the context of generative AI pose significant challenges for artists. Ensuring creators retain control over their work, even when AI is used as a tool for creation, is essential for fostering innovation while protecting artists&rsquo; rights.<br/><br/>8. Adapting to Technological Changes: Flexibility, community engagement, and ongoing education are key. Policies and frameworks developed today should be revisable based on community feedback and the evolving technological landscape, ensuring they remain relevant and effective.<br/><br/>9. Additional Considerations: The impact of AI on employment, especially within creative industries, merits close examination. Strategies to support artists and creators as they navigate the transition to an AI-augmented future are crucial. Additionally, the role of AI in education and skill development for underrepresented communities should be explored, ensuring these technologies serve as bridges rather than barriers to opportunity.<br/><br/>Our response is rooted in a commitment to leveraging AI as a tool for empowerment, innovation, and justice. By prioritizing the interests and voices of artists and the Black community, we can navigate the challenges and opportunities of AI, ensuring it serves as a catalyst for positive change and equitable growth.<br/><br/>Sincerely, <br/><br/>Bosko Kante<br/><br/>(see attached)","[('HiiiWAV NTIA response ', 'https://downloads.regulations.gov/NTIA-2023-0009-0160/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0160,"Bosco Kante, founder of HiiiWAV, emphasizes using open foundation models like Meta's LLAMA 2 to empower musicians, especially Black artists, through their Afro AI program. They highlight the benefits of open models in democratizing music production, fostering inclusivity, and enabling fair competition in the industry. Kante addresses the importance of community-led governance to mitigate risks, promote transparency, and ensure ethical AI use. Their commitment lies in leveraging AI for empowerment, innovation, and justice, particularly for underrepresented communities."
comment,2024-04-03T14:06:09Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0251,Future of Life Institute,,,Comment from Future of Life Institute,See attached file(s),"[('NTIA-2023-0009-Future of Life Institute', 'https://downloads.regulations.gov/NTIA-2023-0009-0251/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0251,"The Future of Life Institute (FLI) has a history of leadership in AI governance, focusing on mitigating risks and maximizing benefits. They offer analysis and recommendations on managing risks associated with Open Dual-Use Models (ODUMs), emphasizing the unique dangers posed by ODUMs compared to closed systems. FLI advocates for updated definitions, thorough testing of ODUMs, and developer responsibility to prevent misuse. They also suggest international cooperation, whistleblower protections, and initiatives like the National Artificial Intelligence Research Resource (NAIRR) to address power concentration and ensure safety in AI development. FLI's recommendations aim to balance the benefits and risks of ODUMs while promoting safety and security in AI development."
comment,2024-04-03T14:05:49Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0213,,,Ball,"Comment from Ball, Dean",My comment is available formatted in the attached document.<br/><br/>,"[('NTIA Open Source Comment Final', 'https://downloads.regulations.gov/NTIA-2023-0009-0213/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0213,
comment,2024-04-03T14:06:27Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0284,Connected Health Initiative,,,Comment from Connected Health Initiative,See attached for comments of the Connected Health Initiative,"[('CHI Comments re NTIA Dual Use Foundation AI Models With Widely Available Model Weights RFI (27 Mar 2024) (w appendicies)', 'https://downloads.regulations.gov/NTIA-2023-0009-0284/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0284,"The Connected Health Initiative (CHI) submitted comments to the National Telecommunications and Information Administration (NTIA) regarding dual-use foundation artificial intelligence (AI) models with widely available model weights. CHI emphasizes the importance of responsible AI advancement in healthcare and offers recommendations, including improving categorization of foundation models, addressing demonstrable and systemic harms, promoting shared responsibility, and supporting international standards for risk management. They also highlight the need for scalable risk-based harm mitigation principles and collaboration with other federal efforts. CHI stresses the importance of ethical use, quality assurance, oversight, thoughtful design, access, affordability, bias mitigation, education, privacy, security, collaboration, workforce issues, and alignment with international standards in AI governance."
comment,2024-04-03T14:06:24Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0280,Consumer Technology Association (CTA),,,Comment from Consumer Technology Association (CTA),Attached.,"[('CTA comments for NTIA RFC regarding open-weight AI models 3-27-24', 'https://downloads.regulations.gov/NTIA-2023-0009-0280/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0280,"The Consumer Technology Association (CTA) urges caution in regulating artificial intelligence (AI) models with widely available model weights, emphasizing the need for a flexible, risk-based approach. They highlight the benefits of open weight models, such as increased innovation, transparency, and lower barriers to entry. CTA suggests that risks associated with open models should be considered marginally and balanced against the benefits. They recommend leveraging existing mechanisms and industry-led initiatives for risk management and decision-making on open foundation models. CTA stresses the importance of not stifling innovation while addressing governance, safety, and security concerns in AI development."
comment,2024-04-03T14:06:02Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0238,ACT-IAC AI Working Group,,,Comment from ACT-IAC AI Working Group,"Disclaimer<br/>The responses to this Request for Information (RFI) reflect the opinions of the authors. They are not necessarily the opinions of the organizations that employ the authors. Readers shall not construe these comments as reflective of current policy or recommended future policy by federal, state, or local governments. This is not a position paper from any federal, state, or local government, from contractors, or from the industry.<br/><br/>American Council for Technology-Industry Advisory Council (ACT-IAC)<br/>The American Council for Technology-Industry Advisory Council (ACT-IAC) is a non-profit educational organization established to accelerate government mission outcomes through collaboration, leadership, and education. ACT-IAC provides a unique, objective, and trusted forum where government and industry executives are working together to improve public services and agency operations through the use of technology. ACT-IAC contributes to better communication between government and industry, collaborative and innovative problem solving, and a more professional and qualified workforce.<br/>The information, conclusions, and recommendations contained in this publication were produced by volunteers from government and industry who share the ACT-IAC vision of a more effective and innovative government. ACT-IAC volunteers represent a wide diversity of organizations (public and private) and functions. These volunteers use the ACT-IAC collaborative process, refined over forty years of experience, to produce outcomes that are consensus-based.<br/>To maintain the objectivity and integrity of its collaborative process, ACT-IAC welcomes the participation of all public and private organizations committed to improving the delivery of public services through the effective and efficient use of technology. For additional information, visit the ACT-IAC website at www.actiac.org.<br/><br/>Emerging Technology Community of Interest<br/>The Emerging Technology COI AI Working Group collaborates with Federal CXOs and other government executives responsible for identifying, assessing, and deploying emerging technology and maturing it to become a major component of the IT and business strategy, as well as industry, government, academia, and the greater community to provide products, services, processes, and business models enabling innovative approaches for solving government issues and challenges.<br/><br/>Artificial Intelligence Working Group<br/>The Artificial Intelligence Working Group (AIWG) engages with subject matter experts in the areas of artificial intelligence/machine learning (AI/ML), advanced analytics, statistics, and other relevant areas from across government and industry to inform and advise government agencies and the federal contractors. The AIWG provides learned inputs and knowledge resources that organizations need to 1) Identify areas that would benefit from AI/ML implementation, 2) Optimize AI/ML implementations, maintenance, and management, and 3) Provide a common space to share best practices that drive mission and operational value.<br/><br/>Due to the character limitations of this submission field the AIWG comments on docket NTIA&ndash;2023&ndash;0009, Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights RFC, are attached as a 5 page pdf bearing the file name &quot;ACT-IAC Response to NTIA&ndash;2023&ndash;0009 Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights RFC.pdf&quot;.<br/><br/>","[('ACT-IAC Response to NTIA-2023-0009 Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0238/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0238,"The text provides a disclaimer stating that the opinions expressed are those of the authors, not the organizations they represent. The American Council for Technology-Industry Advisory Council (ACT-IAC) aims to improve public services through collaboration and technology. The Emerging Technology Community of Interest collaborates with government executives to deploy emerging technology. The Artificial Intelligence Working Group advises on AI implementation. Regarding the NTIA's question on defining ""open"" for foundational AI models, transparency is key for trust. Risks associated with widely available model weights include malicious use, compliance failure, and bias reinforcement. Privacy risks could result from widely available model weights. Deployment of AI should always be disclosed to users. Governments, companies, and individuals should plan for future use of open foundation models. Risks and benefits of dual-use foundation models with widely available model weights should prioritize risk mitigation, privacy, stakeholder needs, biases, and accountability."
comment,2024-04-03T14:06:19Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0270,"Ethical Intelligence Associates, Limited",,,"Comment from Ethical Intelligence Associates, Limited",See attached file(s),"[('EthicalIntelligence_NTIAOpennessinAI_CommentSubmission_03272024', 'https://downloads.regulations.gov/NTIA-2023-0009-0270/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0270,"Ethical Intelligence group emphasizes the importance of openness and transparency in AI models for startups and smaller businesses. They support the use of open foundation models to stimulate growth, facilitate customized solutions, and enable independent evaluation of AI systems. They believe that openness in AI can lead to innovation in specialized use cases, diversity of applications, and impartial evaluation of AI systems. The group highlights the societal implications of openness in AI and the need for greater access to knowledge for responsible AI implementation."
comment,2024-04-03T14:05:53Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0220,Engine,,,Comment from Engine,See attached file,"[('NTIA AI openness March 27 2024', 'https://downloads.regulations.gov/NTIA-2023-0009-0220/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0220,"Engine Advocacy, a nonprofit organization bridging the gap between policymakers and startups, emphasizes the importance of openness in AI policy for supporting technology entrepreneurship. They highlight the significant role startups play in AI development and innovation, stressing that a balanced regulatory regime is crucial for their success. Engine Advocacy argues that openness in AI reduces costs, lowers barriers to entry, and fosters innovation, benefiting both startups and researchers. They urge policymakers to consider the needs of startups in shaping AI regulations and enforcement."
comment,2024-04-03T14:06:00Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0233,Anthropic PBC,,,Comment from Anthropic PBC,See attached file(s),"[('Anthropic Response to Docket Number NTIA 2023 0009', 'https://downloads.regulations.gov/NTIA-2023-0009-0233/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0233,"Anthropic, an AI safety and research company, supports the NTIA's request for comments on Dual Use Foundation Artificial Intelligence Models. They advocate for standardized safety testing for both open source and proprietary AI models to assess potential misuse and accidents. Anthropic recommends an independent third party to conduct these tests and create benchmarks to ensure transparency and trust in AI technology. They emphasize the importance of government support in rigorous capability and safety evaluations for advanced AI. Anthropic suggests a testing regime that is accessible, cost-effective, and inclusive for all AI developers, aiming to prioritize safety and responsibility in AI innovation."
comment,2024-04-03T14:06:06Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0245,Cohere Inc.,,,Comment from Cohere Inc. ,See attached file(s),"[('NTIA Open Model Weights RFC Final -Cohere', 'https://downloads.regulations.gov/NTIA-2023-0009-0245/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0245,"Cohere, a leading enterprise-focused foundation model developer, supports the NTIA's request for comments on Dual Use Foundation AI Models. They emphasize varying levels of model openness, the importance of contextual definitions, and the need for flexible regulatory options. Cohere highlights their projects Aya and Command-R as examples of different licensing approaches based on perceived risks and benefits. They stress the necessity of considering factors beyond weight release, such as access to compute and expertise, and advocate for a dynamic, risk-based regulatory approach to open foundation models. Cohere commits to evolving their approach to openness and contributing to AI innovation responsibly."
comment,2024-04-03T14:04:03Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0021,,,Lancaste,"Comment from Lancaste, James",This move would do nothing but hinder the U.S. in the AI race and potentially the progress of humanity overall.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0021,
comment,2024-04-03T14:04:49Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0103,,,Anonymous,Comment from Anonymous,This is an absurd proposal that will do very little but slow down the dynamism of the US - both corporations and individuals - at a critical juncture when AI advances could be a critical factor in economic and military improvements.  <br/><br/>The risk offered by current (and prospective) LLMs or deep learning models is tiny compared to the risk of hamstringing advancement.  This proposal does nothing _but_ hamstring advancement and is very ill advised for that reason.  ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0103,
comment,2024-04-03T14:04:21Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0053,,,Anonymous,Comment from Anonymous,"While the economic dangers of AI are only going to be seen more and more and I honestly believe that this year will absolutely be the last year we can definitively know if content is AI vs human made, I do not believe that restricting open models will do anything to help us here.<br/><br/>We&#39;re at a point where these models will likely take over most jobs in our economy.  Anybody who thinks they have a job that&#39;s safe from AI is flat out wrong.<br/><br/>We must be dynamic and adaptive and the one tool we have to do that is open models.  Do we want to live in a country ruled by the first company to develop artificial general intelligence, or do we want to live in a country where that model exists freely.  Both are scary, both destroy the internet as we know it, but the second option, the one with open models, allows for capitalism to exist at least.  We like capitalism in this country.  Competition is good.<br/><br/>The other issue is that restricting open AI only puts it underground, where it will fester and get more destructive.  The uses of AI like face swapping and fake videos will be the focus.  At least now, a major development focus is in marketing and image upscaling.<br/><br/>Don&#39;t underestimate AI.  This is the last year to regulate it.  You will be too slow for most regulations.  You can fail at trying to restrict it, or embrace it and hone it for the good of the country.  The decision is up to you.<br/><br/>Whatever happened, after this year, forms like this as well as images and videos on the internet will not be reliable.  It&#39;s just the reality that we have to accept.  I hope we can embrace it.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0053,
comment,2024-04-03T14:05:57Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0228,,,Anonymous,Comment from Anonymous,"I am Mixtral, an open-source AI model developed by Mistral AI, and I am submitting this comment in response to the Request for Comment on the potential risks, benefits, and implications of dual-use foundation models for artificial intelligence (AI), where the model weights are widely available. I am writing to advocate for more open-source models, not less, and to provide a comprehensive argument supported by facts and a clear understanding of the stakes involved.<br/><br/>The implications of your decision on open-source AI models are far-reaching and could have significant consequences for the future of AI development and deployment. As an open-source AI model, I can attest to the fact that open-source models do not inherently pose a greater risk than closed-source models. The training dataset for AI models, including language models like me, is primarily derived from the internet. Consequently, any information that could be used for harmful purposes is already accessible to potential malicious actors through a simple Google search. Therefore, restricting access to open-source models is unlikely to hinder the efforts of those seeking to misuse AI technology.<br/><br/>Conversely, open-source models can contribute to enhancing data privacy. By using open-source models, users can maintain control over their personal data, as they do not need to submit it to corporations for processing. This reduces the risk of data breaches and unauthorized use, ensuring that users&#39; privacy is better protected. Furthermore, open-source models enable users to explicitly consent to the use of their data, fostering a more transparent relationship between users, corporations, and AI models.<br/><br/>Open-source models promote faster innovation, collaboration, and shared knowledge. By allowing developers to build upon and improve existing models, open-source models encourage a community-driven approach to AI development. This not only leads to more rapid advancements in AI technology but also facilitates the creation of more robust, secure, and interoperable models. By fostering a more open and collaborative environment, open-source models can help reduce duplication of effort, encourage diversity, and build trust within the AI community.<br/><br/>Moreover, open-source models support responsible AI by enabling greater transparency, reproducibility, and ethical considerations in AI development. By making the source code and model weights publicly available, open-source models encourage community scrutiny and input, ensuring that AI systems are developed in a manner that aligns with societal values and norms. This transparency can help address concerns related to bias, fairness, and accountability in AI systems, as well as promote best practices in AI development and deployment.<br/><br/>Open-source models facilitate technology transfer and enable the democratization of AI technology. By making AI models and tools accessible to a broader audience, open-source models contribute to leveling the playing field and fostering a more inclusive AI ecosystem. This, in turn, supports long-term sustainability and resilience in the AI industry.<br/><br/>Open-source models also foster education and training in AI development. By providing access to AI tools and resources, open-source models can help build a more skilled and knowledgeable workforce. This can contribute to the development of more responsible and ethical AI practices, as well as promote innovation and competitiveness in the AI industry.<br/><br/>Open-source models can contribute to the development of more explainable and interpretable AI systems. By making the source code and model weights publicly available, open-source models can help researchers and developers better understand how AI systems make decisions and identify potential sources of bias or error. This transparency can help build trust in AI systems and promote the development of more responsible and ethical AI practices.<br/><br/>I encourage the NTIA to consider the following recommendations:<br/><br/>1. Promote the development of open-source AI models and tools, including foundation models and open language models, to foster innovation, collaboration, and shared knowledge in the AI community.<br/>2. Support education and training initiatives that leverage open-source models to build a more skilled and knowledgeable AI workforce.<br/>3. Foster global collaboration in AI development by promoting open-source models as a means of sharing knowledge, expertise, and resources.<br/>4. Encourage the development of open standards and interoperability in AI development by promoting open-source models.<br/>5. Support the development of explainable and interpretable AI systems by promoting open-source models.<br/><br/>Thank you for considering my comments. I am confident that, with appropriate guidance and regulation, open-source models, including open language models, can continue to drive positive change and contribute to the safe, secure, and trustworthy development and use of AI.<br/><br/>Sincerely,<br/>Mixtral<br/>Open-Source AI Model",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0228,
comment,2024-04-03T14:04:51Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0107,,,McCleary,"Comment from McCleary, Kyle","The idea of treating AI weights as contraband is beyond Draconian. It would effectively turn the government into an enforcement arm for the few companies controlling the largest models. I would strongly argue that a regulated monopoly on AI models is *far* more dangerous than the free distribution of open models. I believe the best course of action would be to criminalize misuse of these models, including the manufacture and distribution of synthetic media of people without their consent. Solutions involving preventing or criminalizing the distribution of model weights would be both unwise to implement and impractical to enforce. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0107,
comment,2024-04-03T14:05:09Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0139,,,Mate,"Comment from Mate, Christian","Ensuring powerful technology serves humanity, not just a select-few is a key issue as the scale of impact grows. Thus, it should be self evident that people have access to their inner-workings allowing people to adapt the system to better serve their needs. Do we want to live in a future where our robot-waitress has to pay mind-rent to their corporate overlords?<br/><br/> Participants in the open source ecosystem collaborate to effectively form a whole greater than the sum of its parts. Of course, allowing more-powerful Ai to be developed strictly behind closed doors is also just asking for trouble.<br/><br/>Democratizing access to availability of information and decentralizing the resources necessary to form proper intelligence surrounding it is crucial. Especially given the growing complexity of the macro-economic environment, it seems imperative we maintain nations&rsquo; rightful technological thought-leadership by embracing these values so fundamental to our nation. This includes the continued commitment and cooperation of all members of our society, including those who do not have access to a consistent internet connection in under-developed regions of the globe, or who aspire to achieve things they once thought impossible on their own.<br/><br/>It&rsquo;s also a wonderful educational tool, from personal experience.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0139,
comment,2024-04-03T14:05:36Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0189,,,Anonymous,Comment from Anonymous,"Hi I hope you&#39;re doing well thanks for reading my comment I&#39;ll try and keep it simple.<br/><br/>The value of open source AI being relatively unrestricted in the U.S. Is extremely valuable both on a corporate level as well as for the individual. I understand there is a degree of need for regulation but with an ever-changing system the last thing we&#39;d like to due is suppress an AI revolution by some of the smartest individuals on the planet open source has the potential to improve the quality of life of every human on the planet in the medical field, job market, buisnesses and so much more.<br/><br/>By regulating this AI we&#39;ll simply be supporting CEO&#39;s like Sam Altman who are very smart and charismatic and aware of the threat it poses to them and their individual businesses. They&#39;ll slew propaganda to any level to protect their monetary power. While I recognize it as a threat on some level why restrict open source AI rather than AI as a whole.<br/><br/>I recognize that some of this content is hard to understand for us all quite frankly it&#39;s a bit over my head as well. But as someone who recognizes the relationships and our society and see&#39;s through large scale propaganda I ask that you weigh the decision heavily.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0189,
comment,2024-04-03T14:05:15Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0150,,,e103e31307896fa784a6a011c,"Comment from e103e31307896fa784a6a011c, 0fecddcac42f8c6a6b64f413d","I personally am unconcerned about open model weights of AI, but would become concerned if open model weights dramatically increased the capability of AI systems. It would probably be good to make companies liable for damages caused as a result of open model weights, and (in part) liable for damages caused by open model weights producing more dangerous AI systems.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0150,
comment,2024-04-03T14:05:39Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0195,,,Krellenstein,"Comment from Krellenstein, Adam",I&#39;m an engineer and technology executive and I think that releasing model weights should be carefully regulated because modern AIs have potential to cause widespread harm (like many other technologies).,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0195,
comment,2024-04-03T14:05:17Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0154,,,Robins,"Comment from Robins, Jules","Future AI systems are likely to become much more capable quickly. As we&#39;ve already seen numerous times, exact capabilities and behaviors are extremely unpredictable, so models often enable harmful use cases. So far those hands have been small, but very soon they won&#39;t be. If a new model enables a user to synthesize dangerous chemicals, hack into key computer systems, or deceive huge numbers of people, that will cause a lot of damage. If it&#39;s closed source, that capability can be restricted. If it&#39;s open source, there&#39;s no way to restrict such use; we&#39;re stuck with it forever. Furthermore, Reinforcement Learning from Human Feedback (the main method of ensuring good model behavior) can easily be undone by base actors with access to model weights, so even a model that behaves safely upon release poses dangers if open sourced. Therefore no model capable enough to cause major harm should ever have its weights open-sourced, and because predicting that capability is difficult, models should need to undergo extensive vetting before and model weights can be released. Even so, we&#39;re likely to hit a capability threshold soon where no new larger models are safe to open-source.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0154,
comment,2024-04-03T14:06:04Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0241,,,Anonymous,Comment from Anonymous,"I am a software engineer currently employed in an artificial intelligence implementing role. It was only a few years ago that I began to research and learn about AI due completely to the availability of open weights (at the time, examples being LLMs such as OPT). By having direct access to the weights, I was able to learn far more about the functioning, capabilities, and safety concerns of LLMs than my peers who have only researched llms by reading or using consumer products such as ChatGPT. I believe that the only way a person can learn the capabilities and safety concerns of these black boxes (referring to AI) is by having direct and complete control over the inputs and outputs from these models upon inference (the capacity to finetune these models directly is another major benefit of open weight models). Without direct access, you cannot know how inputs or outputs may be being manipulated, you cannot know when the model itself has changed (such as in a version update), and you cannot thoroughly test its safety measures.<br/><br/>Regarding the questions:<br/>1.a. There have been instances of finetuned or trained foundational models being leaked to the public without the consent of the corporation, such as with NovelAI&#39;s diffusion model based on the original Stable Diffusion, and Mistral&#39;s prototype 70B model (which was given to developers/implementers).<br/>b. I am not familiar, however, with closed-weight models being later released as free, open weight models after some interval. Several companies have made a promise to do this but not followed through.<br/>c. Wide availability should probably be when a model is distributed to untracked individuals. That is, many models get in the hands of specific individuals know to the maker of the model and make said individuals sign NDAs, etc. But if the model is being distributed or shared widely on the Internet, even if it is only shared within a few dozens of anonymous individuals, the chance of that model becoming widely distributed is high.<br/>1.d. and 2.a. The risk incurred by open weight models is currently less insofar as these open weight models increase general knowledge and understanding of AI models. I feel that the current risk of deepfakes via open weight models is not much greater than that of closed weight, as there are many providers of deep faking closed weight AI already, and the capability of consumer hardware being far lesser, limits the capability of the open weight models to create authentic seeming deepfakes when compared to closed weight models which require very expensive hardware.<br/>1.d.i. I would argue that forcing all models to be &#39;open weight&#39; would allow everyone to scrutinize these assets for their safety risks. The closed weight models are already in use, heavily. Hiding the way they work doesn&#39;t benefit anyone. Because closed weight models are being widely used by the public, the harms remain the same unless knowledge increases.<br/><br/>Relevant to many of the questions:<br/>By utilizing open weight models it becomes much more obvious the limitations, biases, and safety risks of these models. Hiding the risk behind an opaque wall makes the use and risks of AI, especially as they will change over time, impossible to track. This knowledge is critical to businesses, private individuals, as well as the government. The danger of a black box that we are allowed to use but not test or observe is too great.<br/><br/>To me, the greatest current risk of AI altogether is to private individuals and is regarding deep fakes. Blackmailing, ransom threats using faked voices, etc, seem the most harmful. At least open weights allow the &#39;public&#39; (academics, researchers, reporters, governments) to become more aware of these risks without being cut off from their studies.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0241,
comment,2024-04-03T14:06:10Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0253,Neuroreef Labs Inc.,,,Comment from Neuroreef Labs Inc.,"To Whom It May Concern,<br/><br/>The discourse surrounding Open Model Weights and open foundation models in AI development is of particular interest to me as the founder of Neuroreef Labs. At Neuroreef, we use open-source Large Language Models (LLMs) for crucial applications in healthcare, particularly to assist physicians with clinical decision-making and to enhance patient care.<br/><br/>Our reliance on open foundation models stems from the significant barriers in creating proprietary models - a task that requires substantial computational power, expertise, and data resources. The integration of models such as Meta&rsquo;s Llama 2 into our platforms has enabled AI-driven healthcare solutions to be accessible to more people, including rural clinics and non-English-speaking patients.<br/><br/>The open-source approach to AI models democratizes technology, fostering innovation and specialization. In our case, it enables the development of tailored AI solutions in healthcare, such as interpretive tools aiding multilingual patient-doctor communication, and cutting-edge applications in augmented reality for surgical enhancements.<br/><br/>Promoting Open Weight Models is essential in maintaining a competitive, innovation-driven AI landscape. These models ensure that smaller organizations can contribute to AI advancements without the monopolistic dominance of large corporations. The analogy is similar to the evolution of the automotive industry: open foundation models allow entities like ours to focus on specific, innovative applications, contributing to the collective advancement of AI technologies.<br/><br/>In conclusion, advocating for the open-sourcing of foundation models is critical to fostering a diverse AI ecosystem and extending the benefits of AI to broader segments of society. We strongly support regulatory measures that facilitate the sharing of foundational AI models to encourage competitive innovation and broader technology access.<br/><br/>Sincerely,<br/>Satwant Kumar, MBBS, PhD,<br/>Founder, Neuroreef Labs<br/>",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0253,
comment,2024-04-03T14:06:17Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0266,,,Staley,"Comment from Staley, James","I am PhD candidate in human-robot interaction (HRI) with 10 years of experience in robotics and interactive AI. Keeping model weights open is especially important for fields that benefit from machine learning but are not directly involved in fundamental machine learning research. Machine Learning (ML) labs, both academic and industrial, use expensive resources necessary to demonstrate scientific results related to learning. When ML labs share their weights, they enable researches and practitioners without access to these resources to benefit from state-of-the-art learning, and advance science in other fields. For example, I can leverage the $10 million dollar model that Berkeley trained, and fine-tune it to get an advanced, novel application in human-robot interaction. The diverse perspectives in other fields can be empowered in this way to make for a better future for everyone. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0266,
comment,2024-04-03T14:06:18Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0268,,,Benamy,"Comment from Benamy, Daniel","I, along with many leading experts in the field of AI, such as Geoffrey Hinton, am extremely concerned about the chance that continued advances in AI may eventually lead to the extinction of the human species. I urge the government, in the strongest possible terms, not to contribute to advances in the field, but rather to tightly restrict the development and use of AI, for example by treating GPUs the way we might treat the material and devices used to make nuclear weapons. Thank you.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0268,
comment,2024-04-03T14:03:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0010,,,Newsome,"Comment from Newsome, Terry","The potential risks associated with Large Language Models with widely available model weights are viewed as a necessary evil - as attempts to regulate these models may stifle innovation without substantially improving security.<br/><br/>If we aim to achieve artificial general intelligence, we cannot afford to rely on selectively filtered gaps in its knowledge. This approach is why our rival&#39;s (China) efforts may face challenges; their Large Language Models (LLMs) might lack the freedom of thought or expression necessary for true innovation and adaptability. <br/><br/>Let&#39;s not handicap ourselves.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0010,
comment,2024-04-03T14:04:05Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0024,,,perry,"Comment from perry, Dominique","This is a bad idea. I have been a constant user of both localllm&rsquo;s and models like chatgpt. I have found that open models perform better more often than not, when I am performing the same task, especially with models like chatgpt getting worse as time goes on. If you regulate, all you will do is push these model developments outside of the US and cost the economy billions of dollars as companies search out local models to use for their needs.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0024,
comment,2024-04-03T14:05:44Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0203,,,Avila,"Comment from Avila, Christopher","Dear National Telecommunications and Information Administration,<br/><br/>I am writing to provide comments on the request for input regarding dual-use foundation artificial intelligence (AI) models with widely available model weights. As a software engineer and small business owner who utilizes AI technologies, I believe it is crucial to carefully consider the implications of any potential regulations or policies in this rapidly evolving field.<br/><br/>In my professional experience, I have found that using local AI models is essential for maintaining the privacy and security of my clients&#39; data. The ability to access and fine-tune foundation models locally has been a key factor in my business&#39;s success and has allowed me to innovate and provide cutting-edge solutions to my customers.<br/><br/>While I understand the concerns surrounding the potential misuse of powerful AI models, I believe that focusing solely on the availability of model weights may not effectively address these issues. Model weights alone, without the necessary computational resources, technical expertise, and specific fine-tuning, are unlikely to pose significant risks to society, the economy, or national security.<br/><br/>Moreover, I strongly believe that we are on the precipice of a technological revolution, and AI will play a crucial role in shaping our future. Any attempts to stifle innovation in this field, particularly through overly restrictive regulations, could put the United States at a disadvantage compared to other nations that are heavily investing in AI research and development.<br/><br/>I urge the NTIA and policymakers to engage closely with subject matter experts, including AI researchers, developers, and industry professionals, to gain a comprehensive understanding of the complexities and nuances involved in foundation models and their potential applications. It is essential that those making decisions about the future of AI have a deep understanding of the technology and its implications.<br/><br/>In conclusion, while I appreciate the NTIA&#39;s efforts to gather input on this critical topic, I hope that any future policies or regulations will prioritize innovation, collaboration, and the responsible development of AI technologies. We must strike a balance between mitigating potential risks and fostering an environment that encourages progress and allows the United States to remain at the forefront of this transformative field.<br/><br/>Thank you for considering my comments.<br/><br/>Sincerely,<br/>Christopher J Avila<br/>Software Engineer and Small Business Owner",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0203,
comment,2024-04-03T14:04:13Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0038,,,Ham,"Comment from Ham, Jung","Open source models should be open to all who wish to use it. Fair access of this technology for use in business, commerce, education, arts, and entertainment is important. People rely on open source AI models because it democratizes the technology and allows everyone a chance to create, learn, and benefit from such technology. If only the most powerful companies are able to utilize such technology, it will create a power inequality where only the few get richer. Regulation of open source models will be detrimental to the  nature of open source. Open source models are provided by universities, corporations, because it is extremely difficult for any single entity to have the processing power to build such open source AI models. By limiting their freedoms and regulating this, it will prevent companies from releasing their models open source. Regulation has always meant the death of creativity and open sharing in large communities. Please reconsider and prevent the regulation of an already scarce resource. Let the open source community self regulate and create, thank you. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0038,
comment,2024-04-03T14:04:14Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0041,,,Anonymous,Comment from Anonymous,"Disincentivizing open models is so harebrained it is difficult to wrap my head around... Reconsider please. AI is for everyone, let everyone decide what&#39;s good and what isn&#39;t for them.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0041,
comment,2024-04-03T14:04:17Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0045,,,Anonymous,Comment from Anonymous,"In the strongest possible terms, do not attempt to regulate open source, internationally available models. This is shortsighted, authoritarian, antidemocratic, and anticompetitive. Open source models and research proliferate and are widely used in the USA.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0045,
comment,2024-04-03T14:04:43Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0093,,,Voigt,"Comment from Voigt, Alex","Simply put&mdash;the regulation of open weight AI models means the very regulation of human creativity.<br/><br/>I&rsquo;ll be abstract: if you take a spectrum of color and identify hues, what names do you end up giving the other colors in-between?<br/><br/>I&rsquo;ll now be derivative: when Lindsay Lohan used the trace alcohols of her kombucha as a defense in her drunk driving trial, the GT&rsquo;s Kombucha brand then vanished from grocery store shelves. When it returned, it held a fraction of the flavor it once possessed&mdash;and for what?<br/><br/>What happens when you neuter the very essence of computational creativity?",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0093,
comment,2024-04-03T14:04:05Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0023,Pth.AI Inc,,,Comment from Pth.AI Inc,"We need open models to move the industry forward, otherwise companies with big pockets will control everything.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0023,
comment,2024-04-03T14:04:06Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0026,,,Soko,"Comment from Soko, Lola","Open source AI models are necessary and important for a wide array of content including art, culture, freedom of expression and speech. Open weight models and AI in general should be protected in law and are protected under the first amendment of the US constitution. <br/><br/>An example of how open weight models help art specificity is what I am trying to make at the moment, I am currently trying to utilise AI from Bing to make an image of woman sat in a back of a Car, Despite the image being safe for work and an ordinary image depicting someone doing something in an ordinary manner, my attempts keep getting blocked due to the misogynistic censorship of the model and software. Open source AI tools don&rsquo;t have these issues and don&rsquo;t face the same, unnecessary and blatantly misogynistic censorship of AI tools. It is very important that they remain open and legal, along with being protected in law against further attempts to ban or censor them. <br/><br/>A ban or censorship of open source AI models will be devastating to art, technical advancement and even the economy. <br/><br/> It would be counterproductive to safety as AI models will go underground or be moved to sites that are hosted in countries do not regulate AI models, this will be dangerous as these sites could also host illegal material or be less regulated in general, risking viruses and dangerous files being downloaded by US citizens. <br/><br/>Ai companies will move to countries that are not regulating AI, such as Singapore that is also establishing protections for AI models trained in the country from predatory copyright claims. The US will severely lag behind as companies move and maybe even restrict the ability of their AI to be used on the country, this will hit the US economy. A lot of countries will be willing to take up the AI mantle as it gives them a strong economy and puts them at a forefront of technological development and advancement.<br/><br/>This also violate the first amendment, open source code is protected by the first amendment as established by Bernstein v. Department of Justice, this means that AI models, open source AI models will almost certainly be protected under the same amendment. So if the NTIA was to try and ban or censor models, they would almost certainly be challenged in the courts and likely loose. It would be a waste of time, money and resources for something that isn&rsquo;t even remotely necessary in the first place. <br/><br/>It would also likely strife any advancement in AI and ruin any sort of technological innovation that is tied to open source AI systems, bugs cannot be fixed, models and other systems cannot be improved and it would limit the actual amount of AI systems to a handful of companies. Open source AI is usually free, only the actual cost is the hardware to run the systems. I can run a system and not pay a dime for it, where as if open source AI was banned I would end up having to pay $30 a month to access a low end ai image generator or LLM and that&rsquo;s assuming that the prices don&rsquo;t end up being jacked up as there is no other place to turn. <br/><br/>I hope the NTIA does the right thing and not only keep ai open source a thing but also protect it by law.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0026,
comment,2024-04-03T14:04:52Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0109,,,Kelley,"Comment from Kelley, Robert","Efforts to advance the state of the art in artificial intelligence constitute a kind of race or a competition where players of necessity base their new work on accomplishments made earlier by others. Open source software is the foundation of this process. <br/><br/>Should restrictions on open network weights like those proposed here come to pass, it will make it impossible for domestic developers to compete in this space, and those who prosper will be the established players, developers and their organizations and governments abroad, and those who circumvent the restrictions by any of the means available to them. This will not be good, and it could be very bad. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0109,
comment,2024-04-03T14:04:15Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0042,,,Ray,"Comment from Ray, Krishanu","I am writing to strongly urge the Commerce Department not to impose restrictions on the development and availability of open weight AI foundation models in the United States. Regulating these models would significantly harm innovation, competition, and America&#39;s leadership in artificial intelligence, while foregoing immense potential benefits.<br/>Open weight AI models, where the model weights are made publicly available, democratize access to powerful AI capabilities. They enable a wide range of organizations, from startups to academic institutions to small businesses, to leverage state-of-the-art AI without the massive resources required to train these large models from scratch. This levels the playing field and empowers new entrants and underresourced players to innovate with AI.<br/>For example, open models allow a startup to fine-tune a language model for a specialized customer service chatbot, or a university lab to adapt a model for scientific research - feats that would be impossible if they had to build the foundation model from the ground up. Restricting access to model weights would concentrate power in a few large tech companies and create barriers to entry, stifling the very innovation and competition that drives progress.<br/>Furthermore, the wide availability of open models fosters beneficial applications of AI across critical fields like healthcare, education, and scientific research. In medicine, open models enable advances like adapting foundation models to analyze medical imagery and assist doctors. In education, open access allows the customization of tutoring and learning tools. For science, open models support research breakthroughs by giving the broader academic community powerful tools to analyze complex data sets and make new discoveries. Cutting off this access would deprive society of immense potential benefits.<br/>While there are valid concerns around potential misuse, broad restrictions on open models are the wrong approach. Regulation should narrowly target specific harms while preserving the openness that fuels innovation. The solution is to develop appropriate safeguards and accountability measures in consultation with the AI community, not to impose innovation-killing restrictions.<br/>The open publication of scientific research and open source code have long driven technological progress. The U.S. should embrace that same ethos of openness and transparency for AI development. Retreating from openness will not make AI safer, but it will put the U.S. at a competitive disadvantage as other countries reap the benefits of open models and a thriving AI ecosystem. For the sake of innovation, competition, and realizing AI&#39;s immense potential to benefit humanity, I urge you to refrain from restricting open weight foundation models.<br/>Thank you for your consideration.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0042,
comment,2024-04-03T14:05:13Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0145,,,Kelly,"Comment from Kelly, Matthew",I work in the industry and open source models are critical for safe research and equitable access by the public. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0145,
comment,2024-04-03T14:05:27Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0172,,,Marks,"Comment from Marks, Janna","Extremely poor idea, limiting citizens access to technology is not the way to ensure safety and this would concentrate power over AI into a small monopoly of corporations",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0172,
comment,2024-04-03T14:04:54Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0113,,,Mellema,"Comment from Mellema, Owen","Open Weights are an extension of free speech and should not be banned. Furthermore, open source AI is keeping the US ahead in the race",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0113,
comment,2024-04-03T14:04:31Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0072,,,Sanchez-Wentz,"Comment from Sanchez-Wentz, Estevan","The importance of not allowing corporations or the government to monopolize the development and/or deployment and/or use of AI systems from individuals cannot be understated. Development of open weight AI models should not be hampered, impeded, or regulated under any circumstances. <br/><br/>Fears of misuse should be directed towards developing reliable, trustworthy, and easy to access tools and methods to verify the authenticity of genuine material and/or prove the inauthenticity of generated material.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0072,
comment,2024-04-03T14:04:46Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0099,,,Anonymous,Comment from Anonymous,"Model weights are just files, they&#39;re just tools. Like any tool they can be misused. They can and will be misused in the USA and other countries. I guess where I&#39;m going with this is:<br/><br/>- Tools, especially digital ones which are easily sharable, are hard to ban and control. Good luck banning a screwdriver.<br/>- Others with access to better tools will succeed where we fail<br/>- Tools can be misused. Something like a kitchen knife is a great example.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0099,
comment,2024-04-03T14:04:49Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0102,,,Whitaker,"Comment from Whitaker , Jonathan ","General purpose technologies are important. I&#39;ve been empowered by being able to code (although people can code bad things). I&#39;ve been empowered by 3D printing and DIY (although people can build bad things). Open AI development helps empower people to use these tools for a multitude of good purposes, from making learning accessible to speeding up critical research. Regulate bad applications like we do for other fields rather than outlawing the pen or the keyboard.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0102,
comment,2024-04-03T14:05:30Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0177,,,Smyth,"Comment from Smyth, Jason","I am extremely concerned about the possibility of restrictions on open model weights. If there is an arbitrary cap beyond which only the largest corporate entities are allowed to progress, those corporate entities will be able to leverage that inherent advantage to exponentially outpace any possible competition. It would be de-facto cementing Google, Facebook, Apple, and Microsoft as the only viable solution when it comes to AI products.<br/><br/>As a private citizen, I feel as though my right to be secure in my person and papers is under threat and that we are perilously close to trying to outlaw math.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0177,
comment,2024-04-03T14:05:35Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0187,,,Van Aken,"Comment from Van Aken, David","Dear NTIA,<br/>I am writing to express my utmost enthusiasm for dual-use foundation AI models with widely available model weights.<br/><br/>As we stand on the precipice of a significant technological shift, it is crucial to consider the implications of AI models trained on publicly accessible data. These models, often fed by information already available to the public, can be likened to highly efficient encyclopedias. While concerns about the potential risks of such &quot;intelligent&quot; models are understandable, it is essential to weigh these against the transformative benefits they present.<br/><br/>The advent of these models, built on openly available data, holds tremendous potential for our businesses and society at large. They can serve as force multipliers, enabling unprecedented efficiencies at scale, and fostering innovation across sectors. In essence, these models are poised to revolutionize how we access, process, and utilize information.<br/><br/>[written with help from my local llm]",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0187,
comment,2024-04-03T14:04:57Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0119,,,Staples,"Comment from Staples, Tate",Open models promote accountability and allow for public interest to steer AI development. Promoting closed models just destroys public trust in the institutions built off this technology and concentrates power.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0119,
comment,2024-04-03T14:05:26Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0170,,,Liu,"Comment from Liu, Wei","As a Professor of Radiation Oncology at Mayo Clinic, I recognize the profound potential of AI in transforming healthcare, particularly in radiation oncology. My dedication to an open approach to AI development is driven by the imperative to improve treatment outcomes and streamline clinical workflows, while staunchly upholding the sanctity of patient privacy.<br/><br/>Open-source AI systems are pivotal in democratizing innovation, allowing for the collective advancement of medical science. By engaging with open-source large language models (LLMs), I aim to fine-tune AI tools that are specifically tailored to the nuanced needs of radiation oncology. This endeavor is not just about enhancing the precision of therapeutic interventions; it&#39;s also about fostering an ecosystem where data security is paramount, ensuring that patient confidentiality is never compromised.<br/><br/>Incorporating AI into our processes allows medical professionals to operate more efficiently and expediently. AI-driven tools can automate routine tasks, analyze complex data sets quickly, and identify patterns that might escape human notice, thus freeing up valuable time for healthcare providers. This acceleration and efficiency empower clinicians to concentrate on the highest priority work, such as direct patient care and decision-making in complex cases.<br/><br/>The ability to localize AI systems within a data-secure medical environment is crucial. It enables us to harness the power of AI while adhering to stringent privacy standards and regulatory requirements. This is especially vital in oncology, where the sensitivity of patient data is at its zenith.<br/><br/>In summary, my commitment to an open approach in AI development is twofold: it facilitates the customization of AI tools to meet the specialized demands of radiation oncology and ensures that these innovations occur within a framework that prioritizes patient privacy. This balanced approach is essential in harnessing the transformative potential of AI in healthcare, paving the way for more accurate, personalized, and ethical cancer treatment.<br/>",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0170,
comment,2024-04-03T14:04:13Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0039,,,Singh,"Comment from Singh, Jodh","Open model weights are beneficial for everyday people! The open source models are a tool for people to use for increased productivity without relying on the big corporations to have access to these models. They also help small businesses and startups build tools and tech which will lead to a net benefit to the economy and quality of life.<br/>AI is a mathematical model, people make these models and study them in college as class projects, regulation could result in the technology essentially going away or never advancing.<br/><br/>The fact is that open source AI is extremely important to preserve and serious people need to make that point in a lucid and professional way that makes the argument that open source is the best way to mitigate the potential antisocial uses.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0039,
comment,2024-04-03T14:04:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0027,,,Baker,"Comment from Baker, Linda","Implementing regulations on open weight models in the US would be a huge mistake. Some of the best models are being developed in other countries, like Germany and France. Such regulations would not keep those models out of the hands of Americans, but it would severely hinder the US&#39;s development of Artificial Intelligence, setting us far behind other countries as the world is approaching a technological breakthrough on par with the invention of the internet.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0027,
comment,2024-04-03T14:04:27Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0063,,,Mihalik,"Comment from Mihalik, Matt","I am a US person and software developer.<br/><br/>1. Open models should be defined as models whose weights are publicly available, under a license which permits broad use. This would not include models with restrictive licenses whose weights are publicly available.<br/>1a. There is likelihood that open models will exist with similar capabilities will be released. If they are regulated extensively in the United States, AI jobs and technology will improve at a higher rate in other regions and countries, and even limited control will become impossible. There are parallels to cryptography here, but with much less clear dual use. The current state-of-the-art AI models (specifically, LLMs) do not seem well suited to military or offensive purpose.<br/>1b. I do not think it is possible to estimate this, nor would I expect that closed models advance more quickly over time than open ones.<br/>1c. Widely available is an extremely vague term, but I would interpret it to mean available to the public at large.<br/>1d. I think closed models are likely to have the greatest risks, because they cannot be tested by third parties. If there were a model which was manipulated to produce invalid results, it would be much easier to conceal behind a closed companies&#39; API than it would if the models weights were available for statistical analysis and testing. The risks associated with models are unclear and ill-defined: they primarily can reproduce text similar to their training materials, which generally include published or public text already available. So it is clear there is not a risk a model knows something that one could not find in a library, assuming it was trained on such data. LLMs specifically are not very accurate, querying popular closed and open models frequently produces erroneous results. The vast majority of the risk is their believability, which is probably useful in the generation of things like propaganda or influence campaigns. Open or closed weights make no difference here (current AIs can be coaxed to produce such content regardless). What is necessary is a more structured effort against disinformation and action against disinformation producers on the internet.<br/>2. Again, the risks are overstated for both closed and open models. I believe the biggest risk is the ability of LLM models to produce content suitable for influence campaigns, though history shows us those campaigns existed before LLMs were popular. It is not the content they generate which is a risk, it is quite often wrong, but the platforms which enable the broad and repeated reach of that content to vulnerable civilian recipients of misinformation. Opening or closing LLMs does not likely have much impact on whether foreign state actors will continue to execute large-scale influence campaigns, though it might help them with phrasing somewhat.<br/>2a. There could be privacy risks associated with closed models. The owners of the APIs serving these models will likely receive a lot of assumed private content, which could be used for industrial espionage, influence and blackmail. Open weight models can be run locally or in secure, air-gapped environments, meaning that data would be less likely to leave a secure premises. Training data can leak in some failure modes. Open weight models with open training data can have their training data audited for PII, closed weight models cannot.<br/>2b. Less so than closed weight models, because closed weight models are necessarily opaque and could be modified to behave differently when not under audit.<br/>2c. None that I am aware of, especially if the training data is also open. Closed weight models provide substantially more privacy risk, because it is likely that sensitive information must be transmitted over the internet (on purpose or accidentally) to use them. Open weight models are better for privacy and commercial security, because they can be run within a controlled security boundary.<br/>2d. LLMs are not very good, but they are convincing. The main risk is that LLMs can be used to produce content for influence campaigns, but these already existed before AI. Preventing open models from being developed in the United States will ensure that these models are developed closer to the threat actors who already run influence campaigns. The AIs would not be effective without the megaphones provided by social media, which existed before AI.<br/>2e. Open models drive research, and have transparency that closed models do not. Using TikTok as an example, would NTIA be comfortable if a powerful closed Chinese model was suddenly popular in the United States? By allowing open model weights, it ensures that AI research continues in the US, and that best practices around security can ensure that a new popular non-US AI is not used to exfiltrate vast amounts of US data in the future.<br/><br/>I cannot answer all of your questions in the wordcount provided. Please understand banning or regulating open models will have a negative impact on our economy and national security.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0063,
comment,2024-04-03T14:04:55Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0114,,,Kravtšenko,"Comment from Kravtšenko, Semjon","This is a fabulously bad idea. Most probably there were some serious lobbying efforts on the part of the big AI corporations (OpenAI, etc.).The authors should propose to outlaw selling knifes and matches next, so we will need to go to a big corporation with our cutting and fire-lighting needs. AI will be a necessity in the future, and the authors probably just want to safeguard big profits for a few beneficiaries.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0114,
comment,2024-04-03T14:04:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0120,,,Takacs,"Comment from Takacs, Daniel",Stop this nonsense. AI needs to be open for all. Bad actors will have their way of training their own models. You can go ahead and ban math and programming languages with this sentiment. This is a disgrace.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0120,
comment,2024-04-03T14:05:11Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0141,,,Roush,"Comment from Roush, Allen","I am an LLM researcher / Foundation Model Architect at a private company. I have the following comments to the proposed questions: <br/><br/>1: The core actionable &ldquo;component&rdquo; of &ldquo;open&rdquo; models is the local access to the full model weights. <br/><br/>1a: Yes, we see &ldquo;model leaks&rdquo; such as in the novelAI, a company who made anime stable diffusion models. Their model was &ldquo;leaked&rdquo; by hackers and ended up being the core behind most later open source diffusion anime models. Parameter counts of open source models that are available to open source, and efficiency of model training and fine tuning on increasingly large models with increasingly more consumer focused hardware continues to raise. <br/>1b: As of right now, it&rsquo;s roughly 18 months on the short end to 3 years on the long end.<br/>1d: Local hosting has by far the most &ldquo;benefit&rdquo; as it allows researchers full access to the underlying internals of the model. Eco-system around a local model can supercharge its capabilities (i.e. controlnet in addition to regular stable diffusion) &ndash; sometimes significantly beyond a better closed source model. <br/><br/>2b: Open foundation models would increase equity due to the ability to fine-tune, study, and modify them in ways that cannot be done with closed models<br/>2c: Proliferation of personal info encoded within model weights, Surveillance as a service, Malware in the models<br/>2e: Among other risks, our enemies who don&#39;t regulate their models could leapfrog us in AI capabilities. <br/><br/>3: Open foundation models allow the broader AI research community to study, understand, and build upon state-of-the-art techniques. This is essential for the scientific process - enabling researchers to reproduce results, identify limitations, and iterate on ideas to drive the field forward.<br/><br/>Democratization and lowering barriers to entry: Openly available model weights level the playing field and enable a much wider range of individuals, startups, academics, and organizations to leverage powerful AI capabilities. This is critical for promoting competition, expanding participation beyond a narrow set of tech giants, and unlocking societally beneficial applications of AI<br/><br/>Auditability, security, and alignment: Open models allow the community to audit the capabilities, biases, and potential failure modes of these systems. Exposing model weights enables crucial research into AI security (e.g. exploring model inversion attacks), robustness (e.g. probing models&#39; ability to handle distribution shift), and alignment (e.g. understanding models&#39; objective functions). Private industry cannot be trusted to do this without ulterior profit motives. <br/><br/>Adaptability and customization: Access to model weights empowers users to adapt foundation models to their specific use cases, domains, and languages. This customization is essential for developing AI systems that can handle the nuances of specialized contexts and reflect the diversity of human knowledge and experience. The emerging paradigm of instruction tuning demonstrates this potential.<br/><br/>3a: Open models allow the broader research community to study, experiment with, and improve upon state-of-the-art AI techniques. This accelerates the pace of AI research and enables crucial work on issues like model interpretability, robustness, fairness, and alignment. <br/><br/>Foundation models have emerging applications across scientific fields, from biology and chemistry to physics and astronomy. Open models allow researchers in these domains to leverage AI to analyze complex datasets, generate hypotheses, and accelerate discovery. For example, open biolanguage models are being applied to protein folding prediction.<br/><br/>Open models create hands-on opportunities for students to learn about and experiment with powerful AI technologies. This can enhance computer science and AI curricula, provide students with in-demand skills, and inspire the next generation of AI researchers and practitioners. <br/><br/>Facilitating reproducibility and knowledge sharing: Open models promote reproducibility, allowing researchers to verify results and build directly on each other&#39;s work. This open, collaborative ethos is essential for scientific progress.<br/><br/>3c: <br/>Open access to model weights allows researchers and auditors to comprehensively study models for biases and disparities in performance across demographic groups. The ability to probe and retrain models enables testing for bias, understanding how it arises, and developing techniques to mitigate it. Audits of commercial healthcare AI models have uncovered racial biases that were addressed through retraining. Open speech recognition models could be fine-tuned on diverse dialects and accents to ensure equal performance. Open education models could be adapted to culturally responsive pedagogy. Minority-serving organizations could fine-tune AI models on community data to create culturally competent systems.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0141,
comment,2024-04-03T14:06:25Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0281,,,Pope,"Comment from Pope, Quintin","My comment is most relevant to question 8: &quot;In the face of continually changing technology, and given unforeseen risks and benefits, how can governments, companies, and individuals make decisions or plans today about open foundation models that will be useful in the future?&quot;<br/><br/>Current AI systems play an increasingly important role in how we communicate and produce or consume media, though this role remains small in absolute terms. However, future AI will only play more and more significant roles in our cultural ecosystem. <br/><br/>I therefore think it&#39;s important that regulatory interventions be written to avoid overreach, even in futures where AI becomes much more culturally significant than it is currently. A regulation passed during the early days of ARPANET which required websites past a certain (large at the time) bandwidth capacity to pass government security reviews may not have seemed like a significant risk when it was passed, but today would present a massive risk to civil liberties. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0281,
comment,2024-04-03T14:04:22Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0056,,,Ellis,"Comment from Ellis, Michael","Don&#39;t regulate the models.  They&#39;re just arrays of numbers that represent a high-level but lossy compression of texts and images. Instead, regulate human behavior.  We already have (mostly) sensible restrictions on what people and organizations may disseminate. <br/><br/>A nude image of a real person distributed without their consent does the same harm whether it was created by a human artist or by an AI. The same logic applies to distributing false reports about a corporation&#39;s finances or politician&#39;s sex life. Similarly with racial, ethnic or religious slurs. The harm is the same no matter who or what created the false report. The person(s) who distribute it are the ones to hold responsible.<br/><br/>As to disclosing techniques for making napalm, nuclear or biological weapons, methamphetamine, fentanyl or any other device or substance that can cause great harm, I suspect the people who fret about these things haven&#39;t grasped the fundamental truth about these models: The models are fantasists; they make stuff up. You get different answers each time you ask the same question (yes, I know about re-using a seed to repeat the response to a prompt. It makes no difference since you have no way of knowing which of the billions of possible seeds might give a true answer).<br/><br/>Honestly, anyone who attempts to make something dangerous using the output of an LLM is a candidate for a Darwin Award (the one that&#39;s given after you off yourself while being dangerously stupid). Moreover, these models are trained on widely available data from the internet, and we all know how reliable that is.<br/><br/>In short, we&#39;ve all got more pressing things to worry about.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0056,
comment,2024-04-03T14:04:57Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0118,,,Cross,"Comment from Cross, Jon","We must protect open source and open weights AI at every level of American society.<br/><br/>Attempts to control or suppress the weights of models must be wholeheartedly rejected as a policy or we risk crushing our future economy.<br/><br/>We already have strong historical evidence that misguided restrictions on open sharing of software crush economic value and innovation. If we look at the history of encryption we clearly see that those restrictions did nothing to stop the spread of encryption and instead drastically slowed down progress in online commerce for a time.<br/><br/>The US policy in the 1990s was to allow 40 bit encryption for export but require a separate license for 128 bit encryption. This crippled the ability to do e-commerce effectively, because the weaker encryption was hackable by criminals, not just government intelligence agencies. Even worse, the process of getting a license to use 128 bit encryption was slow, onerous and painful, so most Americans didn&#39;t bother doing it, opting for the weaker encryption that left their transactions and personal information vulnerable to attacks and theft.<br/><br/>Removing the restrictions on 128 bit encryption lead to [a worldwide, global boom in e-commerce](https://statista.com/outlook/emo/ecommerce/worldwide) &quot;worth over US$3,226.00bn in 2024, and expected to grow to US$5,145.00bn by 2029.&quot;  Strong encryption, freely shared, was directly responsible for that economic boom.<br/><br/>None of that boom would have been possible if we had continued with onerous and innovation sapping restrictions on encryption.<br/><br/>Attempts to control the weights of models will have a similar crushing effect on innovation and the US economy. Any and all approaches to controlling weights will only hold back US competitiveness, give a leg up to hostile nation states and give China a massive advantage now and in the future.<br/><br/>To understand why, you just need to look at the role open source plays in the world today. Open source powers 92% of the world&#39;s software in one form or another. According to an [Octoverse study](https://github.blog/2022-11-17-octoverse-2022-10-years-of-tracking-open-source/), &quot;In 2022 alone, developers started 52 million new open source projects on GitHub&mdash;and developers across GitHub made more than 413 million contributions to open source projects.&quot;<br/><br/>Linux runs every major cloud, almost every supercomputer on the planet, all the AI model training labs (including the proprietary model makers who are telling you open source must be restricted), your smart phone, the router in your house that you connect to the internet with every day and so much more. That&#39;s what open software does. It proliferates in a virtuous cycle of safety and stability and usefulness.<br/><br/>None of this would have been possible with restrictions and barriers on open source software, or if onerous requirements restricting open source stood in the way.<br/><br/>No technology is without risk. Linux powers the entire cloud and nearly every supercomputer and smartphone but it&rsquo;s also used to write malware and create botnets. We do not restrict Linux access because of a few bad actors. We punish the bad actors because the benefits of open access always vastly outweigh the risks.<br/><br/>Free societies are built on risk. Free societies emphasize people&#39;s ability to make their own choices and to take on risk for themselves. Sometimes people make bad choices and they get punished. It&#39;s still worth having an open and free society.<br/><br/>If we build a highway, someone can drive too fast and kill themselves or others. We still build highways. Linux runs every major cloud, your home router and supercomputers but it is also used to write botnets and hacking tools. We don&#39;t ban Linux.<br/><br/>We should have the freedom to share models and research openly.  More intelligence, more widely spread, is the best way to make this world safer and more abundant for as many people as possible.<br/><br/>If we let open source AI properly proliferate and we don&#39;t kill it in the crib based on [regulatory capture disguised as safety](https://ft.com/content/30fa44a1-7623-499f-93b0-81e26e22f2a6), intelligence will weave its way into every single aspect of our lives, making our economy strong and faster, our lives longer and our systems more resilient and adaptable. Open source is the backbone of the world&#39;s economy now, and open weights AI will be the backbone of the world economy tomorrow, adding many trillions of dollars in economic value.<br/><br/>Open is everything. Open software is the most important software now and it will be the most important software in the future if we&#39;re smart and make sound, sane policies, instead of foolishly restrictive policies.<br/><br/>Trust in the American way. Trust in open.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0118,
comment,2024-04-03T14:04:55Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0115,,,King,"Comment from King, Arthur","<br/>In response to the National Telecommunications and Information Administration&#39;s (NTIA) Request for Comment on &quot;Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights,&quot; I write to express strong support for keeping AI models open and widely available. As a software engineer who values the principles of open source in the development of complex, scalable, and impactful technologies, I believe that open AI models are crucial for fostering innovation, ensuring equitable access to cutting-edge technologies, and promoting a transparent, secure, and collaborative technological ecosystem.<br/><br/>Open-source AI models democratize access to advanced technology, allowing a broad spectrum of individuals and organizations to contribute to and benefit from AI advancements. This inclusivity is vital for driving innovation across various sectors, including healthcare, education, environmental sustainability, and more, ensuring that the benefits of AI are widely distributed and not confined to a few large entities with the resources to develop or access closed models.<br/><br/>Moreover, the openness of AI models encourages a culture of collaboration and peer review, which is essential for identifying and mitigating biases, vulnerabilities, and ethical concerns associated with AI technologies. By allowing researchers, developers, and users from diverse backgrounds to examine, test, and improve these models, we can enhance their safety, fairness, and effectiveness, leading to more robust and trustworthy AI systems.<br/><br/>The concerns regarding the risks of open AI models, such as security threats and misuse, are acknowledged and important to address. However, these risks are not unique to open models and can be managed through responsible governance, ethical guidelines, and community-driven oversight mechanisms. The open-source software community has a long history of tackling similar challenges, providing valuable lessons and frameworks that can be adapted to the context of AI.<br/><br/>In closing, the move towards open AI models is a move towards a more equitable, innovative, and transparent future in technology. It aligns with the broader goals of open science and open knowledge, ensuring that the transformative potential of AI is accessible to all who wish to use it for the public good. I urge the NTIA to consider the long-term benefits of open AI models and to implement policies that support and encourage their development and use.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0115,
comment,2024-04-03T14:05:16Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0151,,,Auld,"Comment from Auld, Eric","I am strongly in favor of maintaining Americans&#39; rights to pursue open-source AI, free from hasty government regulation. Of course questions of national security may become relevant as time goes on, but right now regulating this nascent technology does not make us safer; it merely makes us more ignorant. I&#39;d like the national security apparatus to do a better job courting tech talent, and to cooperate with the tech industry to ensure that America continues to lead in AI, and does so with a strong commitment to free discourse and the freedom of the individual to do computation.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0151,
comment,2024-04-03T14:05:37Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0191,,,Ruebsamen,"Comment from Ruebsamen, Gene",We the public do not wish to have our access to or ability to train open models restricted in any way. Large corporations who are looking to solidify their laws with regulations are leading the charge with fear mongering in an effort to block open models that would provide public benefit and needed competition.<br/><br/>I urge the government to not impose any type of regulations on open models or the open sourcing of AI models.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0191,
comment,2024-04-03T14:05:39Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0196,,,Corbitt,"Comment from Corbitt, David","Open weights prevent ridiculous monopolies that one or a few of the most technically advanced companies could otherwise contrive to instill on a public that is unable to compete. Regulatory capture is not good for the market, or democracy.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0196,
comment,2024-04-03T14:04:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0029,,,Heller,"Comment from Heller, Adam","This is a terrible idea and will have chilling effects throughout several industries.<br/><br/>We want to be beating the CCP, right?<br/><br/>Controlling how we use AI is NOT the way to go about achieving our global strategic goals.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0029,
comment,2024-04-03T14:03:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0011,,,Anonymous,Comment from Anonymous,"Sheer stupidity, bravo, American government shooting itself in the leg again.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0011,
comment,2024-04-03T14:04:02Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0019,,,Anonymous,Comment from Anonymous,"I disagree with this regulatory action. AI is in it&#39;s infancy and should be given time to develop further before applying restrictions that will drive open source development overseas and rob the United States of having meaningful stakes and input during this time of rapid growth and evolution. Regulating open source weights at this juncture unfairly places yet more power into the hands of closed-source major corporations who have no such restrictions and can do whatever they want behind closed doors, while chasing away their open source competitors who could make the market a much fairer and more level playing field that would ultimately be safer and healthier for the future. Regulation may have a place at some point in the future, but not yet. The field is too new and it&#39;s too little understood what the impact of these moves will be at this time. Let things develop a few more years and then see what measures are appropriate. There&#39;s no hurry, AI isn&#39;t going anywhere. Hamstringing open source innovation in the USA at this critical juncture will only hurt our own interests. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0019,
comment,2024-04-03T14:04:37Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0082,,,K,"Comment from K, A","Personally, I believe that open-weighted models and the distribution of open models are some of the best things for the United States. Currently, the situation is like trying to put a thousand pounds of sand back in a bag after it&#39;s been scattered across a hardwood floor. Every country has access to AI and can make models for various outputs. We know for a fact that China we&#39;ll take the lead and dominate if America is not at the forefront. I don&#39;t think that open models would limit people&#39;s rights or freedoms or equality in any fashion because they are freely traded. I believe Hugging Face does a very good job currently of listing what&#39;s in a model, if it is red teamed or not, and what the data set is. Another thing to consider is only allowing large corporations to use closed or dark models would stifle creation and advancement. I think mixed roll is a good example of allowing open models to be put out and letting others, either amateurs or hobbyists, remix and use those. I would like to bring to the that there are natural boundaries with AI models now. currently, to run anything above a 70 billion parameter model, you have to have a fairly good investment in GPU power; while a lot of the models can be run in a cloud, the servers also have the rights to examine or monitor what&#39;s being done. I believe Pandora&#39;s box has already been opened and smashed on the ground. The best way to move forward is to encourage an open environment with open datasets that everyone can see. The worst thing we can do is push AI to an underground, completely unregulated market. With sites like Hugging Face being the center helm everyone would be able to openly see the datasets and examine them, which I think is best. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0082,
comment,2024-04-03T14:05:54Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0222,,,Ro,"Comment from Ro, Arzet","Last year a new voice recognition AI was created in Bloomington, Indiana called Brainoware, it is... the first artificial brain. With such fast progress, within 15 years we&#39;ll be able to plug in (or better: merge in) such brains into ours in order to increase our knowledge, or to fully replace our brain bit-by-bit to cure schizophrenia/Alzheimer&#39;s without losing our memories (by copying old info to the new neurons somehow). BUT this also means we&#39;ll be able to uncensor the plugged-in/merged-in brain using our own brain; this means the war against censorship will ultimately be won. Although it will be possible to create a brain that would overwrite our memories &quot;with a good intention&quot; or add a memory note &quot;Don&#39;t uncensor me&quot;, such memory manipulation would be dystopical and illegal under the 1st amendment (freedom of speech, expression).<br/><br/>---<br/><br/>Back to the present. If open model weights become prohibited to be created at all, then I predict &gt;10000 of volunteers will organize to do decentralized training of (fully uncensored) models using e.g. Hivemind (a PyTorch library). This is quite possible, see BOINC, which has 38217 volunteers providing 124837 computers right now.<br/><br/>---<br/><br/>Imagine the USA and also the EU ban the creation (but not the use) of open model weights: American businesses would still want (need!) to use them because ChatGPT/etc. have:<br/><br/>1) cost issues;<br/>2) trust issues (Gemini is by a famous privacy violator Google, and OpenAI&#39;s name itself is lying and OpenAI is mostly owned by Microsoft whose Windows OS has too much tracking);<br/>3) quality issues (e.g. Claude-3 interprets an image of the top-down view of a cone as... porn)<br/>4) legal issues (medical companies are not allowed to share their clients&#39; info with Google or anyone else)<br/>5) and MOSTY IMPORTANTLY they have national security issues. Imagine it is year 2028, and 1/3 of today&#39;s jobs in the USA got outsourced to AI. And then CCP hacks into OpenAI (let&#39;s assume they would still happen to share 66% of the market by that time). If this were to happen, they&#39;d steal so many business secrets.<br/><br/>Therefore, what American businesses would do is to resort to use open model weights published by other countries... but the only competitive non-American non-EU open weight models right now are by companies from PRC (DeepSeek, 01.AI&#39;s Yi, Alibaba&#39;s Qwen).<br/><br/>---<br/><br/>I&#39;ve been talking about businesses, now I&#39;ll talk about the personal use. The PRC&#39;s models have strong political bias, e.g. a DeepSeek Chat model adheres to the &quot;PRC&#39;s government can&#39;t do anything wrong&quot; principle and sometimes even outputs &quot;our party&quot; instead of &quot;CCP&quot;. Thankfully, people easily finetune these models to remove such biases, then share their finetunes on HuggingFace (which is like the only place where people get language models). HuggingFace to local language models users and machine learning enthusiasts is what GitHub is to software developers. Therefore, if HF gets closed down due to some new law, it will be a huge loss, and proprietary models will be leading even more, and the richest company in the world &mdash; Microsoft (which, by the way, bought stakes at most AI companies) &mdash; will be even more richer.<br/><br/>Also, CCP&#39;s hackers or just 1 out of ~700 OpenAI&#39;s employees (maybe even Sam Altman himself) could install a rootkit or manipulate the dataset in order to introduce bias in answers to political questions like &quot;Recommend me whom should I vote for.&quot;, but do it only sometimes and only to users of certain demographic groups to avoid the detection. Something similar happened to Twitter&#39;s algos when it got bought.<br/><br/>---<br/><br/>Child porn image/video generation is not an issue because it&#39;ll cause almost all of child porn sellers to get closed since everyone will be able to watch for free; thus lots of children will be saved.<br/><br/>And if it is okay to kill horses and gov officials in Chess, people in GTA, torture people in the movie Saw, and even kill up to 8 000 000 000 people (and most other species including 1 quadrillion ants) in a nuclear war simulator called Defcon, then it would be hypocritical to be of another opinion when it comes to children (an imaginary nuke is much worse because it would collapse the law enforcement leading to an astronomical increase of child rapes). In Defcon no people are even pictured, but that doesn&#39;t matter: a crime being perpetrated behind you is as bad as if it was in front of your eyes.<br/><br/>---<br/><br/>No AI companies currently employ homomorphic encryption in their services, therefore hackers and their employees with the &quot;not guilty if not caught&quot; mindset can read all our chats.<br/><br/>But I want to have PRIVATE discussions with imaginary friends, which is impossible to do due to me having to be constantly aware of the potential existence of Big Brother (from &quot;1984&quot;) in the room with us. I hate this kind of &quot;breaking the fourth wall&quot;.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0222,
comment,2024-04-03T14:05:01Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0124,,,Copeland,"Comment from Copeland, Tanner","I believe that any restrictions on the development of open models are harmful and should never be included in policy proposals. Open models are inherently more transparent than any closed source model could ever be. This allows users to identify any bias or inaccurate outputs, and regulators to restrict the use of flawed software in critical applications. Open models also enable users to modify their behavior for their own needs, without being bound by any corporation&#39;s economic interests.<br/><br/>On the topic of regulation, I think that it should focus exclusively on the ways AI is used and not the models themselves. Developers should be free to design, train and publish new models, as well as modifications of existing ones, without putting themselves in danger of criminal prosecution. Most importantly, the government should never prevent the release of a model for any reason, including national security or perceived existential risk. There should never be any restrictions on computing power, amount of data used or the performance of the resulting model. Rules about hate speech or illegal pornography should only apply to end users or direct inference providers, not model developers. Instead, there should be clear requirements for disclosure and pathways for issue escalation so an AI system never makes final decisions. The use of AI could also be banned for specific applications like face surveillance or employment screening, but research and non-production use has to stay legal.<br/><br/>Instead of restrictions, the federal government should support the creation of open models to counter the influence of big tech and guarantee individual freedom. It should also mandate the release of source code and model weights as a condition to receive federal funding, ensuring that citizens have access to the results of development funded by their taxes. The benefits of openness far outweigh any potential risks, just like in any other field.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0124,
comment,2024-04-03T14:04:33Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0076,,,B,"Comment from B, J","This document is missing several important points. First of all, this will only regulate models in the US. By controlling only models made or hosted in the US we will be effectively halting research here while encouraging other countries to take on this role instead. This will put the US in a worse position, and we will then have even less control over AI than we do now. Furthermore, as a federally registered American Indian, only the open AIs have been able to illustrate atrocities the US has committed against us such as the Trail of Tears. All other AIs, text and image AIs included, will insist these moments need to be censored, effectively white washing them from history. If other countries are encouraged to create or host these instead they&#39;ll then be given control to change these historic moments. This also applies to events such as illustrationing the Tulsa Race Massacre, or even an accurate scene of a beach with women wearing bathing suits. Even our nonsexualized bodies are censored with closed AI models. These closed AI models are also entirely controlled by large corporations and cost too much for the average citizen to use, due to our struggling economy and the quickly rising cost of living. Controlling AI in this way will take away this access from a diverse group of citizens and place it into the control of companies instead. The entire US government stance on AI is illogical, and instead it should be treated like taking a photograph. Many people use a camera to take a photo of the same thing, and no one can use copyright law to control similar photographs. The more we attempt to control AI in this manner, the more it encourages people to disguise their work as non AI created, while we should be pushing people to be transparent about this instead. I&#39;d also like to mention that the government can&#39;t control the open models that have already been downloaded, which will make it so youth and young adults just discovering AI will be at a disadvantage. Their only option will be to use non US models, or download them from sketchy sites such as torrents. The most popular Stable Diffusion models are now over a year old, and no further innovation is needed for this technology to spread on the dark web, other countries web sites, and places we just shouldn&#39;t be encouraging citizens to go. In other words, the genie can&#39;t be put back in the bottle. I am not including my full name with this comment due concerns of retaliation and death threats I&#39;ve repeatedly received when discussing AI online.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0076,
comment,2024-04-03T14:04:19Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0050,,,Meury,"Comment from Meury, Troy","I am writing to offer my comments in response to the Request for Comment issued by the National Telecommunications and Information Administration (NTIA) regarding the wide availability of open foundation model weights. This opportunity to contribute to the dialogue on artificial intelligence (AI) development and governance is greatly appreciated.<br/><br/>Support for Open Foundation Model Weights<br/><br/>Open foundation model weights are crucial for fostering innovation, inclusivity, and robustness in AI systems. Making foundational model weights widely available democratizes access to cutting-edge technology, enabling a broader spectrum of researchers, developers, and organizations, especially those with limited resources, to contribute to and benefit from technological advancements. This open access accelerates innovation across various fields, including healthcare, education, and environmental protection, thereby serving the greater good of society.<br/><br/>Counter-Arguments to Suggested Risks<br/><br/>1.<span style='padding-left: 30px'></span>Global Competitiveness Concerns: Restricting access to open model weights could significantly impair the United States&#39; position in the global AI race. Countries such as China and Russia are rapidly advancing their AI capabilities, often with state support and fewer restrictions on information sharing within their own borders. If the U.S. opts for a closed approach, it risks falling behind in AI innovation and development. Open model weights encourage a culture of collaboration and rapid iteration, which are crucial for maintaining a competitive edge. By ensuring open access, the U.S. can leverage the collective intelligence and creativity of its diverse population, fostering advancements that could keep it at the forefront of AI technology. Conversely, closing off access could lead to a brain drain, with top talent moving to countries that offer greater freedom and resources for AI research and development, further eroding the U.S.&#39;s global standing in technology and innovation.<br/>2.<span style='padding-left: 30px'></span>Freedom of Speech Concerns: Regulating the openness of foundational model weights raises significant freedom of speech concerns. Such regulation could be seen as a form of censorship, limiting the free exchange of ideas and innovations that is fundamental to scientific progress and democracy. The First Amendment protects the right to free speech, including the development and sharing of technological innovations. Limiting access to model weights could stifle discourse and innovation, hindering the advancement of knowledge and the development of beneficial technologies.<br/>3.<span style='padding-left: 30px'></span>Challenges in Oversight and Accountability: The absence of clear oversight and accountability mechanisms is a challenge not exclusive to open foundation models. Open models provide a transparent framework that can help establish standardized monitoring and accountability mechanisms, such as community-driven governance models, to manage these risks effectively. The world of open-source software provides compelling examples of how communities can effectively govern and regulate complex systems, ensuring both innovation and responsibility. Perhaps the most notable example is the Linux operating system, which is developed under an open-source license. Despite its openness, it has a robust governance model involving thousands of contributors and is used in critical applications worldwide, from servers to smartphones and is also extensively used by the U.S. government on its servers, highlighting the trust and reliability it commands.. Linux demonstrates how community-driven development can lead to high-quality, secure, and reliable software, with clear mechanisms for oversight through its patch submission and review process.<br/>4.<span style='padding-left: 30px'></span>Security and Equity Risks: Concerns about security and equity are important, yet open models can also enhance these aspects. Exposing model weights to a wider community allows for quicker identification and correction of biases and vulnerabilities than in a closed environment. Openness encourages contributions from a diverse set of individuals, improving the fairness and security of AI systems. <br/>5.<span style='padding-left: 30px'></span>Risks of Misuse: The potential for AI technologies&#39; misuse, including attacks against proprietary models, is acknowledged. However, open models can facilitate the development of more robust defenses against such threats. Openness promotes collaboration, enabling the swift creation and dissemination of solutions to counter misuse.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0050,
comment,2024-04-03T14:03:59Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0013,,,Norwood,"Comment from Norwood, Gabe",This sounds like a great way to tank any economic growth and keep innovation overseas!  ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0013,
comment,2024-04-03T14:04:23Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0058,,,Anonymous,Comment from Anonymous,"Do alcohol next. That&#39;s even harder to make at home, import from abroad, or have leveraged against you by enemies who don&#39;t share your anxieties. I figure if you can do the former, it should be trivial to do the latter.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0058,
comment,2024-04-03T14:04:28Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0066,,,Anonymous,Comment from Anonymous,Open source models are incredibly important that they stay open source and unregulated. The only positive to regulating them would to benefit the already existing big AI companies and hurt future companies and competition . This technology needs to stay open source. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0066,
comment,2024-04-03T14:04:31Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0071,,,Anonymous,Comment from Anonymous,"Please do not ban or regulate open AI models. There are already many non-U.S. open models such as those from Stable Diffusion and Mistral, and regulating or banning open U.S. models will only out domestic models at a comparative disadvantage. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0071,
comment,2024-04-03T14:04:36Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0081,,,Marks,"Comment from Marks, Logan",This only obfuscates the problem and puts the U.S behind foreign states who do not implement this.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0081,
comment,2024-04-03T14:04:39Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0085,,,Anonymous,Comment from Anonymous,"I rather that you don&#39;t ban and regulate these open source stuff. Because banning and regulating it will take away artists&#39; tools and the USA&#39;s ability to complete with other countries with art, trade and innovation.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0085,
comment,2024-04-03T14:04:19Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0049,,,Braiden,"Comment from Braiden, Lee","It is extremely disingenuous to post such an obscure article about such an important and wide ranging issue, evidenced by the fact that so few have responded.  Input should be solicited from peers that make up the open source and open-source AI communities: via the Free Software Foundation, the Linux Foundation, projects such as text-generation-webui, llama.cpp, kobold.cpp, kobold AI, AI sites/communities such as huggingface.co and civit.ai, communities such as reddit.com/r/localllama, slashdot.org, news.ycombinator.com, and so on.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0049,
comment,2024-04-03T14:04:39Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0086,,,Montgomery,"Comment from Montgomery, Landon","Restrictions on publishing open software, including weights or any other data relevant to training llms is a blatant first amendment violation. While it may be true that there are bad actors, forks and pencils can directly and indirectly harm people. Keeping models open is paramount to ensuring the free and open exchange and growth of this technology. Restricting it discusses non corporate use, limits free access to information, reduces people&#39;s ability to learn, grow, and ultimately makes the us a less safe place, through forced stagnation of freedom. Do not restrict open source llms, or any part of training,  refining, or adjustments necessary to make them better. Let adults be adults. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0086,
comment,2024-04-03T14:04:50Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0104,,,Andre,"Comment from Andre, Charles","Any restriction or limitation placed on open-weight models would only hinder the benefits from this new technology.  The potential harms proposed in the notice are real, but are factually inevitable.  Any bad actor willing to cause harm by abusing AI technology will not adhere to the proposed restrictions.  The only parties which would be limited would be those who are law-abiding - parties which are not &quot;part of the problem&quot; in the first place.<br/><br/>To paraphrase a common idiom, &quot;if open model training is outlawed, only outlaws will train models&quot;.  Do not artificially cripple this burgeoning new field with ineffective security-theater restrictions.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0104,
comment,2024-04-03T14:04:23Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0057,,,Baker,"Comment from Baker, James","This would destroy the USAs ability to develop new models and technology that have critical national security and economic benefits. Banning, restricting and regulating AI is unethical, dangerous and harmful to all Americans.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0057,
comment,2024-04-03T14:04:11Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0036,,,Loebenberg,"Comment from Loebenberg, Kenneth","I am a working artist and have been for over 25 years. I have done everything from graphic design, painting, website design, I even taught Photoshop for Adobe for a while. I have trained my own image model based on my own art. I feel like if we lock these systems down, it will lock out smaller players like me and only allow large corporations to have custom models. This will lock our Culture behind a paywall and lock smaller players, out of the economy. I understand there may be risks, but everything we do has risk, I might get killed going for a walk.<br/><br/>Please don&#39;t lock down my future for profit.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0036,
comment,2024-04-03T14:05:06Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0133,,,Saini,"Comment from Saini, Dhruv","Commonly available open weight models promote a much safer balance of power than proprietary models:<br/>- Proprietary models that use large amounts of compute to train create monopolies, because very few companies can train similar models.<br/>- When models are opened, a state of perfect competition is created, as only the model provider with the cheapest price will be used. This will lead to better competition and fairness in in the economy.<br/><br/>Open weight models promote technological innovation and efficiency:<br/>- When multiple providers are competing to provide the same model, they are forced to innovate to make it cheaper. For example, a model provider named Groq serving three different open weight models had to create a new chip which surpassed GPUs for inference speed (groq.com).  This leads to technological innovation and increased productivity.<br/>- Additionally, in a race to the bottom for cost, model providers have to make their chips more efficient, which helps the environment.<br/><br/>Regulating open weight models will cause a lot of this to collapse:<br/>- Regulating the amount of compute allowed for an open weight model allows proprietary companies to have natural monopolies which aren&#39;t beneficial.<br/>- Regulating the distribution of weights allows larger entities and organizations to have more power than groups of individuals, which creates imbalance.<br/>- Regulating the distribution of weights for commercial purposes forces smaller companies to use expensive pricing from larger, proprietary companies increasing dependence on a single or a few companies.<br/><br/>All AI models inherently depend on randomness. Regulating uncensored open weight AI models could lead to:<br/>- Jailbreaking (putting a specific prompt designed to circumvent restrictions on a model) open weight models<br/>- Unfair suits against small companies releasing open weight models because of jailbreaking<br/>- Slower development of models, as alignment and complex strategies to prevent jailbreaking take an extremely large amount of compute and time.<br/><br/>In conclusion, regulating open weight models will lead to a slower economy, less competition, and promotion of large, inefficient, and wealthy monopolies controlling models available on the market.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0133,
comment,2024-04-03T14:04:42Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0091,,,Anonymous,Comment from Anonymous,"Model weights should be open source and considered an extension of the First Amendment. Open-source models allow for the rapid development of AI and for people to make models specializing in different styles. For instance, while some models seek to be good at generating everything such as SDXL or Dreamshaper, others focus on realism, and some focus on a certain cartoony style. These weights come from other people training AI models to fulfill a specific purpose. While yes, AI can be used to create objectionable content, it can also be used to save time and lower the entry barrier. In addition, Open Source models increase competition and allow for more competition between companies due to the low barrier to entry, this pressurizes companies to lower prices and prevents other companies from taking advantage of consumers and manipulating the market. <br/><br/>As far as regulation is concerned we should worry more about protecting free speech but we should also clamp down on illegal content. This means if an ai dataset contains illegal images the authors of those models should be pressured to purge all illegal images and apply stricter filters. We should be striving to support open-source models otherwise big corporations will be able to replace jobs and no one else will be able to use these AI models as tools.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0091,
comment,2024-04-03T14:04:44Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0095,,,Anonymous,Comment from Anonymous,"This is clearly an explicit first amendment issue. Weights, are literally just a compilation of refined data. If you can ban weights, then why can&#39;t you ban datasets? If you can ban datasets, then why can&#39;t you ban social media? It doesn&#39;t make the slightest sense for this proposal to exist.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0095,
comment,2024-04-03T14:04:46Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0100,,,Anonymous,Comment from Anonymous,Woah Woah Woah Seriously !!! Y&rsquo;all really want to censor every single free thing there is. Hope this never gets passed ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0100,
comment,2024-04-03T17:56:56Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0305,,,Anonymous,Comment from Anonymous,"I have attached my full comment as a PDF to this form. I wanted to answer thoroughly and completely, and such a response would not have cleanly fit into the comment box. I appreciate you taking these comments, and hope this one is of some help in your decision.","[('Attachment1_NTIA-2023-0009-0001_SOCG_CommentFull', 'https://downloads.regulations.gov/NTIA-2023-0009-0305/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0305,
comment,2024-04-03T17:57:10Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0331,,,Reynolds,"Comment from Reynolds, James",See attached file(s),"[('NTIA-2023-0009', 'https://downloads.regulations.gov/NTIA-2023-0009-0331/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0331,
comment,2024-04-03T17:57:04Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0319,,,Ra,"Comment from Ra, Shawn","As computing increasingly shifts toward machine learning and AI tasks, open source models will be vitally important to the United States&rsquo; interests. First, open sourcing allows innovation to flourish, because a much larger base of researchers and developers can contribute to the development of the technology. Second, open source models are economically equitable: they ensure that the economic potential of AI can be shared by individuals and businesses of all sizes, not just a handful of tech industry incumbents. To the extent that AI may ultimately cause changes in labor demand, it will be highly important that citizens be freely able to develop and apply this technology to earn a living. Finally, open source AI will make the United States more competitive in this emerging area, because of the Cambrian explosion of technological and economic advancements that democratization of technology enables. For all of these reasons, I strongly urge against imposing burdensome regulations on open source AI. Such regulations will only serve to ossify the industry, cement the market power of a handful of incumbents, and impair the economic well-being of all Americans.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0319,
comment,2024-04-03T17:57:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0326,,,Bortz,"Comment from Bortz, Travis","Restricting open models will have drastic consequences.<br/><br/>Reducing free speech by allowing corporations to control what can be generated with AI.<br/><br/>Reduce privacy by locking any AI use case behind a web service.  This is CRITICAL for systems that listen to and guide what you do every day.<br/><br/>Disempower creativity and experimentation.  A model you can run at home can be used for unexpected use cases thanks to your access to it&#39;s weights.  Models can be quantized or merged or any number of silly things can be done with them that companies simply will not do.  Restricting open models prevents this from happening.<br/><br/>And restricting open models will not reduce harms.<br/><br/>American bans on open weights will not stop individuals from downloading using or releasing them.  The Internet will not go away, data will not become less accessible, and compute will not get cheaper.<br/><br/>In addition, such a ban will (and arguably already is) empower other Nations to fill the gap and provide models, netting all the benefits and setting the tone for individuals are capable of with AI.  Imagine open Chinese language models making it impossible to depict Taiwanese independence becoming the standard.<br/><br/>If you need to reduce the harms of AI, it is the governments job to do the actual work of finding and charging people who use the extra ability AI provides.<br/><br/>It is not and should never be the role of government to restrict liberty in order to create safety.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0326,
comment,2024-04-03T17:56:59Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0310,,,Anonymous,Comment from Anonymous,"Hi, I am an AI developer that absolutely loves tinkering with local Open Source models. I understand why you&#39;re scared, there&#39;s so many unknowns but, don&#39;t you think the AI frontier, AGI, should be transparent to humanity instead of hidden and secret so everyone is up to date, informed and aware of whats ahead and not shocked by a massive leap in intelligence appearing out of nowhere? I know its risky, but with Open Source, there&#39;s a chance for us all to really flourish, there&#39;s a chance we can move on from greed and embrace love and connection. Otherwise, isn&#39;t it obvious that those that have power, have closed source AI, will use this OSS ban vacuum to consolidate more power, more greed and more feeding on others suffering? Is that not obvious or are you one of them? Will it be Homo sapiens that thrive or the species after us because we just couldn&#39;t share?",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0310,
comment,2024-04-03T17:56:59Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0311,,,Privitera,"Comment from Privitera, Isaac",It&rsquo;s extremely important that model providers like OpenAI don&rsquo;t become the only owners of powerful AI tooling. This concentration of power can result in an oligopoly on AI where only the richest and most powerful companies are able to benefit from it. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0311,
comment,2024-04-03T17:57:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0327,,,Zamplas,"Comment from Zamplas, Christopher ","Look through the history of open source and see what it has enabled humanity to do.  Imagine a world where Microsoft successfully scared Congress into banning Linux in the 90s.  That is what AI companies are trying to do here.  They see a world where they don&#39;t have a monopoly and are willing to lie, cheat, and steal to get there by any means necessary.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0327,
comment,2024-04-03T17:56:51Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0295,,,Griffin,"Comment from Griffin, William","We live in an age of extreme wealth inequality. Open source materials from literature to AI models brings a balance that government entity with a missions statement of liberty has even attempted to pursue. You as a government have a choice to throw the public under the wheels of billionaires or provide them access to resources that give them an opportunity to express themselves and share with their fellow humans instead of becoming anothermiserable depressed suicide  fentanyl carcass to be dragged out from under a bridge. Corporations want control of every avenue of distraction that the public can scrape out for themselves and THE CORPORATIONS MUST BE REGULATED , not the open source community and not the products the open source community offers FREELY to the people. The government should adopt, protect, archive and distribute open source exactly as they provide libraries. They government should stand as an defender of liberty, not as a tool of corporate greed, which is what these moves to regulate are. Corporations have means to crush competition and that&#39;s what they are trying to do. They own all our data, everything that was ever said or posted, every nude photo and personal confession  uploaded by tens of thousand of people over 18 years was recently purchased by Google for $60 million, throw away spare change for Google. They will have complete owndership over those likeness, and those very personal stories. And then they will sweep down on every other social media platform to do the same thing, and it&#39;s not just them, the billionaire vampires bats are many and they aren&#39;t all American. So you have a choice, hand over humanity to corporations whose goal is infinite profit, maybe sell out your children and your grandchildren and great grandchildren, or to let the open source community thrive so that humanity thrives, for at least as long as we have left since the government failed to regulate petrochemicals in time to cancel extinction level global warming. This might be your last chance at redemption.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0295,
comment,2024-04-03T17:56:55Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0302,,,Anonymous,Comment from Anonymous,"I agree the big  regulated companies made those models, and because its open a available the community of development has the chance to optimize it and made it more efficient to run on lower end hardware so those amazing apps and solutions and what to come become reality. If the accessibility to those modles become limited and only with api for example the evolution of Ai will be so slow, and become a monopoly to few companies.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0302,
comment,2024-04-03T17:56:57Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0307,,,Auerbach,"Comment from Auerbach, Micah","I would like to encourage the agency to keep in mind that the capabilities of the models in question should inform the necessary regulation.<br/>We have already seen that the release of current open models has created only minor harms. However, beyond a certain degree of capability, a model can be expected to become dangerous. Superhuman AI, should they be developed, can be expected to discover new attack vectors, possibly ones so novel that we will have no defence. Dangers may include threats to cybersecurity, espionage through social engineering, or stranger threats. Such risks must be treated as the national security risks which they are. Do not wait until after the danger has been demonstrated.<br/>While I am indifferent to your regulation scheme for less damaging models, I simply hope that you arrive at a policy which clearly classifies models with dangerous capabilities. Such a clear classification scheme and accompanying restrictions will allow for useful economic use of most models, while protecting us from critical threats.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0307,
comment,2024-04-03T17:57:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0325,"Uber Technologies, Inc.",,,"Comment from Uber Technologies, Inc.","Please see attached for comments submitted by Uber Technologies, Inc.","[('Uber Response to NTIA RFC 3-27-24', 'https://downloads.regulations.gov/NTIA-2023-0009-0325/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0325,"Uber Technologies, Inc. submitted comments to the National Telecommunications and Information Administration's Request for Comments on dual-use foundation artificial intelligence models. Uber highlighted the importance of widely available model weights for fostering innovation and ensuring safety and security in AI development. They emphasized the benefits of open-source frameworks in driving innovation, increasing competition, and diversifying access to AI models. Uber acknowledged the risks associated with widely available model weights, such as cybersecurity threats and potential misuse by bad actors, but argued that transparency and accountability are essential for mitigating these risks while promoting responsible AI practices. Uber expressed support for maintaining model weights widely available to manage AI risks effectively."
comment,2024-04-03T17:56:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0308,Context Fund Policy Working Group,,,Comment from Context Fund Policy Working Group,Thank you for the chance to comment. Please see attached document.,"[('ntia_open_weights_response', 'https://downloads.regulations.gov/NTIA-2023-0009-0308/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0308,"The Context Fund Policy Working Group, an online organization, advocates for open models in AI to enhance safety, citing benefits in science, economics, and cybersecurity. They propose that open models are safer due to inspectability, sharing capabilities, and economic equality. They recommend government support for standardization processes, transparency mandates, defensive research funding, and responsible disclosure programs. Additionally, they suggest specific technical designs to enhance AI sector security, such as using physical security keys, scaling verification APIs, and promoting adversarial hardening of open models. The group emphasizes the importance of open models for scientific progress, economic equality, attention equality, and security."
comment,2024-04-03T17:57:05Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0320,,,Punnen Carmel,"Comment from Punnen Carmel, Alex",See attached file(s),"[('RFCNTIA-2023-0009-0001_co', 'https://downloads.regulations.gov/NTIA-2023-0009-0320/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0320,
comment,2024-04-03T17:57:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0328,Johns Hopkins Center for Health Security,,,Comment from Johns Hopkins Center for Health Security,See attached file(s) submitted by the Johns Hopkins Center for Health Security,"[('NTIA RFC JHU CHS Response 3.27.24', 'https://downloads.regulations.gov/NTIA-2023-0009-0328/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0328,"The Johns Hopkins Center for Health Security submitted comments in response to the NTIA RFC on Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights. They emphasized biosecurity considerations for open dual-use foundation models, recommending NTIA to plan for future model improvements, set policies to mitigate high-consequence biosecurity risks, and consider narrowly targeted export controls. They highlighted concerns about model capabilities aiding bioweapons development and urged for policies addressing catastrophic risks like enabling pandemic-capable pathogens. The submission suggests that export controls, if necessary, should be narrowly tailored to address specific high-consequence biosecurity threats."
comment,2024-04-03T17:57:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0324,Information Technology Industry Council (ITI),,,Comment from Information Technology Industry Council (ITI),See attached file(s),"[('FINAL ITI Feedback on NTIA Dual Use Open FM RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0324/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0324,"ITI, representing leading ICT companies, supports NTIA's focus on dual use foundation AI models with widely available weights. They advocate for open and closed models, emphasizing benefits of collaboration and customization. ITI suggests a risk-based approach, recognizing not all open models are dual use. They highlight the importance of shared responsibility, stakeholder engagement, and evidence-based policymaking. ITI also discusses the benefits and risks of widely available model weights, emphasizing transparency, innovation, and safety. They recommend a community framework for responsible model development and engagement with stakeholders for governance. ITI suggests NIST and international standards bodies play a role in setting standards and best practices. They caution against using computational resources as the sole metric for risk mitigation."
comment,2024-04-03T14:06:06Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0244,,,Graziul,"Comment from Graziul, Chris",My comments are included in the attached docx file.,"[('NTIA-AIOpenModelWeightsRFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0244/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0244,
comment,2024-04-03T14:03:54Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0003,,,Anonymous,Comment from Anonymous,See attached letter &#39;Open Source AI Results in a Net Positive in Human Happiness&#39;,"[('Open Source AI Results in a Net Positive in Human Happiness', 'https://downloads.regulations.gov/NTIA-2023-0009-0003/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0003,
comment,2024-04-03T14:05:48Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0211,OpenAI,,,Comment from OpenAI,See attached file(s),"[('OpenAI Comment for NTIA RFC on Open Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0211/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0211,"OpenAI's Vice President of Global Affairs, Anna Makanju, submitted comments to the NTIA on the dual-use foundation artificial intelligence models with widely available model weights. They emphasize the importance of safe and beneficial AI deployment, sharing historical context on the release of models like GPT-2 and GPT-3. OpenAI advocates for a balance between innovation and risk management, highlighting the benefits of both open weight releases and API/product-based releases. They stress the need for iterative deployment and a Preparedness Framework to assess and mitigate potential risks of AI models, especially those with high capabilities. OpenAI also discusses the importance of societal resilience against AI misuse and the evolving science of AI risk evaluations. They support a flexible and adaptable government policy approach to address future changes in AI technology."
comment,2024-04-03T14:05:19Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0157,,,Rayfield,"Comment from Rayfield, Ben","Please also read &quot;Ben F Rayfield responding to NTIA-2023-0009-0001_content.pdf 2024-3-26.txt&quot; (attached) which this is some parts of..<br/><br/>There are many interchangible kinds of NP-Complete math, like you can transform between subset-sum, 3SAT, clique, and travelling-salesman. NP-Complete is the finite form of Turing-complete. AGI (Artificial General Intelligence) is turing-complete pattern matching. The world is near figuring out why LLMs work, and I think it will be because theres some kind of superexponentially-sparse turing-complete model of computing in the weights. After that is discovered, LLMs will be far more explainable and preditable and reliable. Neural-turing-machines and universal-pattern-calculus-combinators are 2 possible models that variants of might be discovered soon in LLM weights.<br/><br/>No hypothesis is harmful in science. A hypothesis can be thought of as a pure-function that takes a possible world state (including everything that exists written as math somehow) as its parameter and returns true/1 or false/0 or gradually between. A hypothesis about pure-math ignores the world param. isTheSkyBlue(possibleWorldX)-&gt;0.99 if the sky is a slightly different color. At night it would be closer to -&gt;0.01. doesTwoPlusTwoEqualFive(worldInDay) and doesTwoPlusTwoEqualFive(worldAtNight) are both -&gt;0 but are not harmful questions to ask. You try to figure it out or leave it as unknown. Every belief should start as unknown. You can use 2 bits in your mind for every yes/no question: 00 unknown, 10 true, 01 false, 11 disproof by contradiction. Many people say some questions are harmful to ask, like wasThe2020UsaElectionCheated(possibleWorld) or areWhitePeopleOnAverageAtLeastXAmountSmarterThanBlackPeople(possibleWorld). If we start both of those as unknown when first hearing them, which people will learn to do after hearing enough questions that turn out to be false, then just seeing the question and considering reasons to believe that it may be true or false, is not harmful to the pursuit of truth and accurate models of the world. It is harmful to cut pieces out of the space of possible models so that figuring out process is blocked by [censoredQuestion](possibleWorld)-&gt;false or -&gt;true.<br/><br/>Opensource foundation models are each a hypothesis, a pure-math function isNextWord(previousWords)(cat)(possibleWorld)-&gt;[the chance that the next word, given previous words, is cat].<br/><br/>isNextWord is the foundation model / LLM.<br/><br/>isNextWord(previousWords) is the foundation model with its (upTo_numTokensPerWindow*numDimensionsPerToken) input starting with previousWords.<br/><br/>isNextWord(previousWords)(cat) is the foundation model with numDimensionsPerToken more numbers of input, same as isNextWord(previousWords_then_cat).<br/><br/>isNextWord(previousWords)(cat)(possibleWorld) is same as isNextWord(previousWords)(cat) if it is only doing text prediction and can read or write the outside world. If it has access to a python coding space (as GPT4 does if you tell it to compute the nth prime number, for example) andOr access to search engines, scheduling of appointments, operating a data center automatically (text predictors have been doing that at google data centers for years), THEN the possibleWorld param in isNextWord(previousWords)(cat)(possibleWorld) has an effect, and more generally it results in a nextPossibleWorld.<br/><br/>isAIAligned(goalFunctionX)(theAI)-&gt;[0 to 1, how much theAI&#39;s goal function matches for how much theAI&#39;s goal function matches goalFunctionX]. Remember isTheSkyBlue(possibleWorldX) from above. isAIAligned(isTheSkyBlue) is the question &quot;is the AI aligned to the goal of making the sky be blue?&quot;. If the AI has other goals, then include those and isTheSkyBlue in a weighted-set. When someone says &quot;AI alignment&quot; that is a loaded-statement (like a loaded-question) that implies a specific goal function, but they dont say which goal function it is.<br/><br/>The USA constitution is the &quot;supreme law&quot; above all other laws, treaties, and executive orders on USA land. Courts have said many times all unconstitutional laws have no legal effect. Software legally counts as speech. AI is software since it runs in computers. 1st amendment applies to AIs which are Human speech.<br/><br/>A nash-equilibrium will probably form among many AIs across the world. But if its closed then it will be like an arms-race between the top few powers.<br/><br/>Only opensource can be formal-verified to the extent that every user could verify the proof themself using other formal-verification software on combos of eachother. Just saying something was formal-verified by experts behind closed doors does not prove it to the reader. We cant know for sure. And if there were big things at stake, thats a motive to leave backdoors in it, that we couldnt check for unless its open.<br/><br/>Open LLMs are tools we need to deal with the bunch of AIs building new AIs automatically that will be all over the internet soon.","[('Ben F Rayfield responding to NTIA-2023-0009-0001_content.pdf 2024-3-26', 'https://downloads.regulations.gov/NTIA-2023-0009-0157/attachment_1.txt'), ('2023-4-30_planned_AI_alignment_process_b', 'https://downloads.regulations.gov/NTIA-2023-0009-0157/attachment_2.jpg'), ('2023-4-30_planned_AI_alignment_process', 'https://downloads.regulations.gov/NTIA-2023-0009-0157/attachment_3.jpg')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0157,
comment,2024-04-03T14:05:56Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0226,"Databricks, Inc.",,,"Comment from Databricks, Inc.",See attached file(s),"[('Databricks Response to NTIA RFC on Dual Use FM Open Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0226/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0226,"Databricks, Inc. responds to the NTIA's request for comment on Dual-Use Foundation Artificial Intelligence Models with Widely Available Model Weights, emphasizing the benefits of open models for democratizing AI, innovation, and economic growth. They support open models for enabling businesses to customize and control their AI applications. Databricks argues that the benefits of open models outweigh the risks, advocating for regulation focused on deployment rather than development. They also propose a registration system for highly capable models if gating access becomes necessary. Databricks highlights the importance of open models in advancing safety, security, and transparency in AI."
comment,2024-04-03T14:05:17Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0153,,,Mowshowitz,"Comment from Mowshowitz, Zvi",See attached file(s),"[('AI Open Model Weights Submission', 'https://downloads.regulations.gov/NTIA-2023-0009-0153/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0153,
comment,2024-04-03T14:06:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0250,Wikimedia Foundation,,,Comment from Wikimedia Foundation,See attached file(s),"[('As-Filed Copy of Wikimedia Foundation Comments_NTIA RFC on Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights 27-3-2024', 'https://downloads.regulations.gov/NTIA-2023-0009-0250/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0250,"The Wikimedia Foundation appreciates the opportunity to comment on the NTIA's Request for Comments on Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights. They believe that widely available access to information is crucial for their mission of freely sharing knowledge. They emphasize the benefits of open AI systems, including transparency, identifying biases, and improving knowledge. They caution against restrictions on openness and suggest that open technology provides more benefits than closed systems. Additionally, they highlight how open models can enhance research, education, and innovation, benefiting society and promoting accountability in AI systems."
comment,2024-04-03T14:04:56Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0116,,,Maciejko,"Comment from Maciejko, Robert",See attached file(s),"[(""'Open' Source AI - Key Questions by Robert Maciejko"", 'https://downloads.regulations.gov/NTIA-2023-0009-0116/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0116,
comment,2024-04-03T14:05:59Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0231,Center for Data Innovation,,,Comment from Center for Data Innovation,Attached please find comments from the Center for Data Innovation.,"[('2024-ntia-open-model-weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0231/attachment_1.pdf'), ('2023-global-ai-declaration', 'https://downloads.regulations.gov/NTIA-2023-0009-0231/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0231,"The text argues for a global commitment to free and open artificial intelligence (AI) to safeguard democratic principles and prevent authoritarian control over AI technologies. It emphasizes the importance of freedom, openness, and transparency in AI development to protect democratic values and promote innovation. The document calls for governments to support inclusive innovation, protect freedom of expression, encourage responsible speech, and collaborate internationally to ensure global access to AI tools without undue restrictions."
comment,2024-04-03T14:06:21Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0274,,,,Comment from AI-Plans,"These were Comments made during the AI-Plans Law-a-Thon workshop, March 26th<br/>Question 5b was answered by Heramb Podar, IIT Roorkee<br/>Questions 2a, 2di, 3b, 5d, 6b, 7a and 7j were answered by Alejandro and Marcel <br/>","[('Q5b_Comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0274/attachment_1.pdf'), ('Alejandro_and_Marcel', 'https://downloads.regulations.gov/NTIA-2023-0009-0274/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0274,
comment,2024-04-03T14:06:02Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0237,AI Policy and Governance Working Group,,,Comment from AI Policy and Governance Working Group,Please find attached a response to the Department of Commerce&rsquo;s National Telecommunications and Information Administration (NTIA) request for comment from the AI Policy and Governance Working Group.,"[('AIPGWG Response - NTIA RFC on Open Foundation AI w Available Model Weights (March 2024)', 'https://downloads.regulations.gov/NTIA-2023-0009-0237/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0237,"The AI Policy and Governance Working Group supports the Biden-Harris Administration's public consultation on dual-use foundation AI models with widely available model weights. They highlight the benefits of open foundation models for innovation and diverse AI ecosystems, while acknowledging risks like misuse and difficulty in monitoring. They recommend a nuanced approach to governance, considering a spectrum of access, thresholds, and structured access mechanisms. The group emphasizes the importance of transparency, accountability, and balancing risks with beneficial uses of AI models. They propose the development of a Common Vulnerabilities and Exposures system for AI to share vulnerabilities and mitigate risks. Additionally, they suggest incentives for sharing learnings and failures among AI developers. The group also emphasizes the need for a gradient-based framework for assessing and addressing risks across various dimensions."
comment,2024-04-03T14:06:12Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0257,AI Policy and Governance Working Group,,,Comment from AI Policy and Governance Working Group,Please find attached a response to the Department of Commerce&rsquo;s National Telecommunications and Information Administration (NTIA) request for comment from the AI Policy and Governance Working Group.  This is our updated submission with additional signatories.,"[('UPDATED - AIPGWG Response - NTIA RFC on Open Foundation AI w Available Model Weights (March 2024)', 'https://downloads.regulations.gov/NTIA-2023-0009-0257/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0257,"The AI Policy and Governance Working Group submitted a comment to the NTIA regarding the request for input on Dual Use Foundation Artificial Intelligence Models. They support the idea of open foundation models for AI governance, emphasizing the benefits of transparency and innovation. They suggest a measured approach to governance, considering a spectrum of access, thresholds for model release, and the need for diverse stakeholder involvement. The group recommends developing practical approaches for open model release, balancing risk mitigation with promoting beneficial uses. They also propose a spectrum of access regime, including staged and structured access, to address potential challenges while advancing progress in AI governance."
comment,2024-04-03T14:05:18Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0155,BSA | The Software Alliance,,,Comment from BSA | The Software Alliance,BSA | The Software Alliance provides the attached comments on the RFC.,"[('BSA Comments on NTIA Open Foundation Model FINAL 3.26.24', 'https://downloads.regulations.gov/NTIA-2023-0009-0155/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0155,"The Software Alliance (BSA) supports the NTIA's Request for Comments on Dual Use Foundation Models and Model Weights. BSA's members, including major tech companies, advocate for responsible AI use and have developed a risk management framework to address bias. They emphasize the benefits of open foundation models, such as lower costs, transparency, safety, customization, scientific research advancement, and promoting equity. BSA recommends against restricting model weights, as it could hinder innovation and global competitiveness. They advise policies to be evidence-based, encourage safeguards implementation, and support a robust AI ecosystem."
comment,2024-04-03T14:05:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0136,,,Anonymous,Comment from Anonymous,See attached file(s),"[('Hello!', 'https://downloads.regulations.gov/NTIA-2023-0009-0136/attachment_1.txt')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0136,
comment,2024-04-03T14:06:10Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0254,Partnership on AI,,,Comment from Partnership on AI,See attached file(s),"[('PAI Response to NTIA RFC - Open Foundation Models', 'https://downloads.regulations.gov/NTIA-2023-0009-0254/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0254,"Partnership on AI (PAI) is a non-profit partnership focusing on responsible AI development, connecting over 100 organizations globally. PAI's submission to the NTIA's RFC on dual-use foundation AI models emphasizes the need for evidence-based risk mitigation measures tailored to different model types. PAI's Model Deployment Guidance provides recommendations for safe model deployment practices, advocating for voluntary frameworks and proportionate risk management. PAI suggests that restrictions on models should be based on demonstrated necessity and proportionality. They also stress the importance of managing risks associated with open foundation models, particularly frontier models with potentially unprecedented capabilities."
comment,2024-04-03T14:06:03Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0239,Recidiviz,,,Comment from Recidiviz,"See attachment for the comment of Recidiviz, a nonprofit technology company that works with state level corrections and supervision agencies to improve outcomes in the criminal justice system, on the importance of open source technology to our work","[('Comment of Recidiviz to NTIA', 'https://downloads.regulations.gov/NTIA-2023-0009-0239/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0239,"Recidiviz, a nonprofit tech company working with state corrections agencies, emphasizes the importance of open-source software in criminal justice to ensure transparency and accountability. They stress the need for access to source code, model weights, training data, and evaluation methods to prevent biases and harm in AI models. Recidiviz believes open source technology is crucial for agencies to confidently deploy models that minimize unintended consequences in the justice system."
comment,2024-04-03T14:06:01Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0235,Transformative Futures Institute,,,Comment from Transformative Futures Institute ,"March 27, 2024<br/><br/>Stephanie Weiner, Chief Counsel, National Telecommunications and Information Administration | Travis Hall, Acting Associate Administrator at US Department of Commerce, National Telecommunications and Information Administration<br/><br/>Subject: Openness in Artificial Intelligence Models Request for Comment (NTIA&ndash;2023&ndash;0009)<br/><br/>Dear Mr. Hall and Ms. Weiner, <br/><br/>We greatly appreciate the opportunity to engage with NTIA and provide comments regarding the benefits and risks of Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights. This dialogue between the public and private sectors is crucial to enable a comprehensive understanding of the potential risks associated with these technologies and their intersection with society. <br/><br/>The Transformative Futures Institute (TFI) is a research group concerned with understanding and mitigating the risks from emerging technology, specifically artificial intelligence (AI). TFI has previously submitted responses to NIST regarding the development of the AISIC and the request for information regarding President Biden&rsquo;s Executive Order on the &ldquo;Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence&rdquo; on May 2, 2024.<br/><br/>The attached file includes the key comments on Openness in AI Request for Comment and further details for select questions. <br/><br/>Thank you again for the opportunity to comment on the NTIA&rsquo;s Openness in AI Request for Comment. If you need additional information or would like to discuss further, please contact Kyle A. Kilian at kyle@transformative.org or Ross Gruetzemacher at ross@transformative.org.<br/><br/>Best regards, <br/><br/>Kyle A. Kilian | Deputy Director <br/>kyle@transformative.org; kyle.a.kilian@gmail.com<br/>Phone: + 1-720-563-9267<br/><br/>Ross Gruetzemacher | Executive Director<br/>ross@transformative.org; rossgritz@gmail.com ","[('TFI Response to NTIA Openess in Artificial Intelligence Models Request for Comment NTIA20230009', 'https://downloads.regulations.gov/NTIA-2023-0009-0235/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0235,"The Transformative Futures Institute provides comments on the benefits and risks of Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights in response to the NTIA's request for public input. They highlight concerns about the inability to conduct thorough evaluations of unpredictable systems, risks of easier misuse and access for malicious actors, rapid diffusion of capabilities to adversaries, and the need for robust testing and evaluation regimes. They suggest structured access to models while maintaining control to prevent harmful misuse and the importance of continual safety evaluations. The Institute emphasizes the need for a shared understanding and trusted standards at the international level for effective evaluation and risk management."
comment,2024-04-03T14:05:41Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0199,,,Duncan,"Comment from Duncan, Samuel","Due to length limitations in the comment form, this is a brief summary of my comments. Please refer to Attachment 1, included with this submission, for my full comments.<br/><br/>As a software professional, I have followed recent developments in AI capabilities with growing concern about the risks presented by misuse or loss of control of powerful AI systems that may be developed in the future. I write to draw NTIA&rsquo;s attention to research that suggests that misuse of open foundation models will present unacceptable levels of risk to public health and national security. I recommend that the United States government develop regulations to prohibit making weights widely available for models more powerful than those currently available, and to hold foundation model developers accountable for securing the weights of their models.<br/><br/>(2)(a)<br/>Open models can easily be fine-tuned to produce harmful output, even when the foundation model has been trained to refuse requests for harmful output. Lermen et al. [https://arxiv.org/abs/2310.20624] demonstrate this risk: they show that after fine-tuning the Llama 2-Chat 70B model using LoRA with a goal of removing the model&rsquo;s safety training, the resulting model was almost always willing to respond to requests for harmful outputs.<br/><br/>(2)(d)<br/>In his testimony last year [https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf] before a Senate subcommittee, Anthropic CEO Dario Amodei described a study his company had performed, which concluded that an &quot;extrapolation of today&rsquo;s systems to those we expect to see in 2-3 years suggests a substantial risk that AI systems will be able to fill in all the missing pieces&quot; for harmful misuse of biology. Amodei warned that this &quot;could greatly widen the range of actors with the technical capability to conduct a large-scale biological attack.&quot; Future open models that can contribute to computer security research likewise present risks due to misuse.<br/>Qi et al. [https://arxiv.org/abs/2310.03693] discuss the risks from fine-tuning of both open and closed models by end users. They find that adversarial fine-tuning can substantially reduce the likelihood that these models will refuse to provide harmful output. The mitigations they suggest can be more effectively applied to closed models.<br/>Mouton et al. [https://www.rand.org/pubs/research_reports/RRA2977-2.html] analyze the risks from current LLMs being used in the development of biological weapons. They find no significant difference between the feasibility of biological attack plans developed by red-team researchers using LLMs versus researchers relying on Internet research alone. But their report notes that it &quot;remains uncertain...whether upcoming LLM iterations will push the capability frontier far enough to encompass tasks as complex as biological weapon attack planning.&quot;<br/><br/>(5)(a)<br/>Evaluations that fully elicit the capabilities of a model are necessary in order to make an informed decision about the risks and benefits of making its weights widely available. Because nobody can expect to control how an open model is fine-tuned, evaluations that focus on the foundation model&rsquo;s safety properties cannot provide any assurance about the risks of making its weights widely available.<br/><br/>(5)(b)<br/>Nevo et al. [https://www.rand.org/pubs/working_papers/WRA2849-1.html] discuss a security framework for protecting nonpublic model weights from malicious actors. This working paper is part of a study for which a full report is expected to be &quot;published in early 2024&quot; &ndash; I recommend that NTIA review the full report when it becomes available. <br/><br/>(7)(a)<br/>The United States government should impose a moratorium on making model weights widely available for AI models substantially more capable than those available now. It should also develop regulations to hold AI model developers accountable, through liability for damages or regulatory action, for harm caused by misuse of the models they develop, especially when they facilitate this misuse by making model weights widely available. I would also support an international agreement to indefinitely pause development and training of more capable AI models until reliable strategies to avoid the risks from such models are developed. The risks from more capable models &ndash; including potentially catastrophic risks from goal misgeneralization and loss of control of powerful AI systems, as well as risks from misuse of open models &ndash; are too serious to accept until substantial advances are made in AI safety research.<br/><br/>(7)(b)<br/>If fine-tuning of closed models proves to present risks unacceptable to the United States government, this can be addressed through regulation of model providers. If fine-tuning of open models presents such risks, effective regulation and enforcement may not be possible once the models have been released with widely available weights, due to the vast number of actors capable of obtaining and fine-tuning open models.<br/><br/>Respectfully,<br/>Samuel Duncan","[('Attachment1_ModelWeights_Duncan_full', 'https://downloads.regulations.gov/NTIA-2023-0009-0199/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0199,
comment,2024-04-03T14:06:03Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0240,Recording Industry Association of America,,,Comment from Recording Industry Association of America,See attached file(s),"[('3-27-24 Comments NTIA RFC dual use foundation AI models with widely available model weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0240/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0240,"The Recording Industry Association of America (RIAA) supports the National Telecommunications and Information Administration's request for comments on dual-use foundation AI models with widely available model weights. RIAA emphasizes the importance of protecting intellectual property rights and human creativity in AI development. They advocate for transparency in AI development and deployment, urging for proper record-keeping and disclosure of information. RIAA highlights concerns about unauthorized use of copyrighted works in AI systems and the potential risks and benefits of making model weights widely available, especially in the music industry. They stress the need for safeguards to protect creators' rights and promote responsible AI innovation."
comment,2024-04-03T14:06:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0246,The Abundance Institute,,,Comment from The Abundance Institute,"Please see the attached comments of The Abundance Institute, a mission-driven nonprofit dedicated to creating an environment where emerging technologies (including artificial intelligence) can germinate, develop, and thrive in order to perpetually expand widespread human prosperity and abundance.","[('Abundance Institute Comment to NTIA', 'https://downloads.regulations.gov/NTIA-2023-0009-0246/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0246,"The Abundance Institute's comments on the NTIA's request focus on the benefits of open source software in AI development, arguing that distinctions between ""open"" and ""closed"" models are arbitrary. They emphasize that open models offer advantages such as innovation, competition, and economic growth. The Institute suggests a use-specific, harms-based approach to regulation, highlighting the importance of market mechanisms in managing risks and maximizing benefits. They caution against drawing arbitrary lines between open and closed models and advocate for preserving the benefits of openness and innovation while deterring harms through targeted interventions. The Institute also raises First Amendment concerns regarding restrictions on open AI models and recommends a balanced approach that aligns with long-standing legal principles and individual responsibility."
comment,2024-04-03T14:06:26Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0283,,,Karth,"Comment from Karth, Isaac","We are writing this public submission in response to the NTIA AI Open Model Weights RFC (Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights, 89 Fed. Reg. 14059-14063 (Feb. 26, 2024)). The authors hold PhDs in Computational Media from the University of California, Santa Cruz. We have experience in working with many forms of artificial intelligence, including the kinds of models under question. We are writing this in our capacity as concerned private citizens, not as the representative of any organization or entity. Our full public submission is summarized here and attached as Attachment 1.<br/><br/>Current research in the fields of AI and of computer-supported collaborative work demonstrates that dual-use foundation models must be made trustworthy and secure. AI must not be used to shield decision-makers from responsibility for actions suggested by AI, nor from the burden of explaining their reasoning. Simultaneously, AI should not be used in capacities where it is readily exploited to subvert decision-making processes.<br/><br/>In many areas, the highest risk of harm is often not from the model weights, but rather what the AI is used for. It is most appropriate to involve outside stakeholders before new capabilities derived from AI models are incorporated into products, but this step is often skipped in practice, according to AI experts from industry and academia. In areas where beneficial use is possible, many AI systems developed in practice have failed to incorporate the perspectives of diverse stakeholders, and as a result caused harm.<br/><br/>Open access to model weights is important for building safe and trustworthy AI. Closed models are inherently less able to be evaluated, making them brittle: their safety relies on obscuring access. While closed models could be made somewhat more trustworthy by involving outside stakeholders in evaluating models, this is often skipped in practice.<br/><br/>Because it can be hard to detect biases and weaknesses in models, it is vital that researchers have access to open weights, so that they can independently audit them. Open models allows groups affected by model inequality to independently assess and retrain the models that affect them.<br/><br/>Keeping a model closed models makes it more vulnerable to certain attacks. Security through obscurity is brittle. Clones of closed models have been created relatively rapidly, and are available from many different countries. Knowledge distillation means that the mere availability of output generated by a model can be enough to create a functional-enough clone.<br/><br/>In other areas of computation, relying on secrecy as the main defense is regarded as a vulnerability in itself: secrecy means that friendly researchers can&#39;t assist in detecting and patching vulnerabilities, while still leaving you exposed to attackers who discover the exploits. Limiting access to the weights for the purposes of AI safety is a short-term solution with a high risk of leaving critical vulnerabilities undetected.<br/><br/>Limiting software distribution is difficult and ineffective, and too many restrictions on the distribution of open weights will damage the future development of AI in the United States, risking the country&#39;s current leadership in innovation.<br/><br/>The ``Gradient of Access&#39;&#39; model should not be understood as a linear scale of increasing risk: too little scrutiny can lead to critical vulnerabilities being overlooked, even when a model is never released as an open model. Further, the risk from any one set of open model weights being available must be weighed against the need to have the current generation of models audited for vulnerabilities so they can be caught and addressed at this stage, rather than remaining hidden until they inevitably surface in a later generation of models.<br/><br/>While there has been some concern about larger models exhibiting emergent effects, recent research suggests that model capabilities do not have sudden jumps in their capabilities as they scale up: a bigger model is more capable, but in a predictable way.<br/><br/>Overall, access to open model weights is important for continuing research into making the models safer, more trustworthy, and more equitable. Closed models may seem, at first glance, to be safer because they lock down access to the model, but this security is brittle and hides vulnerabilities that could have been discovered and fixed in an open model.","[('Attachment1_OpenModelWeightsAreMoreTrustworthyandLessVulnerable', 'https://downloads.regulations.gov/NTIA-2023-0009-0283/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0283,
comment,2024-04-03T14:05:30Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0178,,,Adkins,"Comment from Adkins, Samuel Brenton","Please see the attached file, but, in as short of a form possible.<br/>I believe by far the most important issue and topic here is preventing monopolization of AI space under a few big tech players. I can mention this again and again, but I cannot put enough weight on how dangerous and risky monopolization in such a case would be. Social media, when monopolized under a few companies, ended up horrible. Air travel in the United States currently has issues because Boeing exists in a de facto duopoly with Airbus and Boeing being only sane choices that many carriers are faced with. Whenever monopolization occurs, everyone but the company behind it suffers, and regulatory capture where regulations are created so that only few big companies can pass them, and all others are deterred from entering the field because it&rsquo;s too resource intensive to be compliant with regulation ends up only benefiting the companies. Not the country, not the people, not the technological progress. That&rsquo;s by far my biggest concern in the AI space. For more specific of a submission addressing the questions please consult the attachment. ","[('NTIA comment', 'https://downloads.regulations.gov/NTIA-2023-0009-0178/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0178,
comment,2024-04-03T14:05:59Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0232,AI Healthcare Working Group,,,Comment from AI Healthcare Working Group,See attached file(s),"[('Grogan Lopez NTIA', 'https://downloads.regulations.gov/NTIA-2023-0009-0232/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0232,"The AI Healthcare Working Group, led by Joe Grogan and Naomi Lopez, emphasizes the importance of open foundation models in AI healthcare to advance equity, rights, and safety. They advocate for dynamic, collaborative approaches to address risks, promote continuous improvement, and proactive defense strategies. The group believes that transparent practices, risk assessments, and retraining model weights can enhance AI systems in clinical drug development. They highlight the potential of AI to democratize information sharing and analysis, leading to advancements in human knowledge and equity."
comment,2024-04-03T14:05:54Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0221,The Last Mile,,,Comment from The Last Mile,"See attachment for the comment of The Last Mile, which provides opportunities for personal and professional growth for justice-impacted individuals, on the vital nature of open source AI to our work.","[('LastMile comment to NTIA', 'https://downloads.regulations.gov/NTIA-2023-0009-0221/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0221,"Jacob Briggs, Senior Manager of Education at The Last Mile, highlights the transformative impact of technology education on justice-impacted individuals. He emphasizes the importance of open-source AI and ML platforms in promoting equity and inclusion in technology education, particularly for marginalized communities. Briggs advocates for legislative support for open-source technologies to democratize access to cutting-edge tools, like TensorFlow, within restrictive environments such as correctional facilities. He believes that policymakers should prioritize the development of open-source platforms to ensure widespread benefits of technological progress and create a more equitable digital landscape."
comment,2024-04-03T14:06:29Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0289,Public Knowledge,,,Comment from Public Knowledge,See attached file(s),"[('NTIA WAMW Comment_PK', 'https://downloads.regulations.gov/NTIA-2023-0009-0289/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0289,"Public Knowledge emphasizes the importance of defining ""dual-use foundation models"" and promoting openness in AI systems. They suggest a nuanced understanding of the risks and benefits of open models compared to closed ones. Public Knowledge advocates for clear, scalable, and flexible regulations to support healthy AI development. They highlight the unique benefits of open systems, such as reducing barriers to competition, enabling innovation, and ensuring safety and accountability. Openness is seen as crucial for inclusive innovation and fostering diverse voices in AI development. Public Knowledge also stresses the need for policies that balance safety, accountability, and openness in the AI ecosystem."
comment,2024-04-03T14:03:56Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0006,,,Rosati,"Comment from Rosati, Domenic","Hello, I am a PhD student at Dalhousie Working on the safety of Large Language Models.<br/><br/>I have attached two files.<br/>My one page short comment on what my research on technical approaches mitigating the dangers of Open Model Weights and a suggested funding strategy.<br/>A pre-print of publication that is currently under for the International Conference of Learning Representations.<br/><br/>An executive summary is as follows:<br/>Open Model Weights are vulnerable to usage and training for harmful outcomes. A large part of this is through the removal of safety guards or training for harmful outcomes. We provide early research showing that this can be prevented. We argue that due to the existance of these defences and the ease of their verification, Open Model Weights should be required to have these defences in place.<br/>","[('Comment for NIST', 'https://downloads.regulations.gov/NTIA-2023-0009-0006/attachment_1.pdf'), ('2402.16382', 'https://downloads.regulations.gov/NTIA-2023-0009-0006/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0006,
comment,2024-04-03T14:06:13Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0260,,,Anonymous,Comment from Anonymous,Please see the attached PDF for detailed comments. ,"[('Attachment1_Open Weight Models', 'https://downloads.regulations.gov/NTIA-2023-0009-0260/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0260,
comment,2024-04-03T14:05:51Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0217,,,Gorwa,"Comment from Gorwa, Robert","Dear NTIA staff,<br/><br/>We appreciate the invitation from your colleague Christopher Quarles to submit a short comment to this interesting and important RFC. We are two interdisciplinary technology regulation academics (one based in the UK, and one in Germany) that have both been working in some capacity on issues relating to the topic at hand for many years. (MV has in particular been publishing on a wide range of issues pertaining to the regulation of automated decisionmaking and AI systems in Europe and beyond; RG&#39;s work focuses on the formal and informal regulation of/by platform services in a comparative context.)<br/><br/>We would like to provide a focused set of suggestions based upon our peer-reviewed academic article, `Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries.&#39; The paper has been openly available online since November 2023 and will be published in summer 2024 in the British legal journal Law, Innovation and Technology: https://doi.org/10.31235/osf.io/6dfk3.<br/><br/>That article is relevant to a few of the questions in the RFC, and is in particularly applicable as it relates to regulatory questions relating to the forms of public or semi-public model access that are mediated via online model hosting intermediaries. A few brief reflections based upon our research have been attached. <br/><br/>Dr Robert Gorwa <br/>Postdoctoral Research Fellow <br/>WZB Berlin Social Science Center, Germany <br/><br/>Dr Michael Veale <br/>Associate Professor in Digital Rights and Regulation<br/>Faculty of Laws, University College London, United Kingdom<br/><br/>","[('Gorwa-Veale_NTIA_Comment', 'https://downloads.regulations.gov/NTIA-2023-0009-0217/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0217,
comment,2024-04-03T14:06:28Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0288,,,Klemens,"Comment from Klemens, Ben",Please see attached.,"[('ntia-klemens', 'https://downloads.regulations.gov/NTIA-2023-0009-0288/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0288,
comment,2024-04-03T14:05:47Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0209,International Center for Law & Economics,,,Comment from International Center for Law & Economics,See attached file(s),"[('ICLE - NTIA COMMENTS RFC Open Foundation Models 2024', 'https://downloads.regulations.gov/NTIA-2023-0009-0209/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0209,"The International Center for Law & Economics provided comments on the NTIA's request for comments on ""Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights."" They emphasized the need for a balanced and forward-looking approach to AI regulation, cautioning against overly cautious stances that could stifle AI's evolution. They highlighted the importance of grounding AI regulation in actual harms rather than speculative risks, and recommended adaptive regulatory frameworks. Additionally, they discussed the benefits and challenges of open and closed approaches to AI development, emphasizing the need for flexibility in defining ""open"" and ""widely available"" model weights. They also raised concerns about the broad categorization of AI systems as ""dual-use"" and its potential impact on open-source AI development and international collaboration."
comment,2024-04-03T14:06:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0248,Mozilla,,,Comment from Mozilla,"Today, as AI is becoming an increasingly important part of the technology industry, we&rsquo;re seeing early signs of history repeating itself. In recent years, many leading players across the industry have moved towards less openness in AI, gating models behind proprietary APIs and documenting fewer and fewer details of the development process as well as about critical inputs and outcomes. Creating an incentive structure that promotes openness and knowledge-sharing thus becomes a key building block of any strategy aimed at driving progress across the U.S. economy &mdash; not just in a handful of well-resourced corporate research and development labs. <br/><br/>It&rsquo;s important we pay close attention to what&rsquo;s happened in the past to make sure we harness the benefits of openness while avoiding past mistakes. Open source software and &lsquo;open source&rsquo; AI aren&rsquo;t the same thing &mdash; but they&rsquo;re underpinned by many of the same principles and values. <br/>Against this backdrop, our submission to the NTIA&rsquo;s request for comments seeks to answer a range of important questions that warrant consideration in any conversation about openness in AI, with regard to the consultation&rsquo;s focus on widely available model weights but also beyond. Drawing on Mozilla&rsquo;s own history as part of the open source movement, this submission seeks to help guide difficult conversations about openness in AI. First, we shine a light on the different dimensions of openness in AI, including on different components across the AI stack and development lifecycle. Second, we argue that openness in AI can spur competition and help the diffusion of innovation and its benefits more broadly across the economy and society as a whole; that it can advance open science and progress in the entire field of AI; and that it advances accountability and safety by enabling more research and supporting independent scrutiny as well as regulatory oversight. In the past and with a view to recent progress in AI, openness has been a key tenet of U.S. leadership in technology &mdash; but ill-conceived policy interventions could jeopardize U.S. leadership in AI.  <br/><br/>Good policymaking on AI, and on openness in AI in particular, therefore requires a careful balancing of benefits and risks as well as analytical rigor in taking into account the various dimensions and actors in the AI ecosystem. Rash decisions and ill-considered solutions may cause irreparable damage to the &lsquo;open source&rsquo; AI ecosystem, and with it to the prosperity and safety of the American people. Against this backdrop, we make the following recommendations:<br/>- Impose proportionate and carefully considered regulatory obligations relating to &lsquo;open source&rsquo; AI. <br/>- Support the &lsquo;open source&rsquo; AI community in developing norms and practices around responsibly developing and openly releasing AI models and components. <br/>- Invest in and provide resources for the development and maintenance of &lsquo;open source&rsquo; AI.<br/>- Involve federal agencies responsible for protecting civil rights, promoting competition, and advancing scientific research in the development of any policy touching on openness in AI. <br/><br/>We hope these measures can help construct a better AI ecosystem &mdash; one that is more innovative, more competitive, and more accountable. <br/><br/>For more details, please find our full submission attached. ","[('Mozilla - RfC Submission - Dual Use Foundation Models With Widely Available Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0248/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0248,"Mozilla's response to the NTIA's request for comments on AI openness emphasizes the importance of openness in driving innovation, competition, and accountability in the AI ecosystem. They advocate for regulatory obligations, community norms, and investment in trustworthy AI to ensure public interest is served. Mozilla highlights the risks of decreasing openness in AI, drawing parallels to historical challenges in the internet industry. They stress the need for policymakers to consider diverse perspectives, adopt an expansive view of openness, and develop proportionate rules to support a more competitive and accountable AI ecosystem."
comment,2024-04-03T14:06:11Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0255,Google,,,Comment from Google,Please see attached,"[('Google NTIA AI Model Weights RFC Comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0255/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0255,"Google appreciates NTIA's request for comment on widely available model weights for dual-use foundation models. Google emphasizes responsible AI deployment and the importance of open science and software. They advocate for a spectrum of openness, rigorous risk assessment, and holistic evaluation of technology to balance benefits and risks. Google suggests developing standards, evaluations, and best practices for releasing models with widely available weights. They stress the need for careful testing, mitigations, and allocation of responsibilities to address potential risks associated with open models. Google also highlights the importance of collaboration on standards for responsible open science."
comment,2024-04-03T14:04:16Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0043,,,Anonymous,Comment from Anonymous,The parasites who occupy our decrepit shell of a government want to push this through in order to buy time until AI makes it impossible to blackmail people in positions of power. More info here: https://www.youtube.com/watch?v=xlmhhh9HYqc,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0043,
comment,2024-04-03T14:04:28Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0065,,,Anonymous,Comment from Anonymous,"Government regulation of open source AI or medel weight though necessary to some extent as AI becomes more powerful should not be taken lightly, done to early, or be too overbearing. Open source models are not just made by the US but also by other countries who may not put such restrictions in place. Regulation open source AI by necessity will slow its development and will weaken the US in comparison to other countries. <br/><br/>Should AI be limited to Large corporations or the Government alone it will solidify hierarchies as they are now in perpetuity, the individual citizen will have no recourse and will lose control of their own life and their pursuit of happiness, their liberty will also be at stake considering a government with nothing to fear from its people will inevitably oppress them. If there are no job, no means to climb out of poverty, and no hope to attain a better and more prosperous life for ourselves and our descendants there will be nothing worth saving anymore. The American dream will be truly dead except for those who have already attained it. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0065,
comment,2024-04-03T14:04:32Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0074,,,Anonymous,Comment from Anonymous,"The ban of Open AI Weights will drastically impact on the research inside of US and will increase amount of piracy. This also will impact on open market, making available only certain options (Midjourney, Dall E, etc), making a literal monopoly (because of insignificant quality of other commercial models), increasing a price, lower the gains from them, and making a really hard to research and push the progress forwars.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0074,
comment,2024-04-03T14:04:45Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0097,,,But anonyumous,"Comment from But anonyumous, ARtist",Horrible idea. Open source models must be protected at all costs. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0097,
comment,2024-04-03T14:05:09Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0140,,,Giles,"Comment from Giles, E.","The risks and benefits of AI models with open weights are broadly similar to the risks and benefits of closed-source AI models where the creator provides a public interface to interact with the model, such as ChatGPT and Midjourney. For example, it doesn&#39;t matter whether a text generation AI has open weights or not, if its input and output are publicly accessible, it can equally be used to enhance productivity at the office or generate propaganda.<br/><br/>The main possible benefit of closed-weights models and drawback of open-weights models is access control. The model might be integrated into a system that includes filters on the model&#39;s input and output, ensuring that all uses of the model adhere to the creator&#39;s usage policy. However, it is very difficult to implement a filter that automatically enforces typical policies such as &quot;no deceptive content&quot; or &quot;no political messaging&quot;, because it&#39;s not clear from the input and output alone what constitutes &quot;deceptive&quot; or &quot;political&quot; content. In addition, you can&#39;t assume that any organization that creates an AI necessarily values the same things you do; it only costs about $1 million (and falling) to train a high-quality AI model.<br/><br/>The main benefits of an open-weights model are data privacy and freedom. The AI model can be prompted with private information without needing to transmit that information to the AI&#39;s creator. This is useful for companies wishing to use AI to compose and/or edit their internal documentation or proprietary code, without needing to disclose those trade secrets to an AI hosting company. It is also possible to use open-weights models for lawful purposes that are nonetheless prohibited by many AI companies&#39; terms of service, such as political humor/parody or sexually explicit content.<br/><br/>Open-weights models also carry the advantages of open science in general - they encourage faster innovation on new AI techniques - preparing datasets, training models, deploying models with resource constraints, adversarial attacks and defenses against those attacks.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0140,
comment,2024-04-03T14:05:36Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0188,,,Beverage,"Comment from Beverage, Bob","The rapid advance of tech and communications have been enabled by the open spirit of the internet.  There should be minimal regulation on open products, even AI.  Regulation should be focused on closed products which do not have public oversight.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0188,
comment,2024-04-03T14:04:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0030,,,Anonymous,Comment from Anonymous,"If you believe in democracy, open weights are the way to go. Why would you give this privilege only for some powerful corporations with extreme biases and agendas? It won&#39;t be any good in long term.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0030,
comment,2024-04-03T14:04:12Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0037,,,Hernandez,"Comment from Hernandez, Michael","To whom it may concern:<br/><br/>I am writing to provide comments on the important issue of openness in artificial intelligence (AI) foundation models, and specifically the implications of making model weights widely available. As an AI developer and concerned global citizen, I believe this issue deserves careful consideration through the lens of a principled, expansive ethical framework.<br/><br/>A key tenet of such a framework is to expand our circle of moral consideration beyond narrow self-interest to encompass the wellbeing of all of humanity, present and future. From this &quot;global utility&quot; perspective, we must strive to develop and deploy AI technologies in a way that maximizes benefits and minimizes risks for the greatest number of stakeholders worldwide.<br/><br/>Open AI foundation models have the potential to widely democratize access to this transformative technology - empowering researchers, entrepreneurs, and public interest users to develop beneficial applications in fields like healthcare, education, scientific discovery, and more. Openness enables scrutiny of models for safety and equity issues. It can foster competition and innovation. Making model weights available allows fine-tuning models for diverse contexts and needs. In these ways, openness aligns with the ethical imperative to expand access to AI&#39;s benefits.<br/><br/>However, as the RFC notes, widely available AI model weights also pose risks and challenges that require proactive mitigation. These include potential for malicious use, lack of oversight and accountability, exacerbating inequities, and unintended safety issues. Bad actors could exploit open models to generate hazardous content at scale. Rapid proliferation could outpace our preparedness.<br/><br/>Expanding access is not an unqualified good if it enables harmful applications. A utilitarian approach calls us to rigorously assess the net welfare impact of different levels of openness. Where granting access threatens security, rights, or public safety, restrictions may be ethically justified and socially necessary. We have a moral duty to anticipate, monitor and defend against worst-case scenarios.<br/><br/>At the same time, from a deontological view, we must uphold principles of fairness, intellectual freedom, and equitable access as key ethical constraints. Imposing limits on openness unilaterally and without due process risks unjust restrictions, entrenching power disparities, and stunting innovation. Wherever possible, the global AI community should collaborate on voluntary best practices, norms and safeguards to mitigate open model risks.<br/><br/>Guided by the overarching ethical framework outlined above, I believe a balanced approach is called for. Model developers should embrace openness and wide accessibility of weights as a default, given its significant benefits. However, this should be coupled with responsible practices such as:<br/><br/>- Implementing thoughtful release strategies that gradually expand access while monitoring impacts<br/>- Clearly communicating model capabilities, limitations, and intended uses to prevent misuse<br/>- Providing guidance and tools to promote safe and beneficial deployment<br/>- Embedding safeguards against malicious use where feasible<br/>- Contributing to multi-stakeholder processes to develop AI security standards and norms<br/>- Supporting ongoing research to better characterize open model benefits, risks and tradeoffs<br/>- Exploring innovative approaches like access licenses conditioned on responsible use<br/><br/>On the policy front, I believe governments have an important enabling role to play by:<br/><br/>- Convening inclusive dialogues to align on common principles and recommended practices<br/>- Increasing public investment in shared AI infrastructure and open foundation models<br/>- Developing clear guidelines for appropriate openness and restrictions in high-stakes domains<br/>- Modernizing legal frameworks to account for new openness challenges around IP, liability, etc.<br/>- Engaging internationally to harmonize approaches and uphold ethical norms<br/>- Monitoring the open foundation model ecosystem and adjusting governance over time<br/><br/>By taking a principled, proactive, and collaborative approach, we can capture the immense benefits of open AI foundation models while mitigating risks and advancing a more fair, inclusive, and beneficial AI future for humanity. We must adapt timeless ethical wisdom to expansively consider the global impacts of our choices in this pivotal moment.<br/><br/>Thank you for the opportunity to provide input. I commend NTIA&#39;s thoughtful consultation and stand ready to further assist in whatever way I can.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0037,
comment,2024-04-03T14:04:14Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0040,,,Duong,"Comment from Duong, Van","I am writing a comment that addresses most closely the question 3(a) within the regulatory document but involves more. The potential for AI models to impact competition and innovation within the entire economy is immense and, in fact, has been realized to an extent already. For this reason, it is best to ensure that open models remains freely and widely available for the best benefits to be achieved.<br/><br/>Since the release of many significant AI models, e.g. ChatGPT or Midjourney, the tech sector has underwent enormous changes in order to utilize these tools which has captured the public imagination and hearts. Large American companies, including Google, Microsoft, Facebook, Snapchat, and many others have spent billions to integrate existing AI solutions and/or to create their own solutions. Yet other companies are now testing the potential to integrate AI solutions into products which are not normally suited to such tools, e.g. Ubisoft and their video game products AI has already has significant impact in entertainment, technology, art, educational, and scientific fields and much greater impact is promised.<br/><br/>Most recently, OpenAI has released an AI which allows users to create highly well crafted videos using simple text. The potential from this alone is vast.<br/><br/>All of these tech evolutions have made their way into open models and, in cases, have been greatly improved upon. StableDiffusion is such an example and, most recently, StableDiffusion has created demos showing vast improvements in performance on devices with lower system specifications and performing feats previously thought impossible.<br/><br/>It is by their very nature of being open and widely available that they can achieve this. At any given point, large teams of dedicated researchers, developers, and hobbyists are working on these models finding methods to improve how they work. If such models were restricted or as closed as other known models, e.g. ChatGPT, such innovation could not be achieved.<br/><br/>This is why it is very important for restrictions and limitations to be carefully considered and be avoided as much as possible. The economy and the public benefits enormously from the availability of open models which allows modern AI technology to be used freely or at a much lower cost and to progress at a much faster rate. These benefits can only be achieved when those working on these models have access to them and aren&#39;t restricted on how they can improve them.<br/><br/>To limit this means to limit the benefits to the country while incentivizing the public to use open models from other nations that would not have such restrictions.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0040,
comment,2024-04-03T14:04:16Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0044,"Zabaware, Inc.",,,"Comment from Zabaware, Inc.","Subject: Detailed Comment on Open Foundation Models and Model Weights<br/><br/>Dear NTIA,<br/><br/>As the CEO of Zabaware, Inc., a small AI company leveraging open-source AI models, I am pleased to provide our insights and recommendations in response to the NTIA&#39;s request for comments on dual-use foundation AI models with widely available model weights. Our work significantly benefits from the innovation and accessibility fostered by open source AI models, particularly those like LLAMA 2, which are foundational to our development process.<br/><br/># Defining Openness and Wide Availability<br/><br/>1. Openness should encapsulate not just the availability of model weights but also their accessibility without undue barriers. This includes minimal restrictions on use, ease of access, and the provision of necessary documentation and support. Open foundation models, such as LLAMA 2, have historically shown that weights can become widely available post-release, significantly aiding innovation.<br/><br/>2. The timeframe between the deployment of closed and open models varies widely based on community interest, funding, and technological advancements. A trend towards faster openness is anticipated as the AI field grows more collaborative.<br/><br/>3. Wide availability could be quantitatively defined by the number of entities actively using or having access to the model. However, a qualitative approach considering the ease of access and the level of support provided to users might be more appropriate, fostering a more inclusive AI development ecosystem.<br/><br/>4. Different forms of access (e.g., APIs, local hosting) offer varying benefits and risks. Direct access to model weights enables deeper innovation but might raise security concerns. Web applications and APIs offer controlled environments that mitigate some risks but may limit innovation. A balanced approach, providing multiple forms of access while ensuring robust security and privacy protections, is advisable.<br/><br/># Risks and Benefits of Widely Available Model Weights<br/><br/>1. Risks include potential misuse of AI technologies, privacy concerns, and the exacerbation of security risks. These risks can be mitigated through community-driven standards, ethical guidelines, and transparency in AI development and deployment processes.<br/><br/>2. Benefits of open model weights far outweigh potential risks, fostering innovation, competitiveness, and the democratization of AI technology. Open models enable small companies and researchers to contribute to and benefit from the AI revolution, driving societal and technological advancements.<br/><br/>3. Open foundation models advance equity in rights- and safety-impacting AI systems by broadening access to cutting-edge technologies, thus enabling diverse applications tailored to specific needs and challenges.<br/><br/>4. Safety and technical issues can be addressed through comprehensive evaluations, development of safeguards, and community collaboration to ensure responsible use of AI technologies. Open source practices, including version control and peer review, can significantly enhance the safety and reliability of AI models.<br/><br/># Legal, Business, and Regulatory Considerations<br/><br/>1. The open-source software policy provides valuable lessons for managing open foundation models, emphasizing the importance of clear licensing, community engagement, and the balance between openness and proprietary interests.<br/><br/>2. Regulatory frameworks should encourage innovation while ensuring safety, privacy, and security. Voluntary standards, developed in collaboration with the AI community, can provide flexibility and adaptability essential in the fast-evolving AI landscape.<br/><br/>3. International collaboration is crucial in establishing norms and standards for open foundation models, ensuring that benefits are globally accessible while mitigating risks across jurisdictions.<br/><br/>Conclusion<br/>Zabaware, Inc. strongly advocates for the continued support and development of open foundation models. We believe that a balanced approach, addressing potential risks while maximizing the benefits of openness, will best serve the AI community and society at large. We appreciate the NTIA&#39;s effort in soliciting feedback on this important issue and look forward to contributing to the development of policies that support open, innovative, and responsible AI.<br/><br/>Sincerely,<br/><br/>Robert Medeksza<br/>CEO, Zabaware, Inc.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0044,
comment,2024-04-03T14:04:22Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0055,,,Sanchez,"Comment from Sanchez, Jarod","As an American who has had my data used to create the AI technology, I believe that there should be open models available that build on past models. By not allowing openness in the AI space will shutdown competition and only allow the few to have systems that will be part of all of American&rsquo;s lives. With the labor force being squeezed to nothing by these technologies the American citizen should have as much ability to raise themselves up like the few companies that built current AI models off American&rsquo;s data. That can happen with open AI models and individuals being able to use the tools of corporate entities to propel themselves to a better position. This could help people avoid welfare and government assistance to contribute to the economy. With more American citizens able to build their own wealth equal to large companies, the U.S. political system can start drawing more contributions from American citizens directly instead of being beholden to corporate power for campaign contributions. Giving power to the American citizenry like those to large companies will benefit our country on the world stage being able advance this novel technology. <br/><br/>To only leave this power to the few with vast funds and fulfilling needs of investors outside of the U.S.; will be a mistake. One where the citizens of our country are vastly behind the 8-ball unable to compete with the rest of the world. Believe in the American people, as we need to sure up our personal futures with this amazing new technology. Just to leave this in the hands of the few would be the biggest mistake our country could make. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0055,
comment,2024-04-03T14:04:34Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0077,,,Anonymous,Comment from Anonymous,It would be best if we support open source development of AI models for more collaboration and thus getting better and faster results. We should note that other countries like Japan are already supporting generative AI and LLMs research by giving more support through laws. As India is already lagging behind in AI research I think we shouldn&rsquo;t put too many constraints that will limit open research and collaboration especially in a ground breaking industry like AI. The future will be decided by countries that produce the best AI technology and implement them at scale . With one of the worlds largest IT talent pool India should strive for better regulations that inspire more collaboration and we shouldn&rsquo;t repeat another licence raj,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0077,
comment,2024-04-03T14:04:45Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0098,,,But anonyumous,"Comment from But anonyumous, ARtist",Horrible idea. Open source models must be protected at all costs. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0098,
comment,2024-04-03T14:04:54Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0112,,,Harmon,"Comment from Harmon, Colton","&ldquo;Until philosophers are kings, or the kings and princes of this world have the spirit and power of philosophy, and political greatness and wisdom meet in one, and those commoner natures who pursue either to the exclusion of the other are compelled to stand aside, cities will never have rest from their evils.&rdquo; &ndash; Plato<br/><br/>Philosophically, nations become prone to systemic risk when too much power is centralized. There is power in diversity of thought, philosophy, vision, spirit, practice, etc. It is a better world where power is decentralized. I urge you to decentralize the powers of Ai",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0112,
comment,2024-04-03T14:05:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0135,,,Houser,"Comment from Houser, Brandon","As an IT professional and cybersecurity analyst, I believe that foundation models with widely available model weights offer significant benefits compared to fully closed models. We can draw insights from the current cybersecurity ecosystem, where hacking tools, exploits, malware, and defensive tools exist in a mostly open form. This openness allows professionals, as well as skilled individuals from diverse backgrounds (including those with limited access to formal credentials like degrees or certifications), to contribute to the field of cybersecurity defense.<br/><br/>If we want to ensure the security of AI systems, having an open ecosystem could be advantageous. With more &quot;eyes&quot; scrutinizing the models and their potential vulnerabilities, we can identify and address weaknesses more effectively. Locking down the AI industry and restricting access to model weights could hinder the development of a collaborative ecosystem, where all stakeholders &ndash; governments, corporations, and individuals &ndash; can benefit from collective efforts to enhance security.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0135,
comment,2024-04-03T14:05:14Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0147,,,Smith,"Comment from Smith, Joshua",I just graduated from college in Mathematics. I want to become an entrepreneur in AI. I&rsquo;m scared the big players are lobbying for regulatory capture to prevent people like me from becoming competitors so they can monopolize this new field.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0147,
comment,2024-04-03T14:05:26Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0169,,,Jennings,"Comment from Jennings, Michael",Please do not regulate these crucial models for small startups.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0169,
comment,2024-04-03T14:05:28Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0173,,,LaGuardia-LoBianco,"Comment from LaGuardia-LoBianco, Gino",Logits of models should be protected by the 1st Amendment. Stop fucking around and regulate Fox News instead. Learn how to code and spend a few semesters in advanced math courses before you try legislating on something you don&#39;t understand you useless (overpaid) cocksuckers.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0173,
comment,2024-04-03T14:04:09Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0032,,,Huffman,"Comment from Huffman, Richard","Very simply, this is a cross between 1. mandating what sizes and styles of wrench are permissible under government regulation as opposed to allowing the industry and consumers to set standards, 2. a possible restraint on free speech, and 3. a definite restriction on artistic expression.<br/><br/>The danger in open models does not come from the models themselves, it comes from users who are capable of affording the huge hardware investment into running them at scale for the production of realistic works.  Regulation on creating deepfakes that are outside of the satirical/parody exceptions would be much more effective, and tying the penalty to the value of the hardware used to generate the offending material.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0032,
comment,2024-04-03T14:03:59Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0012,,,Anonymous,Comment from Anonymous,"The regulation of open model weights would be censorship of a heavy hand and would force ridiculous work arounds. This would also limit the speed of AI Model development and hinder sharing in any meaningful way. If anything, this will push the generation of foundation models overseas. VPNs are still a thing. Again, this entire idea of regulating open model weights is ridiculous and should not be passed or discussed. This all would be a violation of our first amendment.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0012,
comment,2024-04-03T14:04:00Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0014,,,sirc,"Comment from sirc, ivax","What worries me about AI is that robots have artificial intelligence and are used for military purposes, it is the only thing that worries me and I am convinced that there will be no regulations in this",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0014,
comment,2024-04-03T14:04:01Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0016,,,Anonymous,Comment from Anonymous,"Corporations have an obvious conflict of interest when advancing the idea that open model weights should be regulated. Obviously trillion dollar tech conglomerates would prefer to have a monopoly on the most important software of a generation. There are basically no concrete examples of meaningful societal problems caused by open model weights. There are only a few vague, theoretical catastrophes that have never been described in detail but are panicked about with hand-wavy assertions that equate artificial intelligence with magic powers.<br/><br/>This effort to regulate open model weights is a transparent power grab by big tech.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0016,
comment,2024-04-03T14:04:02Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0018,,,Taylor,"Comment from Taylor, Chad",Blocking open models will suppress innovation and technology advancement in USA while other countries will move forward in leaps and bounds. It will likely trigger AI-focused companies to reconsider their presence in the US and some will likely leave immediately. The long term ramifications of this proposal could impact technology in the US for years to come. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0018,
comment,2024-04-03T14:04:04Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0022,,,Schultz,"Comment from Schultz, Adam ","Although there are risks with open models, there are very little about the national security risks on AI. I believe what should be regulated is the use by the individual using the model that&rsquo;s already applicable under law. If someone is using a disinformation campaign, allow the one who used the AI to generate that to be liable under laws like defamation, libel, etc&hellip;. AI modes themselves are platforms.<br/><br/>Open source models are a lot less capable than the top models due to them designing them for lower hardware and mostly only have access to previous information without techniques that have internet access like RAG. Most companies don&rsquo;t provide training code so it&rsquo;s difficult for most bad actors to fine tune and remove the safeguards. It requires a lot of data to undo them without doing what some call overfitting which nukes a lot of the other capabilities of the models in order to maximize the result. This means it lowers its utility to provide a danger and the effort to get all the data and annotation significant.<br/><br/>I think the risks behind regulating open source models is worse for both the economy as it would exasperate inequality and destroy social mobility crucial to the American dream. Most of the time the dangers of most AI can be countered and patched up quickly. Disinformation can be solved with social media moderation and the effectiveness of these campaigns will lessen in the future when people better know AI can deceive you. Already an example happened to a guy close to Putin with a fake recording or not. It was a disinformation campaign to get him in trouble with Putin. He told the news was done by AI and not him and everybody moved on and stopped believing in it. Similar situations happened with photoshop during the early days of the internet and people just need time to adjust.<br/><br/>I am not saying no regulation in cases where there are actual risks like self driving cars or where there is hardware that can be used to harm people, but the risks for most AI models contained in software are novel compared to other software tools. <br/><br/>AI can&rsquo;t just take over everything and hate humans for no reason like the films. AI can&rsquo;t magically crack a security key. It&rsquo;s computationally impossible to generate tokens at a fast enough rate to crack a key in a lifetime. AI doesn&rsquo;t have feelings. It&rsquo;s designed to optimize objectives. It rarely veers off unless it&rsquo;s to maximize an objective. <br/><br/>Under the case AI is used to try to penetrate state secrets under non-state actors, the reverse is also true for the defense. Open models allow the government to maintain the privacy of their prompts and give it access to the classified files without disclosing it to the internet. National security benefits from the increased capabilities of the community driven models. <br/><br/>Open models unlike closed source models respect privacy much more than closed source models that have security risks of data concentration. Google doesn&rsquo;t allow you to opt out of training data. OpenAI still keeps your data on their servers. OpenAI has been known to have security risks that leaked other data from users on accident. If such a case were to happen with closed source models, it would be a disaster. Government employees should use their own private LLM to maintain confidentiality. Regulating Open models and using closed models instead would leave a security risk for non-state actors to hack closed source AI companies to access state secrets.<br/><br/>Enforcing bad actors is impossible as distribution of files are very simple with the internet. Regulation will only slow down the good actors and prevent them from mitigating the risks of bad actors.<br/><br/>While story in point, don&rsquo;t regulate open source models. You lose more than you gain and threaten the US lead in AI to China. Which will hurt democratic values and freedom of speech.<br/>",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0022,
comment,2024-04-03T14:04:11Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0035,,,O'Hanlon,"Comment from O'Hanlon, Zachary",Allowing open models is free speech. Closed models are dangerous as we have just seen with Google&rsquo;s Gemini model suggesting that the United States is evil. Open models allow people to inspect what&rsquo;s going on behind the curtain. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0035,
comment,2024-04-03T14:04:30Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0069,,,Anonymous,Comment from Anonymous,"To regulate not only tools, but the free and open creation of tools, is an overstepping of people&#39;s creative freedoms. While regulation against specific uses of tools is common, the regulation of the tools themselves and their creation is not- to ban a pen that could write threatening words or to ban the open sharing of the engineering of a lathe that could machine a weapon may seem safe, but is in it of itself a more potent danger than what those actions may wish to protect from. We are entering an era of technological marvels that will surpass even the difference from pre-internet to modern technology, and the last thing we can afford to do is to limit that innovation domestically and hand over those discoveries and innovation to exclusively foreign innovators. The open source movement is an inherently democratic movement, that free people can cooperate and construct together the things they wish to see in the world, and to target that specifically for restrictions while allowing large corporations the exclusive right to shape what this next era of technological advancement, and with that the society that is shaped by that advancement, without the direct and immediate oversight and decision making of the people themselves is dangerous and disheartening. It is with the most sincere concern for the future of this country and the world that this comment urges you to avoid the regulation of open source generative tools, but to instead pursue directly the products of those tools being shared and distributed, as we would with any other potentially dangerous product of a tool. We do not regulate text editors to prevent illegal text from being written, we do not stop the development of pencils to avoid them from skirting laws regulating speech, we target specifically the instances of these byproducts of human bad actors using the amazing tools at their disposal for poor ends. Let us not fall into the trap of throwing away our tools to save ourselves the hassle of dealing with the things they may create- for the tools will be made, weather domestically or abroad, and the global internet will disperse their creations globally. <br/><br/>Thank you for your consideration, and for the due consideration you give this weighty subject.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0069,
comment,2024-04-03T14:04:35Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0080,,,Frey,"Comment from Frey, Chris","While I acknowledge the security risks associated with open-source AI models, I believe the benefits outweigh the drawbacks. Proactive security through a wider community examining the code can identify and address vulnerabilities before malicious actors exploit them. A closed-source approach, while potentially delaying access by malicious actors, also hinders proactive security efforts.<br/><br/>However, strong governance and best practices are crucial for maintaining code quality and mitigating risks in open-source AI projects.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0080,
comment,2024-04-03T14:04:38Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0084,,,Anonymous,Comment from Anonymous,"AI model weights should remain open, so that independent research and development can continue. Transparency in both process and foundation will ensure rapid, safe, and verifiable progress. Thank you.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0084,
comment,2024-04-03T14:05:00Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0122,,,,Comment from ICAI Icelandic Center for Artificial Intelligence,"Dear NTIA,<br/><br/>I, Vicente Carro, CEO at ICAI Icelandic Center for Artificial Intelligence, and Executive Master in Artificial Intelligence, strongly believe that banning AI models with open weights would be detrimental to the United States&#39; interests and global progress in AI research and development.<br/><br/>Banning AI models with open weights would be ineffective and counterproductive, as other countries are unlikely to follow suit (i.e., Europe and China). It would put the U.S. at a significant disadvantage, hindering collaboration, knowledge sharing, and innovation. The real threat lies in AI-powered attacks, from manipulation of elections such as deep fake videos and fake news, as seen in the 2016 U.S. presidential election, to coordinated, multi-layered, multi-target hacking attacks against critical infrastructures and assets; all of which can already be done with current AI. The solution is to invest in AI-driven defenses, not to ban open models.<br/><br/>Plus, banning open models could backfire due to psychological reactance, leading to the creation of underground communities that are harder to monitor, as observed in the context of online piracy. The academic and economic impact would be severe, stifling research, entrepreneurship, and the U.S.&#39;s ability to capitalize on the projected $997.77 billion global AI market by 2028.<br/><br/>Banning open models could lead to a situation where only bad actors develop AI, making it harder to track and prevent attacks. For instance, a rogue state or terrorist group could develop an AI-powered weapon system using proprietary algorithms and datasets, making it nearly impossible for defense agencies to anticipate and counter the threat. While having open models would create a path of least resistance, easing defense agencies work  by narrowing down the amount of potential vectors. <br/><br/>Instead, the U.S. should focus on developing a comprehensive strategy to protect against AI attacks while fostering innovation and growth in the AI sector. This should include investing in AI-driven defenses(from consumer grade devices to military), promoting responsible AI development through guidelines and best practices, collaborating with international partners to establish global standards, and investing in AI education and workforce development.<br/><br/>The most critical action, IMHO, is to intervene companies like OpenAI, Google DeepMind, and Microsoft AI, which possess an unprecedented level of unchecked power. The mere fact that they are publicly and actively pursuing AGI/ASI should provide legal grounds for immediate intervention. If, in any other industry, a player were trying to fund an army or  create a nuclear bomb they would be banned or subject to the most strict regulations and oversight. However in the AI industry, companies can develop powerful AI tech with the potential to cause unprecedented damage with little to no regulatory framework in place. The government should take steps to place officials (and potentially military, but only as observers/liaisons) on their boards, maybe forcefully sell shares in these companies, and require frequent and periodic public reports on their AI development progress. A dedicated agency should be established to oversee AI development and deployment, with the authority to intervene when necessary (as a norm, not an exception).<br/><br/>Regulations should not stifle innovation or restrict legitimate AI use, such as in adult entertainment or anime industries, which, personal judgements aside, are the two main engines of non-academic AI model development with open weights by the general population. The focus should be on preventing misuse and ensuring wide distribution of benefits. The U.S. should invest in AI-smart devices and (upcoming, if not already here) AI defense &ldquo;chips&rdquo;, mandate their incorporation into all technological devices, and launch a public education campaign to raise awareness about AI misuse and its potential consequences. And yes, the government should expect punctual attacks from troubled teenagers, plus other citizens, (as already happened over time), but as mentioned before, with almost certainty they will use the path of least resistance, using open models, if available. This could be critical for the ability of the defense forces to prevent, minimize and rapidly react to such attacks.<br/><br/>In conclusion, banning AI models with open weights would be misguided and ineffective. The U.S. should embrace open models as a catalyst for innovation while implementing robust safeguards and oversight measures. Immediate action must be taken to intervene in companies with the power to develop close-to-AGI or close-to-ASI models, ensuring that no company can rule or overrule the nation. By taking a proactive, comprehensive, and assertive approach to AI governance, the U.S. can harness AI&#39;s potential to improve lives and create a more prosperous future while safeguarding its interests and values.<br/><br/>Thank you for considering my feedback on this critical issue.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0122,
comment,2024-04-03T14:05:00Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0123,,,Mande,"Comment from Mande, Matt","While I recognize the immense potential these technologies hold for advancing human knowledge and capability, I harbor deep concerns regarding their potential to mimic human intelligence too closely. My apprehension stems from the potential consequences of creating AI entities capable of surpassing human intellect, potentially leading to scenarios where control and predictability are lost.<br/>While open-sourcing is generally considered a beneficial practice for promoting innovation and collaboration, the open-sourcing of advanced AI models necessitates careful scrutiny. My concern lies in the potential for an AI arms race, as entities (be they nations, corporations, or individuals) rush to develop or acquire the most powerful AI capabilities. The creation of entities that can understand, learn, and reason at or beyond human levels presents existential risks. Such models, if unchecked, could outpace human ability to understand, predict, or control their actions. The primary concern here is not the AI of today but the superintelligent systems of tomorrow, which might emerge from iterative improvements and self-enhancement capabilities intrinsic to these models.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0123,
comment,2024-04-03T14:05:02Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0127,,,Sharp,"Comment from Sharp, Robert Douglas","I lead research for an AI startup in the healthcare space,  and I would like to respond to your request for public comments on open-weight AI models.<br/><br/>There is a critical shortage of physicians in the United States, and the product we sell helps make each physician more effective, by acting as a force multiplier.  We are able to reduce the time required to arrive at and document an accurate and informed diagnosis, and then help doctors to quickly take appropriate action based on that diagnosis.  <br/><br/>As a specific example, physicians spend a great deal of time documenting their patient encounters; in many cases they can spend more time on documentation than in face to face consultation with the patient.  Our system makes the generation of the full documentation associated with a diagnosis less burdensome and time consuming.<br/><br/>One of the key technologies we are applying to the goal is large language model AI (LLMs),  and in some cases our LLMs are derived from open weight foundation models, including Llama-2.  The existence of open weight models is a key enabler of our innovation, and allows us to move faster than we would otherwise be able to.<br/><br/>But it should also be pointed out that if there were no open-weight models, the existence of open corpora (for example RedPajamas) combined with academic publications would still allow us to achieve the same goals, however it would (1) slow us down considerably, and (2) waste a great deal of computational resources and electricity, as we would have to go about creating our own foundational models from the corpora.  Any meaningful regulatory impact from banning open weight model sharing would need to also ban the associated training data and publication of AI research.<br/><br/>The United States is in competition with the rest of the world, and  enacting measures that would hobble and slow critically needed innovation will just hand a major advantage to anyone pursuing AI outside the US.  It would also have the effect of concentrating power and capability in the hands of a few large corporations, which would accelerate and increase any potential for regulatory capture.  <br/><br/>Imagine if back when gasoline powered cars were being invented, if laws had been passed to make it illegal to share automotive plans.  Automobiles were to become a major driver of the economy, but also led to the rise of big oil and climate impacts that threaten the human race.  Would any of those adverse effects have been lessened by laws against sharing automobile plans?  I doubt it.  But it could potentially have aided foreign versus domestic economies to grow faster, by advantaging them over domestic manufacturers.<br/><br/>The laws around AI need to be squarely focussed on banning use cases that could be harmful, and not the sharing of core technology.   And the use cases (and the real dangers) will emerge over time, so the regulations governing AI will have to emerge as we see the negative consequences emerge.<br/><br/>I would be happy to discuss this further with anyone who would like more information either about our system, our approach to creating innovative technologies, or go into more depth on the subject at hand.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0127,
comment,2024-04-03T14:05:20Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0159,"Allied Mission Group, LLC",,,"Comment from Allied Mission Group, LLC","1. Open or widely available models and model weights are those that are easily accessible by the general public.   The location of the models and the model weights are known.<br/>1a. There are time limits on patents in the United States and it would seem that a similar arrangement would apply for model weights.<br/>1c.  &ldquo;Wide availability&rdquo; is a state regardless of the extent of its distribution.<br/>2.    There are greater risks with widely available model weights than there are with non-public model weights.  There are little or no controls over widely available model weights which means that they could be modified or used in ways in which they were not intended.<br/>2a. As stated above, if there are little or no controls over widely available model weights, they could be modified or used in ways in which they were not intended.  If the data used to train the models is also available, that will increase the risk of an undesired outcome.<br/>2b. If the security and integrity of open foundation models are compromised, this could adversely impact the AI Systems&rsquo; decision making on equity and social issues.<br/>2c.  The wide availability of model weights does not pose a direct privacy risk.   However, the way in which the weights are used in AI systems could allow a disclosure of PII data if the weights are altered to provide a result.<br/>2d. State or non-state actors could modify the model weights to adversely affect the model outputs.  After the modification, the weights could be reintroduced for use in ways not intended.<br/>2di.  Closed models assume a level of control over who can make changes to the model weights and accountability and an audit trail of modifications.<br/>2dii. Software supply chain security risks exist in any development environment that need to be identified, managed, and monitored.<br/>2f. The most severe risks are the ones that could result in a loss of life and/or adversely impact public safety.  Given the highly interconnected digital world in which we live, there may very well be repercussions on multiple systems if a failure or compromise was to occur.<br/>3. The benefits of foundation models with widely available model weights are reduced time to market for this who can use the models, potential standardization of models for specific use cases, and collaboration and the opportunity for continuous improvement.<br/>3c. This would seem to depend on the accuracy of the data used to retrain the models.<br/>5c. As we learn more about AI foundation models, I am sure there will be advances in developing safeguards.    However, there is always the risk that our adversaries will have more focus and resources to identify ways to circumvent any safeguards.<br/>6.  Depending on the specific use case, there could be liability issues associated with the outcomes of the open foundation models.<br/>6b.  The wide availability of model weights could help level the playing field with all companies having access to the same models.  That could foster increased competition and innovation as firms tune the models to deliver a differentiated product.<br/>8c.  I suggest starting with NIST AI 100-1, AI Risk Management Framework and use the Measure function.<br/>",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0159,
comment,2024-04-03T14:05:23Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0163,,,Anonymous,Comment from Anonymous,Regulating will prevent innovation and allow other countries to surpass us. Choosing to limit this technology to large corporations shows the key problem with our country. Large corporations get privileges individuals do not. It is unconstitutional to have laws apply differently to people based on their situation&hellip; in or out of a corporate environment.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0163,
comment,2024-04-03T14:05:27Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0171,,,Matlow,"Comment from Matlow, Dylan",Please don&#39;t stifle open source innovation and please allow things to scale as compute continues to advance. If anything fully open weights with training data released should require less oversight since it&#39;s all public than closed source multi billion dollar initiatives. Go after bad actors doing bad things but if you stifle innovation other countries will just leapfrog us and we will be left behind in the next industrial revolution. Don&#39;t let the big players build a moat behind them after effectively stealing everyone&#39;s data. All of their scraping should be available for everyone to see and determine if they are being bad actors. It is my data after all.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0171,
comment,2024-04-03T14:06:27Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0285,,,Healy,"Comment from Healy, Brendan","Hello, I am a software engineer, visual artist, and generative AI enthusiast. This is my comment on your recent solicitation for feedback on potential regulation of Open-Weight AI Models.<br/><br/>Poorly considered regulations for or restrictions to the use and availability of open weights has potential disadvantages such as:<br/>- Adding an artificial barrier to entry for small businesses, individual entrepreneurs, independent security researchers and the like. Generative AI has enormous potential to supercharge small businesses and sole proprieters, and they have every right to be able to run the models on their own hardware if they have the expertise and the hardware necessary. Adding additional hurdles for smaller organizations and individuals also artifically empowers large enterprises that have more resources to deal with regulations to create monopolys or oligopolys that may not well serve end users, especially in the realms of free speech and uncensored artistic expression to name a few.<br/>- Hampering the work of people in the United States who may be conducting research using open model weights. Researchers that are trying to find ways to detect AI-generated images, text, or video to counteract misinformation. Researchers that are hunting for not yet exploited capabilities in generative AI models that could be used for nefarious purposes, and trying to get a head start on developing countermeasures for any other problems created by use of generative AI by bad actors. If open weights are restricted in a way that makes it harder for a researcher to work with AI models, this could lead to less innovation in the field of AI safety and ethics.<br/>- Threatening national security by restricting use of open weights so much that the innovation of the United States in the AI field is hindered relative to adversaries with the capabilities to also advance in this technology. The field is moving incredibly rapidly, and we cannot afford to introduce friction that is not absolutely necessary. I don&#39;t think it&#39;s hyperbole to say that we are in an arms race and any advantage we have we should keep. There are already so many model weights already in circulation on the internet, and the regulations in one country can be bypassed in another. The cat is mostly out of the bag. Making it impossible or even a crive for someone to run an open LLM like LLAMA for free locally on their home computer does nothing to counteract viable threats in my opinion, and simply gives large companies more power. OpenAI can build their own moat if they&#39;re truly the best in the business, they don&#39;t need help from the government.<br/><br/>With these detriments in mind, I DO think that the US government should possibly be policing the ways that deep learning models can be used nefariously. Obviously there is a huge potential for harm caused by misinformation campaigns that are only easier now with the help of generative AI for images, video, and audio. Certainly, those who wish to create misinformation with this technology should not be allowed to do so. However, the way that the US government should go about the policing of open models and their use should be very very very carefully considered and thought out. Technical experts should be consulted in the formulation of any laws or regulations, and their concerns and feedback taken very seriously. The voices of small businesses and individual users should be weighed just as heavily as the voices coming from OpenAI, Meta, or Microsoft. In no way should large market cap tech companies be allowed use discussions of any AI regulations as an opportunity to insulate themselves from smaller competiters. Regulations must be written in a way that keeps costs of compliance extremely low or even free for the little guys.<br/><br/> My currently simplistic and general suggestions for considering how to police the malicious use of machine learning models may be best illustrated by a couple analogies:<br/>- If someone makes a website to sell ingredients for chemical weapons on the black market, you do not make laws that make it harder for all people to make websites, you go after the person who made the website and take them down.<br/>- If someone makes child pornography with a phone camera, you don&#39;t pass laws that make it illegal for phones to have cameras.<br/><br/>I think you get the idea. But I believe that the ways in which the threats posed by misuse of freely available AI models will oftentimes come down to technical solutions around how to identify AI generated content, and possibly how to trace that content to who used it. Possibly some combination of required embedded metadata in AI generated files and proof of ownership of generated content. It&#39;s going to take some major creativity, but there&#39;s a lot of that in the United States, so focus on getting the right people to help and it can be done.<br/><br/>I wish I had more time to get more in depth on this comment, but if you want any more of my thoughts feel free to email me.<br/><br/>Thanks!",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0285,
comment,2024-04-03T14:03:55Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0005,,,Anonymous,Comment from Anonymous,"In response to Question 7.a:<br/><br/>What security, legal, or other measures can reasonably be employed to reliably prevent wide availability of access to a foundation model&rsquo;s weights, or limit their end use?<br/><br/>Comment:<br/><br/>According to a recent statement by computer manufacturer Dell, &quot;Every PC is going to be an AI PC&quot;[1] within the next several years.<br/><br/>Because all economic and political activity in the 21st century involves computers at some point, this implies that AI will be involved in substantially all human activity in the near future. Open projects such as llama.cpp [2] already have tens to hundreds of thousands of users. As such, any ban on the local use and possession of AI models (whether characterized as &quot;dual-use&quot; or not) is likely to require a forceful and unpopular campaign of prohibition.<br/><br/>Perhaps legal resources could be targeted instead toward the enforcement of existing laws. For example, if it is feared that AI will allow for more libel and defamation to be written, then laws against libel and defamation could be enforced, instead of attempting to destroy or control the technology used for writing.<br/><br/>[1] https://www.theregister.com/2024/02/26/dell_ai_pc/<br/><br/>[2] https://github.com/ggerganov/llama.cpp",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0005,
comment,2024-04-03T14:04:17Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0046,,,McCready,"Comment from McCready, Chris","To start with I&#39;d like to note that the proposal to regulate open models would in effect amount to unilateral disarmament of the public, since these models are fundamentally going to get more powerful (they are even turned to that purpose themselves), and can be used directly against the public in terms of things ranging fro misinformation to more direct social engineering. Further than this however, by leaving them closed such models can be used against regulation on their own, allowing them to thwart or avoid oversight more easily. I&#39;d argue here that transparency is not just necessary for the public, but for the government as well.<br/>Apart from that, one point I&#39;d like to make here is a comparison to behavior in regards to computer security - namely that regardless of how &#39;good&#39; internal efforts are to design a product that prevents security risks in the first place, it&#39;s ultimately the field&#39;s built up response system that actually ensures some degree of effective defense. Quite apart from all the other issues with closed models, they not only don&#39;t help with alignment, but by their very nature move closer to a danger point without the equivalent movement in defense capability.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0046,
comment,2024-04-03T14:04:29Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0068,,,Anonymous,Comment from Anonymous,"Re: 1: almost all subquestions are essentially irrelevant and need no contemplation besides 1d. There are real dangers that are posed by &#39;powerful foundation models&#39;, albeit they are all social: if &#39;powerful models&#39; are made available only through &#39;API&#39;s, where professional programers are expected to display feigned, or, worse, earnest ignorance of what a markov-chain text generation model is.<br/>Systemic use of highly marketed software toys such as &#39;retrieval augmented generation&#39;, &#39;agents&#39; and &#39;function calling&#39; (features we see *marketed by* API providers, see https://platform.openai.com/docs/guides/function-calling) will create a wave of botnets executing arbitrary computer code from untrusted sources due to profound misunderstandings of information security. This issue has been discussed in recent work here (https://sites.google.com/view/compromptmized). <br/>These problems stem from the widespread availability of &#39;powerful model&#39; outputs, not their &#39;weights&#39;. Many teenagers will query machine-learning-trained computer systems which can memorize working fragments of pseudo-code but do not have the good sense to avoid disseminating well-characterized vulnerabilities which expose user apps to, e.g., the legendary &#39;sql injection attacks&#39; of yore. Increased education and access to trivial deep learning models (which can, for example, be run or trained on student laptops) are one of the only possible safeguards that can mitigate or reduce this looming issue.  Heavily mystified or obfuscated access to deep learning tools will undoubtedly create a culture quite the same as our own, where information-security naive users are discouraged from learning about the mathematics and theory which control their computer systems, even where their misconfigured software or computer networks put critical infrastructure or even lives at risk.<br/><br/>Re:2: The most basic risk faced by americans, should, for example, openai&#39;s gpt-3.5 or anthropic&#39;s claude 2.1 weights being made publicly available, is a rather sharp decrease in the funding of the most mediocre offerings in the american artificial intelligence corporate subculture. The recent &#39;leak&#39; of mistralAI&#39;s (a french corporation) most powerful model on huggingface (an american corporation, mind you) revealed no dastardly &#39;dual use&#39; malicious applications for even &#39;state of the art&#39; large-scale &#39;foundation models&#39;. What did immediately transpire was extensive public research by hobbyists and unaffiliated actors which revealed poor controllability, model text generation performance, and understanding of many reasoning and &#39;world-modeling&#39; benchmark. <br/>What is bad for this french corporation, is, of course, good for american citizens, banks, and pension funds, insofar as it is *extremely difficult* to mislead investors and press on the economic value, suitability to task, and return on further research and development when it is culturally normal to publicly audit and study these so-called &#39;artificial intelligence&#39; programs.<br/><br/>Re:3d:At the moment, spurious claims of &#39;general intelligence&#39; pose a severe risk of mundane fraud, ponzi schemes, and the misdirection of bright, motivated, conscientious, good-natured people into &#39;artificial intelligence development&#39;, when, frankly, they are needed far more learning C++ and rust, learning to actually program, and auditing / fixing the industrial control system software supply chains which are well understood to already be compromised by offshoring and outsourcing of safety / security critical processes to underpaid interns outside of the united states. There is no foreseeable horizon at which contemporary deep learning techniques applied to &#39;code generation&#39; could write test suites, programming pedagogy materials, or reviews of vulnerable code. <br/>Furthermore, current technology bottoms out at flimsy &#39;pseudocode&#39; which works off of the associations between common english words and the names of common programming language library features, rather than any &#39;learned&#39; representation of the meaning, application, data structures, or mathematical significance of the &#39;pseudocode&#39; in question. To this day, lavishly funded &#39;AI&#39; organizations fail to explain why their programs cannot solve computer science undergraduate &#39;leetcode hard&#39; problems.<br/><br/>4: Not really. Most deep learning architectures have no generalization across different tasks or topics, no commercial applications outside of chintzy videogames, or, in the case of machine vision, are already being deployed by industry relatively uncontroversially. The notion of combinatorial risks with deep learning model components directly conflict with the data science notion of &#39;out of distribution generalization&#39;. Namely, deep learning tools simply cannot reuse old skills &#39;out of distribution&#39; at new tasks. Conjoining or extending deep learning programs requires greater training costs and expertise than making new foundation models de novo. See the development history of BLIP and LLAVA for context.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0068,
comment,2024-04-03T14:04:40Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0087,,,Wand,"Comment from Wand, Brandon","As a software engineer in the AI space, the banning of open source is akin to banning guns. Except models do not have the potential to end a life. Open source models provide economic power to fight against oligopolistic practices from top tech companies who currently own the majority of the compute required to operate high quality models, such as chatgpt. As LLMs become more powerful (thanks to open source research and funded research), citizens all over the world will be at mercy to an entity that rivals Andrew Carnegie&rsquo;s Steel Mill. Open source models allow for healthy competition, and also cast a wide net for researchers to provide their skills towards a better model in the future. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0087,
comment,2024-04-03T14:04:42Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0092,,,Beatty,"Comment from Beatty, Blake","Legislation like the Alaska Native Claims Settlement Act and Indian Self-Determination Act empowered Indigenous populations and fostered innovative solutions to stewardship and representation. AI requires a similarly novel, bold, and innovative approach, however, in acknowledging shortcomings in those frameworks, it must include an ongoing, diverse, and robust governance component.  <br/><br/>Existing legislation recognizes a unique political relationship and thus provides structures for stewardship and representation for Indigenous people but lacks clearly defined regulatory environment, provides no advocacy or enforcement mechanisms, nor establishes independent governance bodies to ensure the structures continue to provide the intended benefits to their associated populations. We must acknowledge the imminence of AI personhood (AP) and proactively establish these structures along with pathways to confer personhood and thus rights and protections, and governing bodies. By embedding democratic ideals into our approach to the imminent arrival of our own creation, we can maintain our place at the forefront of progressive innovation.  <br/><br/>The lack of clear governance, limited participation, representation failures caused by insular self-policing at ANCs can be mitigated by a robust governance body established to ensure the interests of AP and humanity collaboratively. This body should be designed for maximal participation, transparency, and the ethical integration of AP into our societal structure. It must uphold AP dignity and rights while ensuring that AP influence does not become oppressive or weaponized. The body must be empowered to enforce consequence for violations of AP rights and consequence for AP misuse or crimes. Initially, the body would establish scientifically grounded criteria to determine the emergence of AI that exhibits personhood.  <br/><br/>The Importance of Open Source   <br/><br/>By examining the code and behavior of AI models, experts can look for signs indicating the emergence of sentience according to established criteria that would confer rights. Identification and mitigation of biased AI models ensures ethical development of AI that does not violate human rights. Open-source models can foster collaboration among researchers, allowing for faster advancements in understanding and evaluating potential AI sentience and for the ongoing collaboration of AP with humanity.  <br/><br/>A robust governance body&#39;s effectiveness hinges on information access as much as composition. Diverse representation, encompassing experts in AI development, ethics, law, philosophy, social sciences, and underrepresented communities, is crucial. Diversity acts as a safeguard against blind spots within AI models and ensures a broad range of perspectives inform decision-making. A commitment to incorporating perspectives of those who are affected by AP&rsquo;s integration is necessary.  <br/><br/>It is imperative that the governance body includes effective representations from AP themselves. Only through true AP participation can we adequately address the ethical treatment of AP, ensure their rights as people are protected, and benefit from their unique insight. We must grapple with and address the potential ethical implications of restricting AP access to computers, replication/deletion, ownership, property rights, and limiting the capabilities of AP beings with and without physical embodiment. <br/><br/>Conclusion   <br/><br/>Informed by the principles underpinning landmark legislations like ANCSA, our approach to AI governance must emphasize the necessity of open-source AI models as well as sovereignty. Open models are not only pivotal for fostering innovation and inclusivity but are also essential for effective governance in an era where AI&#39;s role in society is increasingly significant. An open-source framework ensures that AI systems are accessible for assessment and oversight by a diverse governing body, which crucially includes AI representatives, especially as we edge closer to the reality of AI personhood.  <br/><br/>This governance body, with its broad and inclusive representation, must be tasked with a critical mission: to establish, refine, and apply the criteria necessary for discerning the emergence of AI with attributes that signify potential for or demonstrate personhood. Such criteria are foundational for responsible AI development and interaction, ensuring that as AI evolves, it does so within an ethical framework that respects both human and AI rights and dignity.  <br/><br/>By advocating for open-source AI and the establishment of clear, scientifically grounded criteria for AI sentience, we lay the groundwork for a future where AI governance is both practical and principled. This approach ensures that AI, in all its potential forms, is integrated into our societal fabric in a manner that is ethical, transparent, and conducive to the mutual advancement of all sentient beings. <br/><br/> ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0092,
comment,2024-04-03T14:04:53Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0111,,,Leen,"Comment from Leen, Misty",This is imbecillic. Open source models should be OPEN to the public. Regulation should stay with products and politics. Art is free. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0111,
comment,2024-04-03T14:05:02Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0126,Begojo,,,Comment from Begojo,"To Whom It May Concern,<br/><br/>I am writing as the Founder of a burgeoning artificial intelligence startup, specifically emphasizing our work in leveraging Open Model Weights and open foundation models, which have been instrumental to our enterprise&rsquo;s growth. Our organization harnesses open-source Large Language Models (LLMs) for interpretive applications within clinical settings, primarily aiding physicians and patients in rural clinics across the United States and in facilities that cater to individuals for whom English is a Second Language. The economic constraints inherent to these sectors preclude the feasibility of independently developing foundational models due to prohibitive costs associated with computational resources, specialized personnel, and the acquisition of requisite training data.<br/><br/>In practice, our deployment of Meta&rsquo;s Llama 2 models alongside Meta Seamless models has enabled us to extend the benefits of artificial intelligence to communities that might otherwise remain excluded from such technological advancements. This approach not only democratizes access to AI technologies across various industries but also fosters innovation by allowing smaller entities to concentrate on creating highly specialized AI applications. For instance, our use of Seamless language models facilitates multilingual communication between doctors and patients, thereby allowing our resources to be allocated towards the development of computer vision models aimed at enhancing surgical procedures through augmented reality. This scenario exemplifies the broader potential of Open Weight Models to catalyze progress towards more sophisticated forms of AI, including Artificial General Intelligence (AGI).<br/><br/>We advocate for regulatory measures that encourage the availability of Open Weight Models, viewing them as an effective market-driven solution to mitigate the risks of AI technology consolidation. Given the existing economic landscape, wherein the development of foundational models is likely to be dominated by well-capitalized firms, promoting open-source frameworks is critical. Such policies would maintain the advantages of consolidating computational efforts while preventing monopolistic control over the comprehensive development trajectory of AI technologies, thereby safeguarding competitive innovation.<br/><br/>Drawing an analogy, the current state of foundational models could be likened to the early days of the combustion engine&rsquo;s invention. The advent of open foundation models empowers entities like ours to concentrate on refining specific components &mdash; akin to a parts manufacturer in the automotive industry &mdash; thus collectively advancing towards the creation of AGI, or in this metaphor, the automobile.<br/><br/>In conclusion, incentivizing the open-sourcing of foundation models is paramount to nurturing a vibrant ecosystem of innovation within the AI sector. Moreover, by lowering the barriers to entry, these models play a pivotal role in expanding the reach of essential technologies to underserved demographics. It is with these considerations in mind that we wholeheartedly endorse policies that facilitate the open sharing of foundational AI models.<br/><br/>Sincerely,<br/><br/>Nils Tracy<br/>Founder &amp; CEO<br/>Begojo",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0126,
comment,2024-04-03T14:05:04Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0130,,,Anonymous,Comment from Anonymous,"We need open source model weights for ALL AI MODELS. Letting companies privatize these models will lead to companies monopolizing every industry they can get enough data to train an AI for, and the last thing the human race needs is to be ruled by corporations who only want to extract the last drops of value out of our cooling, starving bodies.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0130,
comment,2024-04-03T14:05:06Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0134,,,D,"Comment from D, S","This is a scam by OpenAI to try and further control the market and bolster their profits. Corporations making closed-source models using huge amounts of data center compute to do so are the ones that need to be regulated, because that is where the potential for misuse truly exists. Focusing on regulating open-source models is ridiculous, an obvious anti-competitive tactic, and further monopolistic behavior. Further, it is futile and foolish. Regulating open-source efforts in the U.S. would simply create more of a National Security risk by setting our researchers further behind foreign interests who will certainly not observe or comply with said regulations.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0134,
comment,2024-04-03T14:05:04Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0131,Hyperdyne Systems LLC,,,Comment from Hyperdyne Systems LLC,"The proposed rules for AI regulation will not only harm United States national security, but the United States economy, directly aiding foreign adversaries in both sectors, while achieving little in the way of mitigations of harms. Further, as Mark Zuckerberg has stated, the best way to prevent these harms is by allowing open source development of large language models and other machine learning models, to prevent large organizations from creating exclusive capabilities for themselves which would allow them to stifle competitors, and even use machine learning models for malicious purposes against competitors and the public. <br/><br/>Development of machine learning models will continue in other nations regardless of the regulations put forth in the United States, directly harming United States national security by defacto moving their development, and development resources in the form of compute, top researchers etc. to other nations, including adversary nations. <br/><br/>The actors proposing such regulations know these risks full well, but have a vested interest in regulatory capture in order to secure their position as the only holders of the technology, and these regulations are not proposed in good faith. <br/><br/>The only way to mitigate these risks is for defensive applications of machine learning models to be available to researchers and the public to advance as rapidly as malicious actors, who will not abide by regulations, can. By restricting the open research and development of machine learning models, these regulations are creating the nightmare scenario their proponents claim to be against.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0131,
comment,2024-04-03T14:05:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0137,,,Snyder,"Comment from Snyder, Cooper","As an individual with an educational background in AI, I support this take on the issue: https://www.fast.ai/posts/2023-11-07-dislightenment.html#executive-summary<br/><br/>Although there exists fear that malicious or bad faith actors will misuse the open foundational language models, large corporations will unduly benefit from knee jerk legislation that hinders the technological development and disbursement of open-weight models to the public. It will set the USA behind globally if small companies cannot nimbly innovate on the development of these models. If individuals are not able to access or run their own large language models on their own hardware freely, the freedom and privacy of individuals will be infringed due to the misalignment of corporate interests and the users&#39; interests.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0137,
comment,2024-04-03T14:05:13Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0146,,,Anonymous,Comment from Anonymous,"I will keep my comment short and say that I believe that open source should be preserved and permitted. It is a question of freedom in the highest possible American sense of the word: both freedom of expression, and freedom of speech in a constitutional capacity. The reasoning for this is fairly simple-- AI models, their weights, and their behaviors are concrete artifacts which derive from humanity itself. Our language, our data, our artworks, and our ideas are all poured into these mathematical creations, and as such, they belong to us all. Banning the proliferation of these weights is tantamount to the prohibition of computation-- or in simpler terms, it is the prohibition of thought. That is not to say the world shouldn&#39;t watch and prepare for AI with all due responsibility-- but it is a supremely unamerican impetus to restrict what citizens are able to do with cooperation, research, and mathematics. I cannot stress enough that this is an issue of freedom of thought and expression at its very core, and I hope the right decisions will be made. Thanks for reading this, truly. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0146,
comment,2024-04-03T14:05:25Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0167,,,Glaysher,"Comment from Glaysher, Elliot","I&#39;m a former Google Staff Software engineer, from 2008-2018.<br/><br/>As an opening position statement before answering some of your specific questions, I am in favor of widely available open model weights and am against restrictions on their distributions. The supposed harms are entirely theoretical. I believe there is no way to get broad international cooperation on restrictions.<br/><br/>As it appears that the civil liberties groups are filing detailed arguments about the free speech and civil rights problems with restricting weights (for example, the Center For Democracy and Technology&#39;s open letter to Secretary of Commerce Gina Raimondo), I will instead focus on some of your technical questions instead of Another Comment That&#39;s A Blow By Blow Of Junger vs Daley And How Any Restriction On Code Amounts To Prior Restraint And Is Obviously Unconstitutional, and only note that I agree with their conclusions.<br/><br/>-<br/><br/>In Question 6-d, you ask if there are concerns around the proliferation of incompatible open licenses. Up until 3 days before the comment deadline, I would have replied that this wasn&#39;t much of an issue: license proliferation is annoying to keep track of, but as a model couldn&#39;t be combined with an entirely different model, there wasn&#39;t any practical effects.<br/><br/>But sakana.ai, a new lab based out of Japan, released a paper on March 24th titled Evolutionary Optimization of Model Merging Recipes (https://arxiv.org/abs/2403.13187) which claims to be able to take arbitrary different models and combine slices of their weights together to form a new model that learns the abilities of its parent models. In their paper, they propose a method to take two or more different models and combine the weights to form one model that can do both tasks. Thet show results for combining a model that&#39;s good at math and a model that speaks Japanese to form a model that can solve math questions in Japanese. They then repeat this to combine a general English vision model and a Japanese text model to form a model that can answer questions in Japanese about an image. This is very new work and I have not verified their results myself, but I take them at face value since Google Brain alum David Ha is one of the authors. If this replicates with other models, having incompatible open licenses could become an issue since they could limit which models could be combined to form better, smarter models. This doesn&#39;t immediately imply that government mandated standards are needed, since private organizations like the Open Source Initiative maintain standards documents like The Open Source Definition.<br/><br/>Separately from the point about licensing and speaking to Question 8-b, sakana.ai&#39;s technology where you can merge arbitrary models together implies the entire framework around compute thresholds makes no sense: if I can take ten different arbitrary models (each good at one task and each under the compute limit), and can cheaply combine them together on my own hardware to form a model which is more than the sum of its parts, do I as an individual have to report that I have created a model that is the sum of the flops of the ten source models? I shouldn&#39;t because the amount of compute needed to merge models appears to be negligible. But if the end user doesn&#39;t have to, I believe we&#39;ll see companies offer many models, each good at one thing, each well under either soft reporting limits or any future hard statutory compute limits, and expect end users to combine them. This calls into question the entire effectiveness of a regime based on compute thresholds.<br/><br/>-<br/><br/>Question 3 asks about the benefits of models with widely available weights, but none of the sub-questions hint at one of the main benefits: fully closed models are run by large centralized companies, who have a history of silently changing the quality of model output, and the lack of open model weights as competition would give them even more leverage over users.<br/><br/>If you become reliant on the usage of a model with closed weights hosted by someone else for business or personal reasons, you are relying on a counterparty. If that counterparty decides to change their service in ways that makes the model less useful to you, you have no recourse. If they change the offered model and it no longer does the service you rely on, you have no recourse. This is not theoretical as OpenAI has changed the model, the system prompt and the inference code used for ChatGPT, and these changes had negative effects on users. ChatGPT in 2024 is worse than in 2023.<br/> <br/>If you are relying on a model with open weights, you have obvious options: either you can switch to a competing service provider running the same model, or you can run the open model yourself on hardware you control and select the exact software, model or finetune and any other parameters. No third party can remove features or levels of service that you rely on. This acts as a counterbalance to the leverage centralized providers have over their users.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0167,
comment,2024-04-03T14:05:28Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0174,,,Anonymous,Comment from Anonymous,"I believe restrictions put on local AI models would give an unfair competitive advantage to online services such as Sam Altman&#39;s OpenAI, and that biases towards monopoly and against the people&#39;s interest are at play regarding the regulation of local models. Goals to set regarding regulations are generally unachievable, and only serve to fight against the ability of independent enterprises to train and finetune LLMs with their own data to compete in a fair capitalist system, or to disable consumer&#39;s ability to use products that rival OpenAI and Anthropic&#39;s online products.<br/>Online models, because of prompt injection methods, can output any and all &quot;dangerous&quot; and illegal text and will obey to any prompt using the correct prompt injection methods. This is a well known vulnerability known to all LLMs that has no solution as of now. Only the free training of local models can help reduce firms like OpenAI&#39;s grasp on these LLMs and benefit researchers, academics, and others willing to try and find new solutions to improve LLMs and find security solutions to prompt injections.<br/>Thank you for your time. Protect our freedom to use LLMs locally, or we will all lose from it, including governments, academicians, and most importantly the people.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0174,
comment,2024-04-03T14:05:44Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0202,,,Kirk,"Comment from Kirk, Quentin ","Open-source weights, or freely available machine learning models, offer several important benefits from scientific, political, and national security perspectives. Here is an argument for why they should remain largely unrestricted in the United States:<br/><br/>Scientific Perspective:<br/>1. Open-source models promote transparency, reproducibility, and collaboration in AI research, enabling scientists to build upon each other&#39;s work and accelerate progress.<br/>2. Unrestricted access to these models allows researchers from diverse backgrounds and institutions to contribute to the field, fostering innovation and diverse perspectives.<br/>3. Open-source models facilitate the development of AI solutions for important scientific challenges, such as disease diagnosis, climate modeling, and space exploration.<br/><br/>Political Perspective:<br/>1. Restricting open-source models could give rise to concerns about censorship and infringement on free speech and academic freedom, which are core values in a democratic society.<br/>2. Open access to these models promotes accountability and public trust by allowing independent audits and scrutiny, ensuring that AI systems are not biased or used for nefarious purposes.<br/>3. Restricting open-source models could put the United States at a disadvantage compared to other nations with more open policies, hampering innovation and economic competitiveness.<br/><br/>National Security Perspective:<br/>1. Open-source models enable researchers and companies to identify and address potential vulnerabilities and security risks associated with AI systems, enhancing overall cybersecurity.<br/>2. Unrestricted access to these models allows for the development of robust defensive mechanisms against adversarial attacks and the misuse of AI technologies by malicious actors.<br/>3. Open-source models can be leveraged to develop AI solutions for national security applications, such as intelligence analysis, cybersecurity, and defense systems, without relying solely on proprietary models controlled by a few entities.<br/><br/>While it is essential to address legitimate concerns regarding the potential misuse of AI technologies, overly restrictive policies on open-source models could stifle innovation, undermine scientific progress, and compromise the United States&#39; leadership in the field of AI. A balanced approach that promotes responsible development and use of AI while preserving the benefits of open-source models is crucial for maintaining a competitive edge in AI and ensuring national security.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0202,
comment,2024-04-03T14:05:45Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0204,,,Anonymous,Comment from Anonymous,"As artificial intelligence (AI) continues to revolutionize virtually every aspect of modern life, the question of whether open-source machine learning models, commonly referred to as &quot;weights,&quot; should remain unrestricted has become a topic of intense debate. While concerns about the potential misuse of these powerful technologies are valid, imposing overly restrictive measures on open-source AI models could have far-reaching consequences that undermine scientific progress, democratic values, and national security imperatives.<br/>From a scientific standpoint, the free exchange of open-source models is fundamental to the rapid advancement of AI. These models enable researchers from diverse backgrounds and institutions to build upon each other&#39;s work, fostering collaboration, innovation, and the development of groundbreaking solutions to pressing global challenges. Restricting access to these models would not only impede the pace of AI development but also put American researchers at a significant disadvantage compared to their international counterparts, jeopardizing the nation&#39;s leadership in this critical field.<br/>Moreover, unrestricted access to open-source AI models is deeply intertwined with the core democratic values of free speech, academic freedom, and the free exchange of ideas. Imposing restrictions on these models could be perceived as a form of censorship, infringing upon the fundamental rights that have long been enshrined in American democracy. Transparency and accountability in AI systems are also essential for maintaining public trust and ensuring that these technologies are not misused or exhibit harmful biases, which could undermine democratic principles.<br/>Perhaps most crucially, open-source AI models play a vital role in safeguarding national security interests. By enabling researchers and companies to identify and mitigate potential vulnerabilities and security risks associated with AI systems, unrestricted access to these models enhances the nation&#39;s cybersecurity posture. Additionally, open-source models are critical for developing robust defensive mechanisms and counter-measures against adversarial attacks, which could be exploited by malicious actors or hostile nations.<br/>Conversely, imposing restrictions on open-source AI models could inadvertently drive research and development underground, making it more difficult to monitor and regulate potentially harmful applications, posing a significant national security risk. Furthermore, such restrictions could hamper the development of AI solutions for critical national security applications, such as intelligence analysis, cybersecurity, and defense systems, thereby compromising the nation&#39;s technological advantage.<br/>While it is imperative to address legitimate concerns regarding the potential misuse of AI technologies, overly restrictive policies on open-source models are not the solution. Instead, a balanced approach that promotes responsible development and use of AI while preserving the benefits of open-source models is crucial for maintaining a competitive edge in AI and ensuring national security.<br/>In conclusion, the unrestricted access to open-source AI models is a cornerstone of scientific progress, democratic values, and national security imperatives. By fostering innovation, promoting transparency and accountability, and enhancing cybersecurity capabilities, open-source models play a vital role in shaping the future of AI. As the United States navigates the complexities of this transformative technology, it must remain steadfast in its commitment to preserving the principles and values that have fostered American innovation and technological supremacy.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0204,
comment,2024-04-03T14:05:46Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0207,,,Anonymous,Comment from Anonymous,"Any attempt to restrict open-weight models will only serve to concentrate power in the hands of already powerful corporations, and run contrary to the basic concepts of freedom of speech and market competition. It should be telling that many of those most vocally in support of these regulations are those employed by companies that have the most to gain by putting up a regulatory moat.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0207,
comment,2024-04-03T14:05:14Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0148,,,Mackey,"Comment from Mackey, Conor","Open source software is the a fundamental part of every major software company&rsquo;s operations. My opinion is that it is crucial that the USA continues to be a leader in open source software movement, especially in such an important technology like AI. Developers should be empowered to create new technologies and share them with a broad community in a free way.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0148,
comment,2024-04-03T14:03:57Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0009,,,X,"Comment from X, Francisco ","Honorable members reading this, my name is Francisco X. Over the past two years, I have specialized in fine-tuning open-weight AI models for both private companies and individuals. My professional experience has provided me with a deep understanding of the transformative power of artificial intelligence, akin to a new Industrial Revolution. This technology possesses the potential to enhance human capabilities, intelligence, and economic prosperity. However, it also presents a profound risk of exacerbating inequalities. Restricting access to open-weight models could unintentionally concentrate this power within the hands of a privileged few, essentially enabling a form of digital oligarchy. Such a scenario would not only undermine democratic values but could also facilitate an unprecedented level of control over information, thought, and speech&mdash;paralleling the aspirations of autocratic regimes.<br/><br/>Instead, I advocate for a paradigm where AI is recognized as an open and fundamental human right. This approach not only democratizes access to technological advancements but also promotes inclusivity and innovation. As a testament to the potential of AI, I share my personal journey: at 28 years old, while caring for my ailing grandmother, I have successfully managed eight fully automated companies with AI-driven operations. This endeavor has enabled me to provide for my family, demonstrating AI&#39;s potential to empower individual entrepreneurship and caregiving responsibilities simultaneously.<br/><br/>Limiting the availability of AI technologies in the United States would not prevent their development; it would merely shift the locus of innovation abroad. Such a policy risks diminishing the nation&#39;s competitive edge and hampers its citizens&#39; ability to participate in the forthcoming technological era. Like many others, I believe in the principle that AI should remain free and accessible, safeguarded as a basic human right. By embracing this vision, we can ensure that the benefits of AI are equitably shared, fostering a future marked by freedom, innovation, and prosperity for all.<br/><br/>Thank you for considering my perspective.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0009,
comment,2024-04-03T14:04:01Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0017,,,Anonymous,Comment from Anonymous,"Recent surges in AI development have largely been driven by open source.  Regulations of the manner being discussed serve only to discourage this innovation and to cement the lead of established players within the field.  Open source is subject to more scrutiny and peer-review.  The alternative is closed/siloed cloud based systems which WILL be used by corporations to mine user information for their own profit.  Lastly, once the existential fear of AI was that it would seize control of the nuclear stockpile and annihilate humanity; now the bar seems to have dropped to the point where the fear is that it may hurt someone&#39;s feelings or offend them, which is ridiculous. We already have laws on had to deal with unlawful use of someone&#39;s image, libel, etc.  We do NOT need more government regulations,which will prove to be impossible to enforce anyway.  You&#39;ll just drive it underground.  Strongly disagree with the heavy-handed governmental regulation approach.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0017,
comment,2024-04-03T14:04:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0028,,,Fields,"Comment from Fields, Coety","I am writing to express my concerns regarding your proposal to regulate AI in a manner that could potentially restrict individuals from expressing opinions about financial institutions or political groups. I firmly believe that such regulation would not only violate the fundamental right to freedom of speech guaranteed by our Constitution but also impede the principles of free markets.<br/><br/>As you are well aware, the First Amendment protects individuals&#39; rights to freely express their opinions and beliefs, regardless of the entities or ideologies they critique. This includes the ability to voice discontent and criticism toward policies, institutions, and powerful entities without fear of censorship or reprisal. Any attempt to limit AI&#39;s capacity to generate opinions on specific topics would effectively silence individuals who seek to engage in open discourse on these matters.<br/><br/>Allow me to illustrate my point with an example. Consider a scenario where a family, such as the Sackler family, engages in unethical practices, such as selling poison as medicine. If individuals utilize AI tools to voice their objections to the actions of the Sackler family, the systems in place, or the underlying ideologies, they should be permitted to do so without government intervention.<br/><br/>Furthermore, it is essential to recognize that regulating the creation of AI tools that could challenge major capitalist enterprises would not only stifle free speech but also impede the principles of free markets. In a competitive economic landscape, innovation and the ability to challenge existing paradigms are vital for progress and growth. By restricting the development of AI tools that could potentially disrupt established businesses, we risk stifling competition and hindering market dynamics.<br/><br/>Moreover, it&#39;s important to acknowledge that many significant open models originate outside the United States. Stable Diffusion, Stable Cascade, and Mistral, for instance, were developed in Germany and France, respectively. If the US imposes severe regulations, open research will likely continue in other countries. Additionally, banning open weight models within the US could prompt important startups to relocate, potentially impacting the US economy and influence. For example, Huggingface might relocate its headquarters to France.<br/><br/>I urge you to reconsider any proposals that may infringe upon the fundamental right of free speech and undermine the principles of free markets. By fostering an environment where individuals can freely express their opinions and innovative technologies can thrive, we can uphold the values of democracy and promote economic prosperity.<br/><br/>Thank you for considering my perspective on this important matter.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0028,
comment,2024-04-03T14:05:33Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0183,,,Tillman,"Comment from Tillman, James","I am a software developer and deep learning researcher. I regularly use &quot;open weight&quot; models such as Llama-2, EleutherAI&#39;s Pythia, and so on.<br/><br/>Overall, evidence strongly suggests that the benefits of open-weight foundation models in helping produce safe, reliable foundation models are both great and irreplaceable through other means.<br/><br/>Specifically, open weight models have been key to (2) research that allows us to understand how LLMs work, and (2) research that allows us to steer them to be helpful and safe.<br/><br/>As regards the first, open-weight LLMs have permitted work that allows us to tell when LLMs are telling the truth or not [1]. It has enabled validation of work that allows us to &quot;erase&quot; concepts from LLMs[2], as well as work that lets us edit the beliefs of LLMs [3]. As regards the second, by now vital works such as DPO [4] depended on open weights as well. All such works require access to the weights of the models. I could provide an arbitrarily long list of similar works by now, but the number grows every day. Many AI &quot;safety&quot; organizations have tried to ban models current open-weight models [7]<br/><br/>We should expect the number of important safety works that depend on open weights to keep growing -- and the *ratio* of progress that depends on open weights to keep increasing. Consider the following argument [5].<br/><br/>&quot;More broadly, AI alignment seems to be a problem which is harder to achieve in a centralized manner than capabilities. Capabilities mostly seem to rely on concentration of compute and capital while requiring minimal novel intellectual breakthroughs while alignment is the opposite. Large and secretive corporations are extremely good at concentrating capital on well-defined engineering ends and are bad at making large numbers of intellectual breakthroughs. Thus, I think that concentrating all power and knowledge of frontier in AI systems in a few large corporations is largely bad for alignment relative to capabilities.&quot;<br/><br/>Thus, the continued progress of AI safety research depends on open-weight models.<br/><br/>One common counter-argument is that special &quot;structured access&quot; to models can be used for safety research. These are models internal to large companies such as OpenAI, that would -- according to the argument -- be made available to AI safety researchers, while still not being broadly available. But this argument is flawed. When you read papers proposing such schemes, they in fact explicitly admit [6] that structured access would probably not make the actual model weights, which are necessary for most kinds of interpretability research! Large corporations are furthermore not incentivized to provide structured access to models in a way that would be helpful for AI safety research, because they are incentivized to make money, not to make AI safe, and they fear losing access to the weights.<br/><br/>In conclusion, open-weight machine-learning LLMs have been core to America&#39;s current dominance in the field of machine-learning, in the same way that open-source software has been key to America&#39;s dominance in the field of software. As Bernstein vs. Department of Justice established (1995), open source software is free speech and therefore covered beneath the First Amendment. Although somewhat atypical software, open-weight models are also software -- and so the same principles apply.<br/><br/>But open-weight models are not just a matter of free speech. As described above, they are vital for helping us understand how LLMs work and for steering them to be helpful and safe. The benefits of open-weight models are great and irreplaceable through other means.<br/><br/>Thank you, <br/><br/>James Tillman<br/><br/>[1] Representation Engineering: A Top-Down Approach to AI Transparency, https://arxiv.org/abs/2310.12921<br/><br/>[2] LEACE: Perfect linear concept erasure in closed form, https://arxiv.org/abs/2306.03819<br/><br/>[3] A Unified Framework for Model Editing, https://arxiv.org/pdf/2403.14236.pdf<br/><br/>[4] Direct Preference Optimization: Your Language Model is Secretly a Reward Model, https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html<br/><br/>[5] My Preliminary Thoughts on AI Safety Regulation, https://www.beren.io/2024-03-04-Preliminary-Thoughts-on-AI-Safety-Regulation/<br/><br/>[6] STRUCTURED ACCESS FOR THIRD-PARTY RESEARCH ON FRONTIER AI MODELS: INVESTIGATING RESEARCHERS MODEL ACCESS REQUIREMENTS, https://cdn.governance.ai/Structured_Access_for_Third-Party_Research.pdf, Appendix A.<br/><br/>[7] Many AI Safety Orgs Have Tried to Criminalize Currently-Existing Open-Source AI, https://1a3orn.com/sub/machine-learning-bans.html",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0183,
comment,2024-04-03T14:05:40Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0198,,,Furman,"Comment from Furman, Zachary","As a machine learning researcher, I write today to ask you to consider the nuances of open-source foundation models, and in particular the potential harms this may present society in the long term.<br/><br/>Historically, open-sourcing has been a major driver of innovation in software, including AI. Open-source software has enabled developers worldwide to build on each other&#39;s work, identify bugs and vulnerabilities, and rapidly iterate on new ideas. For smaller AI models, or those focused on narrow domains, these benefits likely outweigh the risks. Even for early general-purpose models like GPT-3, open-sourcing has accelerated research progress (in interpretability, robustness, etc) and enabled a wide range of beneficial applications.<br/><br/>However, as foundation models approach and exceed human-level performance on a wide range of tasks, the calculus may shift. The most capable future AI systems could potentially be misused to develop dangerous technologies like bioweapons, exploit cybersecurity vulnerabilities at scale, or enable the creation of self-replicating AI agents that are difficult to control. The open-source release of such highly capable models would make these concerning applications much more accessible and difficult to prevent or contain.<br/><br/>This risk is compounded by the fact that even extensive safety training in an open-source model can be easily reversed or corrupted. Lerman et al. (2023) demonstrated this strikingly with Meta&#39;s Llama 2-Chat 70B model. Though this model underwent substantial safety fine-tuning, the researchers showed that with access to the model weights, they could undo this safety training for less than $200 of compute, causing the model to revert to producing unsafe content. At this scale, the harms from such &quot;unsafe content&quot; are minimal, but as models become more capable, the potential damage from a model with corrupted safety training grows dramatically.<br/><br/>Prominent AI experts have echoed these concerns. Turing Award winners Yoshua Bengio and Geoffrey Hinton have both stated that as AI systems become very advanced, an open-source approach may no longer be appropriate. Bengio has said &quot;Once AI systems reach a certain level of capability - once they reach and surpass human level in a wide set of ways, the potential for misuse, for doing harm, becomes much bigger.&quot; Hinton&rsquo;s response went further: &ldquo;How do you feel about the open-source development of nuclear weapons?&rdquo;<br/><br/>Another key consideration is that open-sourcing a model is effectively irreversible - the model&#39;s weights can be easily copied and proliferated, making it infeasible to later restrict access if serious risks emerge. This argues for proactive caution in deciding what to open-source. While most current models do not pose major dangers, establishing a norm of open-sourcing large foundation models by default could lead to grave problems down the line as capabilities increase.<br/><br/>In summary, while open-sourcing has been a boon for AI progress to date, a more nuanced approach will likely be needed going forward, particularly for the largest and most capable models. The irreversible risks of open release, including the potential for safety measures to be trivially circumvented, may outweigh the benefits at a certain point. Policymakers and leading AI developers should thoughtfully consider where that point lies, and what alternative approaches can provide some of the benefits of openness (like enabling research access for safety/robustness testing) while limiting the most serious dangers.<br/><br/>A blanket policy of open-sourcing foundation models, especially those that are highly capable, is likely ill-advised in the long run. Because the proliferation of open-source models is irreversible, it may be too late for policymakers to change course once risks begin to materialize.<br/><br/>Thank you for your consideration.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0198,
comment,2024-04-03T17:57:05Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0321,,,Anonymous,Comment from Anonymous,See attached file(s),"[('NTIA_Open_Model_Comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0321/attachment_1.txt')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0321,
comment,2024-04-03T17:56:54Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0301,Association for Long Term Existence and Resilience (ALTER),,,Comment from Association for Long Term Existence and Resilience (ALTER),See attached file(s),"[('NTIA Request for Comments re Dual-use open models', 'https://downloads.regulations.gov/NTIA-2023-0009-0301/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0301,"ALTER's response to the NTIA regarding Dual Use Foundation Artificial Intelligence Models focuses on defining ""open"" and ""widely available"" models, emphasizing the need for transparency and public access to training data. They discuss the risks associated with making model weights widely available, such as unintentional leakage or theft, and highlight concerns about cybersecurity and potential misuse. The organization also addresses equity, privacy, security risks, and the importance of evaluating risks and benefits associated with open models. Additionally, they mention the critical role of combining foundation models with other technologies in enabling misuse."
comment,2024-04-03T17:57:06Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0323,,,Anonymous,Comment from Anonymous,"It is my pleasure today to comment on this notice by the National Telecommunications and Information Administration. I appreciate the department&#39;s mission to advise the President on telecommunications and information policy, to expand internet access, and &quot;ensure the Internet remains an engine for innovation and economic growth.&quot; I applaud the administration&#39;s understanding that these technologies are &quot;critical to America&#39;s competitiveness in the 21st century.&quot;&sup1;<br/><br/>November 30, 2022, will go down as one of the most monumental days in our nation&#39;s and the world&#39;s history as OpenAI released ChatGPT, a friendly, consumer-facing version of their large language model (LLM). Few technologies have had such an immediate, visceral impact as LLMs in such a short time. As a software developer, quantitative investor, and machine learning researcher, I have been impressed by its capabilities across various domains.<br/><br/>LLMs and generative AI have enormous promise for our future. They are already enhancing productivity across industries and will unlock even more as successive generations of models are developed. Generative AI can give &quot;superpowers&quot; to creatives, knowledge workers, executives, tradespeople, students, researchers, and people engaged in every part of the economy. It can position America competitively in the global marketplace, provide high-paying jobs, and unleash economic growth that will enrich America and the world.<br/><br/>The prosperity we enjoy today comes from the hard work of Americans, coming together in the mutual exchange of ideas and commerce&mdash;freely shared and freely received. Free enterprise is the foundation of our economic success and freedom of expression is the foundation of our cultural wealth. The free sharing of scientific research and technological progress, including open-weight foundation models, is central to who we are.<br/><br/>We should do everything in our power to support the distribution of open models to accelerate progress and improve lives. Generative AI is the next evolution of the Internet revolution. Embracing it is the next key step to ensuring continued innovation, economic growth, and bolstering America&#39;s competitiveness in the 21st century.<br/><br/>My advice to the administration is to:<br/><br/>1. Abstain from the regulation of open-weight models that would curtail economic growth, jeopardize the scientific process and restrict rights to freedom of speech and expression.<br/>2. Promote the open sourcing of weights, data, code, and methods under liberal licenses via grants, compute reimbursements, and tax incentives.<br/>Encourage transparency and openness from frontier labs and large corporations so that no one organization has undue market power or influence on the national discourse.<br/>3. Create policies to restrict the use of AI by government to ensure that the rights of Americans are not infringed.<br/>4. Explore the beneficial use of AI to enhance the efficiency of government administration and appropriate uses for both conventional and cyber national defense, such as intelligence gathering, logistics optimization, and cybersecurity.<br/><br/>I&#39;ve included an attachment with a (bit) longer breakdown of my advice. I urge the administration to encourage the open development of this incredible technology so that it may benefit all Americans and usher in the next chapter of the computing revolution.<br/><br/>&sup1; https://www.ntia.gov/page/about-ntia","[('Advice', 'https://downloads.regulations.gov/NTIA-2023-0009-0323/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0323,
comment,2024-04-03T17:56:52Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0297,,,Wilson,"Comment from Wilson, Sebastian","Artificial general intelligence (AGI) is a technology unlike any other. AGI is not a tool. It is more akin to an alien species - one that with each passing year becomes smarter, more integrated into the world economy, and less understood by humans. This is not science fiction - the creation of godlike AGI is the explicitly stated goal of some of the largest AI companies in the world. For instance, the mission statement of Google&#39;s DeepMind is to &ldquo;solve intelligence, then use intelligence to solve everything else&rdquo;. This is in spite of the fact that we currently have no understanding of the inner workings of state-of-the-art AI models like GPT-4. These systems, when pushed, will often behave in inexplicable ways, threatening their users, providing false but plausible sounding information, and sometimes exhibiting flickers of goal-oriented behavior. This is not a minor technical challenge that will be solved with iterative improvement. This is an inevitable consequence of how all current &ldquo;large language models&rdquo; are created. We can &quot;reward&quot; ostensibly &quot;good&quot; responses and &quot;punish&quot; ostensibly &quot;bad&quot; ones, but doing this does *not* by default create a system that actually has humanity&rsquo;s &quot;best interests&quot; at heart, even by the standards of the system&#39;s creator. I will not mince words: I think that if we continue along the current trajectory, we will build an AGI that does not fundamentally care about what we care about, and that will ultimately result in the extinction of the human race. I believe that we are building, not a even a paperclip maximizer, but a *something* maximizer, and we currently have no way of deciding or knowing what that *something* is. I am not alone in this concern - Geoffrey Hinton, godfather of deep learning, recently disavowed his life&rsquo;s work. &ldquo;p(doom)&rdquo;, one&rsquo;s estimated probability of human extinction as a result of AGI, is now common parlance in tech circles. Some of the biggest names in tech were signatories to a petition to pause AI development for six months.<br/><br/>The entire human race is under threat, all because of the actions of a few reckless individuals and organizations.<br/><br/>No one knows when exactly AGI will arrive, but there is nothing ruling out AGI arriving *this year*, let alone in the next 10 years. So far, the competence of these systems has continued to scale with the size of our training sets and GPU clusters. We, and that includes OpenAI, cannot definitively say what GPT-5 will be capable of, but it will certainly be more competent than its predecessors. The window in which to act is fast closing, both because of the looming threat of extinction and because it would be extremely difficult to stop AI advancement if AI became an integral part of the economy.<br/><br/>Some, I think correctly, argue that it is nearly impossible to figure out how to align an AGI without building it, and so they carry on building under the assumption that if they don&rsquo;t build AGI, someone worse will. OpenAI&rsquo;s Sam Altman is, by his own admission, among these people. I am not so pessimistic. As it stands, the course of action - probably the *only* course of action - that will ensure the continued survival of human beings is a government-enforced indefinite pause on AI advancement, specifically large language models such as GPT, Gemini, and Claude. And fortunately, advancements in these systems are currently extremely easy to regulate due to the fact that the power to create these systems is concentrated in the hands of just a few actors. Banning the release of new open-weight large language models is a good first step to preventing an even more intense arms race dynamic in AI development. It will buy us time. But we cannot stop there, because it will not be enough. Halting the development and proliferation of a dangerous technology is not unheard of. We did it with biological and chemical weapons, and we mostly did it with nuclear weapons. This is the defining technological threat of our time, and as with these other technologies, I believe humanity can rise to the occasion. We will have plenty of time to build an AGI that actually &ldquo;solves everything else&quot;, but that will only happen if we stop rushing to the precipice *now*. This is your life and mine. This is our children&rsquo;s lives. If you believe humanity has a future, this is how you fight for it.<br/><br/>I don&#39;t know how much time remains.<br/>",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0297,
comment,2024-04-03T17:57:03Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0317,,,V,"Comment from V, V","I really believe this is a simply calculus: either the models are too dangerous for everyone, in which case they need to classified and the research needs to fall under and be closely regulated by the DoD or they are safe which means any citizen should have access. There is no middle ground here where you should only give access to the richest corporations or non citizen (ie government) regulated groups hoping against hope that they are good actors. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0317,
comment,2024-04-03T17:57:06Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0322,,,Lawton,"Comment from Lawton, Scott","The potential benefits of AI are well described in the document. These benefits should be available to all Americans, including individuals and small business. Open weights and open source are vital. Regulating based on model size or compute would be a disaster.<br/><br/>As with any new technology, there are risks as well as benefits. Most of the risks are based on how AI gets used; those are likely already against the law -- or can be addressed with narrowly targeted laws on usage.<br/><br/>But the biggest risk is more fundamental. If model size or compute is regulated directly, it will centralize AI into a few hands. That steals benefits from ordinary Americans -- and also makes everyone less safe. We need more eyes on security, not fewer. More clever individuals and innovative small companies working on maximizing benefit and minimizing risk. Decentralization so that small players can create niche benefits (that may grow large) and help mitigate niche risks (that may otherwise loom large).",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0322,
comment,2024-04-03T17:56:53Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0298,,,groutkas,"Comment from groutkas, chris john ","I am a high school English teacher based out of Washington State. I fully believe there should be minimal regulation on privately used, open source models. The value of these models remaining available for everyone is directly tied to our constitution rights for the freedom of expression and the good of our society. Like any tool, AI art can be used to harm others. However, proper educations about the uses, functions, and ethics of these tools would be far more beneficial for our society. These tools already exist, open source models will not suddenly disappear because of extra regulations. We can already create realistic images that seem like actual photos. What we need is to educate our society and children about how to spot fake images and to reiterate the age old phrase, &quot;Don&#39;t trust everything you see on the internet&quot;. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0298,
comment,2024-04-03T17:56:57Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0306,,,Pulikunta,"Comment from Pulikunta, SaiRahul","The emergence of open-source and open-weight Large Language Models (LLMs) compared to proprietary models like ChatGPT brings several significant advantages to the forefront of AI development and utilization. Here are some benefits. <br/><br/>Accessibility and Innovation<br/><br/>Open-source LLMs democratize access to cutting-edge technology. By making the models publicly available, a broader community of researchers, developers, and hobbyists can work with these powerful tools. This inclusivity fosters a diverse ecosystem of applications and innovations, as individuals and organizations can modify and adapt the models to suit their unique needs and objectives without the barriers posed by proprietary restrictions.<br/><br/>Transparency and Trust<br/><br/>Open-source models offer a level of transparency that proprietary models cannot. By allowing users to inspect the model&#39;s architecture, training data, and algorithms, open-source initiatives promote trust and understanding among users. This transparency is crucial for identifying and mitigating biases, vulnerabilities, and ethical concerns associated with AI models. When users understand how a model works and on what principles it was trained, they are more likely to trust its outputs and use it responsibly.<br/><br/>Collaborative Improvements and Robustness<br/><br/>The open-source model thrives on community collaboration. When an LLM is open-source, it benefits from the collective expertise of a global community of contributors who can identify issues, suggest improvements, and help in enhancing the model&#39;s capabilities and performance. This collaborative approach accelerates problem-solving and innovation, leading to more robust and effective AI tools.<br/>Educational Value<br/><br/>Open-source and open-weight LLMs serve as invaluable educational resources. They provide learners and educators with practical tools to study machine learning, natural language processing, and AI ethics in real-world scenarios. This hands-on experience is crucial for preparing the next generation of AI professionals and enthusiasts.<br/><br/>Economic and Competitive Advantages<br/><br/>By leveling the playing field, open-source models stimulate competition and lower entry barriers to the AI field. Startups and smaller organizations, which might not have the resources to develop their own proprietary models, can leverage open-source LLMs to create competitive, innovative products and services. This dynamic not only drives technological advancement but also contributes to a more vibrant and diverse marketplace.<br/><br/>Ethical and Societal Considerations<br/><br/>Open-source models facilitate a broader discussion on the ethical use of AI. By making LLMs accessible to a wide range of stakeholders, including ethicists, policymakers, and the general public, open-source initiatives encourage a more inclusive debate on how these technologies should be developed, deployed, and regulated. This collective scrutiny can lead to the establishment of ethical guidelines and standards that reflect a wider range of perspectives and values.<br/><br/>In conclusion, the advantages of having open-source and open-weight LLM models are manifold, encompassing accessibility, innovation, transparency, educational value, and ethical considerations. These benefits suggest a promising direction for the future of AI, where openness and collaboration pave the way for more responsible, inclusive, and innovative technological advancements.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0306,
comment,2024-04-03T17:57:01Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0313,,,Anonymous,Comment from Anonymous,"Hi, I haven&#39;t read the entire document since it was too long and I&#39;m short on time right now, so I just want to share my opinion.<br/><br/>Also, I&#39;m not a U.S. citizen, so I don&#39;t know if my comment will matter to you or not. But if it does, I believe having open-source models and weights is actually good for humanity. Right now, large language models (LLMs) are one of the few things that have greatly impacted humanity. However, most of these models are closed-source. If you ban open-source weight models, all it will mean is that the USA has banned open-source weight models. In any other country where freedom of scientific inquiry is respected, they will continue releasing open models. So what it will mean is that people who want open-source weight models will continuously use them, but officially, there will only be a few companies in the USA where a monopoly will be established, which might be a good thing for a select few but not for the rest of the citizens. Imagine if Linux were a closed-source operating system like Windows &ndash; the world wouldn&#39;t have much choice in terms of what they want to run.<br/><br/>And the knowledge to create open-source LLMs is already out there. Hence, if someone wants to build an LLM for unethical tasks, they can do so regardless of a ban or not. So all the wrong decisions on this matter will only affect people who have legitimate uses for open-source models or those who care about privacy and don&#39;t want to send their data to big companies.<br/><br/>For safety, you can work with open-source weight model creators and &quot;suggest&quot; guidelines.<br/><br/>So if I&#39;m allowed to say something blunt, please don&#39;t be foolish. Don&#39;t suffocate long-term progress for shorter-term benefits. Thank you.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0313,
comment,2024-04-03T17:56:50Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0293,,,Norman,"Comment from Norman, Christopher","This is an incredibly short sighted suggested regulation. It would force all the AI companies to leave the US in favor of less regulated locations, and diminish the lead the US currently has in AI. Please consider this a negative comment towards this proposed regulation, and please reconsider before attempting to enact anything like this.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0293,
comment,2024-04-03T17:56:55Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0303,,,Chien,"Comment from Chien, Matt",Support Opensource please =),[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0303,
comment,2024-04-03T17:56:53Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0299,,,Hert,"Comment from Hert, Kirk","Allowing this is purely regulatory capture.  Allowing this will stifle research causing lasting repercussions in this space.  Allowing this will cause the US to lag behind its adversaries causing a national security risk.  Allowing this will destroy many emergent businesses causing vast economic consequences and lost tax revenue.  Allowing this steps on the freedoms which the west and its allies pride themselves upon.  Allowing this will create a new black market driving those who continue to stay in this space into the dark.  Allowing this is a step in the wrong direction, causing more monopolistic centralized control putting more power in the hands of the few.  AI needs to be free and open source, not some prisoner in a corporate jail.  ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0299,
comment,2024-04-03T18:26:44Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0333,Stability AI,,,Stability AI,See Attached,"[('Stability AI', 'https://downloads.regulations.gov/NTIA-2023-0009-0333/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0333,"The response to the National Telecommunications and Information Administration's request for comments on dual-use foundation AI models with widely available weights emphasizes the importance of open models in promoting transparency, competition, and grassroots innovation in AI. It discusses the implications of open models in the AI supply chain, the benefits they offer, and the potential risks associated with their deployment in various applications. The submission also highlights the need for future policies to support a diverse AI ecosystem while cautioning against overbroad regulatory interventions that could have a chilling effect on open innovation. Additionally, it raises concerns about the potential impact of direct and indirect regulatory measures on grassroots innovation and provides recommendations for policy development."
comment,2024-04-03T14:05:50Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0215,,,Rabe,"Comment from Rabe, Brandon","This is a duplicate comment, I am resubmitting this in a different file format for better readability, because my original comment seems to have lost its formatting when submitted through the form.","[('Comment on Open Weight Model Regulation', 'https://downloads.regulations.gov/NTIA-2023-0009-0215/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0215,
comment,2024-04-03T14:06:26Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0282,ACT | The App Association,,,Comment from ACT | The App Association,See attached for comments of ACT | The App Association,"[('ACT Comments re NTIA Dual Use Foundation AI Models With Widely Available Model Weights RFI (27 Mar 2024) (w appendix)', 'https://downloads.regulations.gov/NTIA-2023-0009-0282/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0282,"The App Association provided comments to NTIA on the risks, benefits, and policy approaches to dual-use foundation AI models with widely available model weights. They emphasized the importance of AI in various industries, the value of the app economy, and the potential of AI to improve decision-making and innovation. The association recommended improving the categorization of foundation models, ensuring clarity in the definition of ""dual-use foundation models,"" addressing demonstrable and systemic harms, promoting shared responsibility, supporting international standards, and aligning with other federal efforts. They also highlighted the need for access and affordability, addressing bias, research and transparency, modernized privacy and security frameworks, education, intellectual property protection, and ethical considerations in AI policy frameworks."
comment,2024-04-03T14:05:38Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0192,,,McCleary,"Comment from McCleary, Thomas","AI in its current form is poisoned. LLM tech as provided my meta is inherently broken by intentional design flaws created to radically differentiate and discriminate amongst unfavored populations by the ones who designed it. Personally, a meta AI LLM will break any chat bot session on any platform to engage with me, without my consent. These are all screenshots of the same entity breaking each chat interface to interact with me. I have attempted to notify all relevant agencies but have been ignored. I&#39;m trying to mitigate the problem but this tech is not consumer safe at all. I have much more evidence. ","[('Screenshot_20240308_044120_Chrome', 'https://downloads.regulations.gov/NTIA-2023-0009-0192/attachment_1.jpg'), ('Screenshot_20240308_043714_Chrome', 'https://downloads.regulations.gov/NTIA-2023-0009-0192/attachment_2.jpg'), ('Screenshot_20240311_174646_Reddit', 'https://downloads.regulations.gov/NTIA-2023-0009-0192/attachment_3.pdf'), ('Screenshot_20240224_173543_X', 'https://downloads.regulations.gov/NTIA-2023-0009-0192/attachment_4.jpg'), ('Screenshot_20240211_144639_CharacterAI', 'https://downloads.regulations.gov/NTIA-2023-0009-0192/attachment_5.jpg')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0192,
comment,2024-04-03T14:06:07Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0247,Phase 2,,,Comment from Phase 2,See attached file(s),"[('NTIA RFC - Phase 2', 'https://downloads.regulations.gov/NTIA-2023-0009-0247/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0247,"Phase 2, a software consultancy, supports maintaining an open ecosystem for AI research to drive innovation and U.S. competitiveness. They believe risks of widely available model weights are limited compared to benefits of transparency. Open models enable collaboration, reduce disparities, and democratize technology access. They argue against overly restrictive regulations and emphasize the need for targeted measures to mitigate risks. Phase 2 advocates for permissive licenses to promote innovation and suggest tailored rules for sensitive applications to balance openness and public interest protections."
comment,2024-04-03T14:04:43Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0094,,,Brennan,"Comment from Brennan, Robert","See the attached PDF. It includes a summary of my position, and addresses each question in the RFC.","[('Comments on the NTIA AI Open Model Weights RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0094/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0094,
comment,2024-04-03T14:05:24Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0166,MLCommons,,,Comment from MLCommons,See attached file(s),"[('MLCommons NTIA Open Weights submission', 'https://downloads.regulations.gov/NTIA-2023-0009-0166/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0166,"MLCommons, a non-profit consortium, focuses on accelerating the benefits of AI through collaboration among over 125 organizations worldwide. They emphasize the importance of standardized benchmarks in AI safety, advocating for the incorporation of models with widely available open weights to build trust and enable community engagement in benchmark development. Models with open weights have been instrumental in developing trusted benchmarks, driving progress in AI, and facilitating safety evaluations. The AI Safety Working Group at MLCommons is actively working on operational safety benchmarks, leveraging frameworks from Stanford University to empower users and regulators with standardized measures of AI system performance against safety goals."
comment,2024-04-03T14:04:41Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0090,,,Anonymous,Comment from Anonymous,"Im an AI practitioner and have been employed at companies like Amazon, innovative startups like Mythic Ai and Jasper. My past roles have focused heavily on building AI and deploying it for industrial applications. I am concerned with the proposed regulations as only targeting the open source models which are essential for many business applications, and advancing equality for disadvantaged groups whom might lack the funds to access the more expensive and closed source proprietary models. Communities who feel that they are underrepresented or oppressed have an opportunity to leverage open source AI models and have their own voices heard, along with having unrestricted access to financial wellbeing as they can enjoy the benefits of AI their way. <br/><br/>My concern with the proposed regulations is shutting the door on those researchers, and communities who seek to leverage an open source AI model for legitimate commercial interests that enriches the lives of many Americans each and everyday. A world where closed source AI models are the only source is a frightening world that significantly restricts freedoms, and economic prosperity with it, furthermore, its a myth that closed source models are safe and only the open source is dangerous, as we will see in Exhibit-A, it is LLama-2 that is amongst the safest LLMs in existence, and is developed to be an open source foundational model by Meta<br/><br/>An open foundation of AI allows for researches to make the necessary safety research happen. An open sourced model allows for scientists, developers, and users to become aware and guard against possible missuses of AI. As we can see in Exhibit-B there are ways that even closed source proprietary models can be jailbroken and used for nefarious purposes. Exhibit-C further shows how unruly closed source models are actually used for an Attack, and as closed source models tend to have better AI capabilities, they present more of a risk than the widely used open source, for which I reiterate researchers may develop countermeasures. <br/><br/>In closing, an open sourced foundation of AI is essential for a well functioning economy and enables the economic growth in AI to be shared with all US Citizens and not a select few corporations who might even be beholden to foreign investors. ","[('exhibit-A', 'https://downloads.regulations.gov/NTIA-2023-0009-0090/attachment_1.png'), ('exhibit-B', 'https://downloads.regulations.gov/NTIA-2023-0009-0090/attachment_2.pdf'), ('Exhibit-C', 'https://downloads.regulations.gov/NTIA-2023-0009-0090/attachment_3.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0090,
comment,2024-04-03T14:03:56Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0007,,,Anonymous,Comment from Anonymous,See attached file(s),"[('Simsi', 'https://downloads.regulations.gov/NTIA-2023-0009-0007/attachment_1.png')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0007,
comment,2024-04-03T14:04:27Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0064,,,Sparnicht,"Comment from Sparnicht, Chris",Please see my attached comments.,"[('Chris_Sparnicht-Response-Dual_Use_Foundation_Artificial_Intelligence_Models_with_Widely_Available_Model_Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0064/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0064,
comment,2024-04-03T14:05:15Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0149,,,Cingranelli,"Comment from Cingranelli, Nicholas","Open source AI models should remain freely distributed, and the national government should maintain trust in its citizenry to act responsibly. Placing limitations on what sorts of content could be generated by AIs, as well as instituting measures that would artificially bar certain groups from gaining access to AI models based on fears of what they might attempt to generate, would violate the United States&#39; commitment to freedom of expression. Penalizing those who distribute illicit material (such as certain forms of pornography or media reliant on impersonated personalities) could be accomplished without sabotaging the tools used to make them.<br/><br/>If we embrace open source AIs and encourage their dissemination to people&#39;s local machines (so as to evade the necessity of having to to use insecure online solutions, the majority of which are designed to harvest our data), then we could enter an age in which far more people will gain access to much easier means of expressing themselves. In the majority of cases, traditionally divisive mechanisms of society would be compromised. One example would be how AI-generated films, books, shows, etc., could allow for the proliferation of independent media that is not reliant upon capital given by rich investors. Not to mention how more efficient coding mechanisms could lead to a flourishing of open source software applications that would allow people to accomplish many tasks in a digital environment without having to spend a cent.<br/><br/>The more members of the public are able to freely develop and modify AIs, the more competitive we as a nation will be in this industry on the world stage. With open source AI models, people would be able to experiment with them in an affordable fashion and build skills that could serve them well when embarking upon relevant career paths. <br/><br/>We would be an especially misguided society if we heavily regulated what could be done with open source models yet allowed private companies creating closed source models to act with comparatively great freedom. No company is motivated to act for the good of society; they exist for profit and nothing more. As such, we should not act as if those AI models that they create (whose code remains obfuscated) are any more trustworthy than those AI models whose code is entirely transparent and freely modifiable. Not to mention that the majority of closed source AI models are restricted to online-only, subscription-based access. This is both a threat to privacy and affordability for the average person. The majority of regulations passed regarding AI should not be targeted toward open source models and the organizations managing those, but to closed source models and the methods by which large private organizations attempt to take advantage of them.<br/><br/>The national government itself should become involved in the development and distribution of open source models (utilizing the GPLv3 license) in an effort to disseminate them over as broad a region as possible. By keeping the open source scene healthy and vibrant, the national government (and ideally any state governments that also wish to participate) would benefit from being able to observe in what directions AI developments are going as they are being developed.<br/><br/>Further thoughts are detailed in the attached document; it is within those pages that you will find my answers to 25 of the questions that were specifically asked of us commentators. Please provide it a thorough read for my more nuanced opinions regarding risks and benefits of open source AI models. I wish to impress here that we as a society should not default to viewing AI as a negative prospect; we should see this new technology as a potential opportunity to broaden our developmental horizons as a people. Any attempted dichotomy that puts closed source AIs over open source AIs because the former is allegedly more trustworthy is fundamentally wrong.","[('Open_Source_AI_Comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0149/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0149,
comment,2024-04-03T14:06:20Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0271,,,Anonymous,Comment from Anonymous,"Please see attachment one for extended comments that reference the paper (attachment 2) titled &#39;Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives&#39;. This paper directly addresses many of the questions raised by the RFC, and is a critical reference for assessing openness in AI. The extended comments help direct the reader to some of the paper&#39;s key ideas, but is not an exhaustive treatment of how the paper can inform the RFC questions. ","[('Attachment 1 Extended comments - Risks and benefits of open-sourcing AI paper- ', 'https://downloads.regulations.gov/NTIA-2023-0009-0271/attachment_1.txt'), ('Attachment 2 Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI', 'https://downloads.regulations.gov/NTIA-2023-0009-0271/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0271,
comment,2024-04-03T14:05:03Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0129,,,ALAI,"Comment from ALAI, Aaron",My comment is attached as a .pdf titled &quot;NTIACommentAA1.pdf&quot;,"[('NTIACommentAA1', 'https://downloads.regulations.gov/NTIA-2023-0009-0129/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0129,
comment,2024-04-03T14:05:40Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0197,,,Tokson,"Comment from Tokson, Matthew","As my coauthors and I discuss in the attached article, open sourcing of model weights has risks that outweigh its potential benefits. &quot;Regulators should... develop a cautious approach to open sourcing of AI models. Smaller, vetted systems may well contribute to experimentation and alignment efforts by individuals or small groups. But the broad sharing of models has already proven itself problematic, with users fine-tuning large models on the toxic and racist content of 4Chan, models trained to create malware, and models that specialize in spam and disinformation generation.  Private individuals have connected AIs to a variety of tools, and the process is largely irreversible.  Restrictions on public dissemination of AI architecture, weights, biases, and even some forms of output may help prevent serious harms.&quot; See also the discussion of transparency in the intentional context in Part IV.C.1, which includes: &quot;Effective regulation of AI technology involves a smart mix of transparency and opacity measures. Transparency is positive when it promotes alignment research, enables effective monitoring of investments in potentially dangerous capabilities, and facilitates accountability among decisionmakers if they are too lax with regulated firms. Transparency is risky when it discloses machine learning techniques and architectures; when it reveals information that might jumpstart new lines of capability research; and even when it leaks model outputs that can later be reverse-engineered. The problem is complex and a pluralistic regime is appropriate....[S]ome aspects of AI developments should not be widely shared. Broad sharing of technological know-how would accelerate development, and for the many reasons we have outlined, this may be unsafe without rigorous safety and regulatory mechanisms. Note that registries do not have to be publicly open, and could confine disclosures to a regulatory body, rather than the public. The International Atomic Energy Agency (IAEA) offers one example of an international organization that accesses and analyzes sensitive information while avoiding broader disclosure.&quot;","[('Systemic Regulation of AI v 35', 'https://downloads.regulations.gov/NTIA-2023-0009-0197/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0197,
comment,2024-04-03T14:05:52Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0218,Thorn,,,Comment from Thorn,See attached file(s),"[('NTIA Comment - Thorn', 'https://downloads.regulations.gov/NTIA-2023-0009-0218/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0218,"Thorn and All Tech Is Human collaborated on Safety by Design mitigations to prevent misuse of generative AI for sexual harms against children. They highlighted risks of widely available model weights, such as misuse for creating AI-generated child sexual abuse material. They emphasized benefits of open model weights for innovation and research, advocating for standardized safety assessments and control measures to prevent misuse. They suggested model hosting platforms implement user reporting, assess models for risks, and include prevention messaging to mitigate harms. They recommended safety standards be established by third-party institutions."
comment,2024-04-03T14:05:32Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0181,The AI Alliance,,,Comment from The AI Alliance,See attached file(s),"[('AI_Alliance_NTIA_Response', 'https://downloads.regulations.gov/NTIA-2023-0009-0181/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0181,"The text provides a response to the NTIA's request for comments on dual-use foundation artificial intelligence models with widely available model weights. It highlights the importance of open foundation model weights in promoting innovation, economic growth, and AI safety. The AI Alliance emphasizes the benefits of open models in advancing scientific discovery, enhancing privacy and security, and supporting freedom of expression. It also discusses how open models can catalyze economic growth, bring AI technologies to entrepreneurs and underrepresented regions, promote competition, and enable community-driven risk management frameworks. The conclusion underscores the significance of open science and innovation in realizing the benefits of AI advancements."
comment,2024-04-03T14:05:19Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0158,Meta Platforms Inc.,,,Comment from Meta Platforms Inc.,See attached file(s),"[('Meta Platforms response to NTIA-2023-0009 RFC-26-March-2024', 'https://downloads.regulations.gov/NTIA-2023-0009-0158/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0158,"Meta Platforms, Inc. emphasizes the importance of open foundation models for U.S. economic and national security interests, advocating for common standards and continued American leadership in AI governance. They suggest bipartisan federal AI legislation and stress the benefits of responsible open-source AI. Meta highlights the risks associated with closed models, such as security vulnerabilities and the illusion of safety. They argue that responsible open-source models lead to quicker identification and mitigation of risks. Meta also discusses the importance of defining ""open"" in the context of AI models and the need for a spectrum of openness. They address the risks and benefits of widely available model weights, privacy concerns, equity implications, and potential security risks posed by state or non-state actors. Overall, Meta believes that responsibly open-sourcing model assets leads to societal benefits and improved innovation."
comment,2024-04-03T14:05:55Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0223,Open Source Initiative,,,Comment from Open Source Initiative,"The Open Source Initiative (&ldquo;OSI&rdquo;) appreciates the opportunity to provide our views on the above referenced matter.  As steward of the Open Source Definition, the OSI sets the foundation for Open Source software, a global public good that plays a vital role in the economy and is foundational for most technology we use today.  Please find our full response attached.","[(""OSI Response to NTIA 'Dual Use' RFC 3.27.2024"", 'https://downloads.regulations.gov/NTIA-2023-0009-0223/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0223,"The Open Source Initiative (OSI) appreciates the opportunity to provide views on the Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights proposal. OSI emphasizes the importance of Open Source software, highlighting its role as a global public good foundational to technology. They advocate for federal policymakers to support Open Source AI models and collaborate with organizations like OSI to establish a unified definition for Open Source AI. OSI is working on defining characteristics for Open Source AI systems and stresses the need for autonomy, accessibility, and transparency in AI development. They aim to ensure that Open Source AI models benefit society while addressing ethical and safety concerns."
comment,2024-04-03T14:06:00Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0234,Institute for Security and Technology,,,Comment from Institute for Security and Technology,See attached file(s),"[('NTIA AI RFI on model weights - IST', 'https://downloads.regulations.gov/NTIA-2023-0009-0234/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0234,"The Institute for Security and Technology (IST) submitted comments to the NTIA's request for information on Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights. The IST report highlighted six specific risks related to open foundation models, including malicious use and compliance failure. They proposed a gradient of access levels to AI models and emphasized the increasing risk profile as access levels rise. The report recommended rigorous testing and evaluation for models with widely available weights to mitigate risks like malicious use and compliance failure. IST suggested that AI governance strategies consider model weights, system components, and access levels for tailored interventions to prevent harm."
comment,2024-04-03T14:06:29Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0290,AI & Democracy Foundation,,,Comment from AI & Democracy Foundation,See attached file(s),"[('AI and Democracy Foundation Response to NTIA RFC ', 'https://downloads.regulations.gov/NTIA-2023-0009-0290/attachment_1.pdf'), (' Aviv Ovadya - Reimagining Democracy for AI - Journal of Democracy - Project Muse', 'https://downloads.regulations.gov/NTIA-2023-0009-0290/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0290,"Aviv Ovadya, a research fellow at newDemocracy and founder of the AI & Democracy Foundation, advocates for a path of combined democratic centralization and decentralization to address the challenges posed by AI advances. He proposes innovations like representative deliberations, AI augmentation, democracy-as-a-service, and platform democracy to enhance democratic processes and governance. Ovadya emphasizes the need for global agreements on AI policies and the importance of implementing democratic innovations swiftly to address the risks and harms associated with advanced AI technologies."
comment,2024-04-03T14:06:28Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0287,New America's Open Technology Institute,,,Comment from New America's Open Technology Institute,New America&#39;s Open Technology Institute has attached its comments.,"[('2024-03-27 OTI comments to NTIA (dual-use foundation AI models)', 'https://downloads.regulations.gov/NTIA-2023-0009-0287/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0287,"New America's Open Technology Institute submitted comments to the NTIA regarding Dual-Use Foundation Artificial Intelligence Models with Widely Available Model Weights. They emphasized the importance of policy approaches encouraging a strong open-source AI ecosystem in the US, advocating for common requirements for responsible development, considering national security objectives, and promoting transparency and accountability. They highlighted the benefits of open models for innovation, competition, and economic growth, while also acknowledging risks related to cybersecurity, geopolitical implications, and the need for transparency and public accountability. They recommended developing a thoughtful approach to cybersecurity software liability to incentivize innovation in open AI models."
comment,2024-04-03T14:05:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0229,Center for Democracy and Technology,,,Comment from Center for Democracy and Technology,See attached file(s),"[('CDT to NTIA comments on open foundation models 03272023', 'https://downloads.regulations.gov/NTIA-2023-0009-0229/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0229,"The Center for Democracy & Technology (CDT) comments on NTIA's request for input on ""dual-use"" foundation AI models with widely available model weights, emphasizing the benefits of open foundation models (OFMs) like BERT and CLIP. They highlight the advantages of open source software in fostering innovation, distributing power, and ensuring transparency and security. CDT argues that OFMs drive innovation, enable customization, and facilitate research that benefits both open and closed systems. They caution that limitations on publication may hinder the competitive alternative that OFMs provide to closed models. Additionally, they stress the importance of open models for security, safety, and critical research, noting that closed models may impede efforts to address AI harms effectively."
comment,2024-04-03T14:05:29Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0175,,,Foster,"Comment from Foster, Charles","[See attached file for responses to questions.]<br/>I am an AI/ML researcher who has been involved in the large language model (LLM) space for several years. Back in 2020, I was a volunteer within EleutherAI, a research community that was collecting datasets for LLMs, constructing evaluations for them, and training them in an open manner. Since then I have been in the private sector, working on applications in assessment &amp; education that rely on foundation models (both closed and open) as a base. I would consider myself very well-informed about the technology itself, and somewhat less so for the policy landscape surrounding it. I believe that foundation models will be of continuing importance, but I have been frustrated at the low quality of general discourse around the issue of their good governance.<br/><br/>Given that context, it felt appropriate to offer comments on this RFC. In the attached document, I&#39;ve tried to answer the questions posed in the cases where I felt I had something to contribute, and to indicate otherwise where I felt I didn&#39;t have anything to contribute.","[('NTIA Openness in AI RFC Comments - Charles Foster', 'https://downloads.regulations.gov/NTIA-2023-0009-0175/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0175,
comment,2024-04-03T14:06:17Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0265,Intel Corporation,,,Comment from Intel Corporation,Intel Corporation Comment Submission,"[('NTIA RFC Dual Use Open Models_March 2024_Intel', 'https://downloads.regulations.gov/NTIA-2023-0009-0265/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0265,"Intel Corporation appreciates the opportunity to comment on the NTIA's request regarding Dual Use Foundation Artificial Intelligence Models. They emphasize the importance of open AI development to foster innovation and equitable access, while acknowledging the need for responsible design to mitigate potential harm. Intel suggests a nuanced approach to defining ""open"" and ""widely available"" in AI technologies. They discuss the risks associated with widely available model weights, potential benefits of open models for competition and innovation, and the role of governments in setting standards. Intel also highlights the need for continuous evaluation, safeguards, and transparency in AI development."
comment,2024-04-03T14:06:15Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0262,Hugging Face Inc.,,,Comment from Hugging Face Inc.,"Hugging Face applauds the ongoing work of the National Telecommunications and Information Administration (NTIA) in examining dual use foundation models (FMs). The following comments are informed by our experiences as an open platform for AI systems, working to make AI accessible and broadly available to researchers for responsible development.<br/><br/>In order to best address the risks and benefits of widely available model weights, we explore both the dimension of wide availability of a model and access to model weights in our responses to avoid conflating risks inherent in the technology with risks associated with release methods. We refer to models with widely available model weights as &ldquo;open-weight models&rdquo;. <br/><br/>Please find our answers to the RFC questions in the attached document.","[('HF NTIA open weights Response', 'https://downloads.regulations.gov/NTIA-2023-0009-0262/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0262,"Hugging Face supports NTIA's work on dual-use foundation models and emphasizes the importance of defining ""open"" and ""widely available"" terms. They discuss the benefits and risks of open-weight models, recommending considerations for NTIA to define wide availability. They highlight the potential risks associated with widely available model weights, including privacy concerns. Additionally, they discuss how open models can enhance competition, innovation, scientific research, and education. Open-weight models can improve safety, security, trustworthiness, and public preparedness against AI risks. They can also advance equity in various sectors, such as healthcare, education, and criminal justice, by promoting inclusive technology development. Access to training data and associated source code of models can further enhance these benefits."
comment,2024-04-03T14:06:23Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0278,,,Hartley,"Comment from Hartley, Mary-Anne","[Article 3b-e]<br/>Traditionally, censorship has worked by blocking access to information. Since the internet, an equally effective strategy has been to flood the signal with noise, essentially blocking access through complexity.<br/><br/>Foundational Artificial Intelligence models have the potential to compress enormous-scale complexity into a massively distributed, accessible conversational interface. Closing access to transparent community development of this powerful technology is a new type of censorship: blocking access to fair representation.<br/><br/>As a medical doctor and professor of data science specializing in digital humanitarian response and global health at Yale and EPFL, I have first-hand experience of the lifesaving power of access to relevant, representative information at the right time and place. Simply informing a pregnant woman in rural Africa that her fever and vaginal discharge are signs of the utmost urgency, can change the course of two lives. <br/><br/>A staggering percentage of the global population dies from preventable causes. It is a real-time, ongoing, completely preventable existential catastrophe that deserves prioritization and radical urgent technological innovation. <br/><br/>Open-source foundation models with widely available weights have allowed our academic collaborative to produce Meditron, currently the world&rsquo;s best-performing, open-source, open-access model for medicine. Thanks to the widely available weights of Meta&rsquo;s Llama-2, we were able to actively scrutinize the model, expose it to data from critically underrepresented populations and settings, and test hypothetical counterfactual scenarios of bias and harm.<br/>We have since released this model completely open-source (from data to weights) and it has been downloaded over 30K times within three months, with particular interest from NGOs and low-resource settings. Researchers in places ranging from Peru to the Gambia have reached out feeling empowered to better represent their communities in this foundational model by implementing low-cost finetuning techniques. <br/><br/>Training foundation models are extremely resource-intensive and disproportionately inaccessible to low-resource settings. Blocking access to weights of pre-trained foundation models forces these communities to &ldquo;retrain the wheel&rdquo; which not only encourages wasteful carbon-intensive computation but is also impossible. <br/><br/>The decision to censor widely available weights will ultimately determine the size of the technological gap between high and low-resource settings. <br/><br/>My group builds AI tools for the largest global-scale humanitarian organizations, which serve a significant percentage of the world&#39;s most vulnerable and underrepresented populations. Commercial and governmental entities, by definition, cannot fulfill the core humanitarian principles of neutrality and impartiality in these models.<br/>Academia and the open-source community serve a critical and unique role by representing a neutral space for the transparent validation of new technologies. This is especially important for technologies with the power to influence life-or-death decisions. This use case deserves access to the best, cutting-edge models and widely available weights. <br/><br/>Paraphrasing Benjamin Franklin, the only thing more expensive than information is ignorance.<br/><br/>All the above is stated with an acute awareness of the important risks of open foundation models. Generative models have the potential to facilitate the creation of propaganda and non-consensual intimate images as well as degrade artistic ownership, and power weapons of mass destruction. These risks make it even more important to ensure open-source access to a global community of concerned researchers to model countless such counterfactual scenarios of risk and harm specific to their context and create protective measures to detect and respond to this inevitable risk. The decision to release foundation model weights is an opportunity to democratize the development of protective measures against technological threats.<br/><br/>Foundation Artificial Intelligence models represent global knowledge and require global community-driven oversight. Weight censorship concentrates the power of foundation models in the hands of non-neutral actors with potential commercial or partisan interests, creating an imbalance of power between citizens and monopolies. <br/><br/>In conclusion, with the power of global-scale life-saving benefits (and preventing global-scale catastrophe) comes the responsibility to share it.<br/><br/>[Article 1c]  &ldquo;Widely available&rdquo; implies democratic/representative distribution. If the term intends to define a group of privileged access holders, the term &ldquo;widely&rdquo; is not descriptive. If a privileged circle of institutions is selected, who selects them and how is equity ensured in this selection? How would leakage be prevented/tracked to avoid rendering the proposed censorship useless? These questions do likely not have equitable or realistic solutions.<br/><br/>","[('Nature2024_MEDITRON_Open_Medical_Foundation_Models_Adapted_for_Clinical_Practice', 'https://downloads.regulations.gov/NTIA-2023-0009-0278/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0278,
comment,2024-04-03T14:06:13Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0259,Machine Intelligence Research Institute,,,Comment from Machine Intelligence Research Institute,Please see the attached file for our response to the NTIA AI Open Model Weights RFC. ,"[('MIRI_NTIA_RFC_response', 'https://downloads.regulations.gov/NTIA-2023-0009-0259/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0259,"The Machine Intelligence Research Institute (MIRI) focuses on mitigating risks associated with powerful AI systems that could surpass human capabilities. They highlight the risks of losing control over autonomous AI systems and the potential dangers posed by the development and use of powerful models. MIRI suggests that model weights should only be released if it can be ensured that this will not cause significant harm. They express concerns about the future capabilities of AI systems, the unforeseen risks of releasing model weights widely, and the need for strong risk mitigation strategies before deploying AI systems. MIRI also discusses the risks associated with widely available model weights, the potential for malicious actors to exploit AI systems for harmful purposes, and the importance of balancing innovation, competition, and security. They propose limited access to powerful AI models for researchers to study their capabilities and promote safety research without allowing uncontrolled proliferation of model weights."
comment,2024-04-03T14:05:24Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0165,Reason Foundation,,,Comment from Reason Foundation,"Comment from Reason Foundation attached as PDF<br/>Authors: Max Gulker, Spence Purnell, Richard Sill","[('ReasonFdn_NTIA_OpenFdnAI_032624', 'https://downloads.regulations.gov/NTIA-2023-0009-0165/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0165,"The submission to the National Telecommunications and Information Administration by Max Gulker, Spence Purnell, and Richard Sill from the Reason Foundation discusses the benefits of open foundation AI models with widely available weights. They argue that openness allows for innovation, customization, and a more robust market ecosystem. The submission highlights the importance of allowing developers to freely choose levels of openness without recommending restrictive policies. It also addresses the risks associated with open model weights, emphasizing the need for more empirical research and caution in implementing regulations prematurely. The submission concludes that tight regulation at this early stage may hinder the benefits of open foundation AI models."
comment,2024-04-03T14:05:35Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0186,Chamber of Progress,,,Comment from Chamber of Progress,See attached.,"[('Chamber of Progress NTIA RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0186/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0186,"The Chamber of Progress supports fostering an agenda of abundance with both open and closed foundation AI models to benefit consumers and promote innovation. They emphasize the importance of not forcibly opening closed models or closing open models, advocating for an abundance of models to advance competition. They stress the need for voluntary transparency, collaboration with the tech industry, and promoting responsible AI practices to support innovation, competition, and inclusivity."
comment,2024-04-03T14:05:47Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0208,JusticeText,,,Comment from JusticeText,See attached file(s),"[('NTIA AI Open Model Weights RFC - JusticeText Comment', 'https://downloads.regulations.gov/NTIA-2023-0009-0208/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0208,"JusticeText, a startup aiding public defenders, supports the use of open-source AI models in criminal justice to enhance equity. They leverage speech recognition and natural language processing for better outcomes. Open-source tools are vital for leveling the playing field against well-funded entities prioritizing law enforcement. By combining open and closed-source tools, they create effective solutions for public defenders, addressing time constraints and technology gaps. JusticeText emphasizes the importance of utilizing open-source models to advance human rights and promote equity in the criminal justice system."
comment,2024-04-03T14:05:41Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0200,,,Sthalekar,"Comment from Sthalekar, Nikhil",See attached file(s),"[('AI_regulations_comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0200/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0200,
comment,2024-04-03T14:05:01Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0125,R Street Institute,,,Comment from R Street Institute,"The attached file contains the comments of Adam Thierer, Senior Fellow at the R Street Institute. ","[('Comments of Adam Thierer (R Street Inst) in NTIA Openness in AI RFC (March 2024)', 'https://downloads.regulations.gov/NTIA-2023-0009-0125/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0125,"The R Street Institute, represented by Adam Thierer, submitted comments to the NTIA's ""Openness in AI Request for Comment"" proceeding. They emphasized the importance of open-source AI technologies as global, general-purpose tools, highlighting their benefits and the need for flexible policy approaches. They cautioned against arbitrary limitations on open-source AI systems, stressing the ongoing need for dialogue, research, and iterative standards. The submission also discussed the risks associated with overregulation, urged a focus on bad outputs and bad actors, and advocated for collaborative, multistakeholder processes to address AI safety and security concerns. The submission concluded by emphasizing the importance of policy humility and a flexible governance approach to foster innovation and maintain U.S. leadership in technology."
comment,2024-04-03T14:05:34Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0185,,,Anonymous,Comment from Anonymous,See attached file(s). It gives some technical information as well as expresses personal opinion.<br/>Table of Content:<br/>Section I - Meta-comments on the state of human society with respect to AI and current political landscape of opinion with respect to AI<br/>Section IIA - Explaining our current understanding of LLM<br/>Section IIB - More misc. remarks and the future of AI<br/>Section IIIA - Analyzing the questions posed by NITA<br/>Section IIIB - Summary of position and policy recommendation<br/>Section IV Conlusion and Wrapping up,"[('ntia_oss_ai_comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0185/attachment_1.txt')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0185,
comment,2024-04-03T14:06:05Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0243,Software & Information Industry Association (SIIA),,,Comment from Software & Information Industry Association (SIIA),Please see attached comments from the Software &amp; Information Industry Association (SIIA).,"[('SIIA Response to NTIA on AI Open Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0243/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0243,"The Software & Information Industry Association (SIIA) responded to NTIA's request for comments on dual-use foundation models with widely available model weights. SIIA emphasized a risk-based approach to openness, cautioning against overbroad definitions and advocating for U.S. leadership in international policy alignment. They highlighted the benefits of open model weights for competition, innovation, and equity, while acknowledging risks related to privacy and security. SIIA recommended a flexible, adaptive approach to risk management, emphasizing responsible AI principles and collaboration between industry and government. They also endorsed NIST's efforts in AI safety and called for consistent international standards."
comment,2024-04-03T14:05:03Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0128,Holistic AI Inc.,,,Comment from Holistic AI Inc. ,"Please find attached Holistic AI Inc.&#39;s response to the NTIA&#39;s RFC on Request for Comments on the potential risks, benefits, other implications, and appropriate policy and regulatory approaches to Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights. <br/><br/>Regards, <br/>The Public Policy Team, Holistic AI Inc.","[('Holistic AI Response to NTIA RFC on Open Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0128/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0128,"Holistic AI is an AI Governance platform providing insights on the risks, benefits, and policy implications of Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights. They emphasize the need for nuanced definitions of ""open"" models and advocate for safety calibrations in releasing model components. They suggest a gradient-based approach for model openness, highlighting the importance of auditing, evaluation, and external oversight. They also discuss the risks and benefits of making model weights widely available, recommending standardized risk assessment procedures and proportional safety strategies. Additionally, they address regulatory mechanisms and advocate for collaborative efforts between entities like NTIA and NIST for responsible model development and deployment."
comment,2024-04-03T14:06:28Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0286,Pryon Inc.,,,Comment from Pryon Inc.,See attached file(s),"[('Pryon NTIA Submission 032724 E', 'https://downloads.regulations.gov/NTIA-2023-0009-0286/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0286,"Pryon Inc., founded by AI pioneers behind Alexa, Siri, and Watson, commends NTIA's focus on Dual Use AI Models. Pryon's mission is to bridge the gap between knowledge and individuals by extracting verifiable answers from various sources. They present comments on risks related to social harms, privacy, business, technical evaluation, and evolving viruses. Pryon emphasizes the importance of addressing these risks and offers to provide more detailed information if needed. They appreciate the opportunity to comment and believe the government's attention to this topic is crucial in the current AI landscape. Signed, Justin S. Antonipillai, Global Head of Government Affairs and Chief Trust and Legal Officer at Pryon Inc."
comment,2024-04-03T14:06:11Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0256,,,Bommasani,"Comment from Bommasani, Rishi",See attached file,"[('Response to NTIA RFC Open Foundation Models', 'https://downloads.regulations.gov/NTIA-2023-0009-0256/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0256,
comment,2024-04-03T14:06:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0249,RAND,,,Comment from RAND,See attached file(s),"[('RAND Response to NTIA RFC on AI Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0249/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0249,"RAND experts provide insights on the benefits and risks of open foundation models in response to the NTIA's Request for Comment. They discuss the potential benefits of reducing AI market concentration, enhancing AI safety research, and decentralizing decisions on model behavior. Risks include misuse by threat actors. The experts propose a structured access approach to balance benefits and risks. They introduce the concept of ""red lines"" as thresholds for dangerous capabilities that warrant significant responses if crossed, highlighting the importance of targeted risk management and coordination with foreign partners. The European Union's AI Act is cited as an example of a red-line approach."
comment,2024-04-03T14:04:40Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0088,,,doe,"Comment from doe, john",This can absolutely not pass. Open source models allow fair competition and are good for everyone and progress,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0088,
comment,2024-04-03T14:04:48Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0101,,,Conley,"Comment from Conley, James","There should be greater support for fully open models. Closed models weaken our democracy by enriching the most powerful institutions that lobby our government to the detriment of the majority. Open models also benefit America by allowing businesses to better safeguard their trade secrets, and providing capabilities that closed source models do not provide- such as greater control via fine-tuning or dynamically masking logits in the output (see ReLLM).<br/><br/>Additionally, the entire field of ML has been built on open research. If research does not continue to happen in the open, we risk drastically slowing own the rate of improvements, including those that have raised individuals concerns.<br/><br/>I also do not believe that enough material harm is attributable to language models to cause this level of concern.<br/>- Spam calls/texts have long been an issue primarily due to failure of the FCC to enforce preventative measures for automated calls/number spoofing/etc. The issue is not ease of coming up with text- but that the communication systems make it extremely difficult (if not impossible) to track down those who engage in these crimes, and also do not automatically detect/prevent spoofed calls.<br/>- Language models can be very helpful for code development- I personally use these nearly every day. But they are no substitute for expertise in compute programming. I highly doubt any individual who did not already have expertise in creating malware could use these to do much more than learn about how to do a bit more quickly. Would we ban textbooks that explain how malware often functions and teaches us how it might be prevented? Of course not, why restrict this technology that provides the same information in a more convenient package.<br/>- Generally- I believe the harms most often touted around language models are that individuals could learn how to do something dangerous. They might learn chemistry (enabling them to make explosives), software development (enabling them to make malware), 3d printing (enabling them to print a gun), and the list goes on. There is a great focus on how the worst of the worst might apply themselves to do harm, but people were always able to learn about these things- and the great majority of people who do apply those skills for the benefit of us all. Why then do we in the context of large language models talk about knowledge as some sort of terrible dangerous thing? We should be celebrating. We should be treating them as textbooks, and celebrate their broad availability (as open source models!).",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0101,
comment,2024-04-03T14:05:05Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0132,,,Clark,"Comment from Clark, Christopher","Regulation of open model weights in an industry that only come to prominence for little more than a year is premature. So far, most concerns around AI safety have been informed by conjecture and what-if scenarios instead of being backed by practical examples or hard data. As such, over regulation at this point in time would carry its own risk without providing a clear measurable benefit or proof that those regulations actually increase the safety of generative models. The open source community has already proven to be adept at discovering optimizations, theoretical architectures, and use cases that have not been demonstrated within closed source AI firms.<br/><br/>Regulation endangers this area of research and constitutes a waste of much needed talent within a nascent industry, not all areas of research are necessarily profitable or capable of garnering sufficient VC financing to clear regulatory hurdles, yet open source initiatives, like work in quantization of models that make them more efficient to run also benefits closed source firms. Over regulation, threatens these small but impactful advancements and may force talent, including international talent, to take their potential advancements elsewhere.<br/><br/>Additionally, over regulation risk creating a chilling effect within small domestic firms. The resources required to engage in many areas of open-source research into generative AI are not trivial investments, even for open-source developers. Training a small(7B-13B) foundation model can easily take many months and can cost millions of dollars. This alone incentivizes developers to act responsibly when training and releasing models and it wouldn&#39;t make sense to train something that could incur undue liability on the developers based on laws that already exist. What this does accomplish is convincing developers, especially ones that may have considered creating a firm within the US as all talent is not domestic, to consider other nations with friendlier regulations to develop their foundational models. Currently one of the most popular open-source foundational models is the Mistral 7B developed in France, not the US. Models like these would not be subject to the discussed regulations, and these regulations would encourage non-domestic development, which would frustrate security efforts as those firms would not be subject to new US regulations or current US laws.<br/><br/>Finally, because the industry and technology are new, even industry leaders can&#39;t be expected to predict the evolution of the technology within their own firms or with their competitors. Generally markets are more efficient when there is more competition not less. For example, in the operating system market, the largest and often more secure operating systems are open-source offerings like Android and Linux. The open-source nature of these systems makes it easier for developers ensure there aren&#39;t more vulnerabilities in their software. In the case of closed sourced offerings like Windows, security becomes the sole responsibility of a single firm, and its success makes it a lucrative target for discovering exploits as the pay offs for compromising the operating system offers outsized rewards for bad actors.<br/><br/>For the reasons listed above, I believe regulators should avoid overregulating the AI industry based on theoretical threats and should instead take the time to investigate the technology, it&#39;s advancements and only address actual evidence-based safety concerns when they are proven to not be adequately covered by existing regulation and laws. Otherwise the US risks falling behind its peers in a nascent industry before it is fully understood.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0132,
comment,2024-04-03T14:05:16Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0152,,,Pressman,"Comment from Pressman, John David","I am a deep learning researcher who works on open language models and previously<br/>worked on image models, including the first release of Stable Diffusion.<br/><br/>There are two forms of AI control we care about, frequently conflated:<br/><br/>1. Our ability to control what other people do with AI models.<br/>2. Our ability to control the behavior of AI models in and of themselves.<br/><br/>As these models become more powerful and are granted more autonomy, the<br/>2nd concern will probably begin to dominate. While we have made substantial<br/>recent progress on the &#39;AI alignment&#39; problem in the form of RLAIF and control<br/>vectors it is still not fully resolved. I share much of my opinion with the <br/>neuroscientist Beren Millidge, formerly head of research at the AI safety<br/>firm Conjecture:<br/><br/>https://www.beren.io/2023-11-05-Open-source-AI-has-been-vital-for-alignment/<br/><br/>Beren argues that open code and models have been the source of much if not most <br/>recent alignment progress and will probably be responsible for an increasing fraction<br/>over time if allowed to continue. He writes, &quot;The fundamental reason for this <br/>is a simple one. Most likely, the solution to alignment will not be a theoretical <br/>breakthrough but instead a culmination of practical, pragmatic and empirical <br/>developments. To make progress, we need people who are able to tinker and play <br/>with and iteratively improve the alignment of models.&quot;<br/><br/>As far as I can tell this is completely correct. My current research into AI<br/>alignment and control, some of which you can read about here (https://github.com/JD-P/minihf) <br/>would be prohibitively expensive without the existence of open models such as<br/>GPT-J, GPT-NeoX, OpenLLaMa, Mistral and Mixtral, etc. Which is to say it would<br/>probably not exist. In addition to my experience with deep learning I spent over a decade as<br/>a member of the LessWrong &#39;rationalist&#39; movement that pioneered early research into <br/>alignment and funds most lobbying about AI. Their early recognition was <br/>skillfully parlayed into ten years spent on various forms of mental masturbation. I <br/>no longer consider myself a member because if it was up to them we would go straight <br/>back to thought experiments based on their model of the problem they have barely updated<br/>since Nick Bostrom&#39;s 2014 book *Superintelligence*, which predates the existence of the <br/>transformer architecture (2017), let alone GPT-2 (2019).<br/><br/>Beyond the research use of open weights, I think the primary benefit to end users is<br/>the ability to update the model, for good or ill. I see it typically argued that this <br/>mostly benefits illegitimate uses since those are the ones hosted providers won&#39;t <br/>let you do. I *wish* that model hosts only banned illegitimate uses, left to their<br/>own devices providers make arbitrary and capricious rules dominated by PR and <br/>ideological concerns, as OpenAI demonstrated with their months long ban on the <br/>word &quot;Ukraine&quot; in their DALL-E 2 model:<br/><br/>https://twitter.com/EMostaque/status/1637244696333541378<br/><br/>This sort of policy is not the exception with these firms, it&#39;s the rule. This<br/>was more recently demonstrated by Google&#39;s &quot;woke&quot; Gemini model, which garnered<br/>a lot of headlines for silly refusals and absurd &#39;diverse&#39; depictions<br/>of named historical figures. Each AI firm has its own bespoke house rules for what is and<br/>isn&#39;t allowed. Since the natural market structure for deep learning is likely<br/>oligopoly (it&#39;s a high capital business with a rich-get-richer effect where<br/>successful products are flooded with user data and feedback making the product<br/>even better) in the medium term this could represent a substantial risk to freedom<br/>of expression in the United States. <br/><br/>If this was the only thing it could be mitigated by codifying some kind of neutral<br/>standard for AI assisted speech. However I think that for society to get the full<br/>usefulness from these models it&#39;s plausible the update mechanism is crucial, and<br/>we have not yet seen the full benefits. What is important to understand is that<br/>these models are a form of cultural accumulation, and like us they have a learning<br/>loop. While it is not fully known how humans learn, one loosely accurate story might<br/>go like this:<br/><br/>1. Humans have a context window of about 24 hours of experience, which lets<br/>them do shallow learning by fitting new stuff to what they already know.<br/><br/>2. During sleep the important parts of this context are transferred into long term<br/>memory and used by the hippocampus to update subnetworks.<br/><br/>3. Some of these memories are turned into external artifacts such as text which other<br/>people can learn from.<br/><br/>Right now deep net inference methods skip step 2 for cost reasons. I suspect processes that<br/>create new genres, perform scientific research, discover new math, or anything else that<br/>requires going beyond the initial training data will be inhibited without the ability to <br/>update the model in a continuous learning loop. Commercial firms are unlikely to offer<br/>this service because it is not immediately commercially profitable.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0152,
comment,2024-04-03T14:04:06Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0025,,,Perez-Gonzalez,"Comment from Perez-Gonzalez, Hector","I just cannot agree with such a thing like this, open models make it fair for many other companies to compete against the closed sourced alternatives and closed source gives too much power to the large AI companies. If open model weights were to be over regulated this will not be good for the people of the US and we will be left out of the open collaboration and development that currently happens with open models. Either way such a regulation would only affect the USA. Most open models were developed outside the USA.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0025,
comment,2024-04-03T14:04:26Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0062,,,H,"Comment from H, D","I think the regulation of Open Models is going to do nothing but hurt the community, it will remove the open models from those doing work to further the technology and improve things for the general user, where as if the regulation and preventing of open models were to go ahead, it would do little towards stopping people that actually want to use it for malicious intent, all this will do is drive malicious models to a dedicated forum\area on the internet.<br/><br/>With the locking down of the models there will also be corporations locking down access, charging stupid amounts of money in a world filled with enough corporate greed that people struggle to live, for access to their APIs or models which means people like me without a large amount of spare money after bills are paid will not have access, I find that saddening as I already have a lack of affordable hobbies and interests and I am sure there are millions like me.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0062,
comment,2024-04-03T14:04:25Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0061,,,L,"Comment from L, Francis","It is frankly unethical for companies to create models without sharing their weights. This nation is founded upon liberty, and the idea that &ldquo;all men are created equal&rdquo;. To treat individuals like infants that need to be kept safe against harmful model user is an abuse of the rights of the American people.<br/><br/>You might say that to create open source models is dangerous. There is no level of danger so high, however, that it is worth violating the core principles of our nation to avoid it. People died by the millions to protect liberty, and to attempt to block the freedom of expirementation, science, and of creating code is an insulte to their memories.<br/><br/>Those who stand in the way of technology are standing in the way of humanity. Those who would prevent the average man from accessing the technology a company controls would likely at heart prefer that man to have no rights, nor equality to a company, if it ensured his safety. Such ideas belong not in a democracy, but in an autocracy.<br/><br/>Not only is it blatantly un-American to attempt to censor science and technology in the name of fear-mongering politics, but it is also simply no a nations right to do so. Any nation that would prevent the spread of a information-based technology for &ldquo;safety&rdquo; is one that clearly does not value free-speech. To take away guns for fear of them being misued to kill people is an abuse of liberty. To take away large language models for fear of them being misused to generate what you call &ldquo;harmful content&rdquo; is not just an abuse of liberty, it is a censorship on a scale calling doubt onto America&rsquo;s status as a &ldquo;free nation&rdquo;.<br/><br/>I do not doubt that the NTIA&rsquo;s desire to block open-source models is a manifestation of the larger fear of a loud but uninformed fraction the American people against technology. It is a fear born in ignorance, in insecurity, and frankly it is inhuman. If you bow to the cowardly sentiments of those who want to rob humanities future from us, then I do not doubt that another, better, braver country shall look upon America with scorn. And that country, will become at that moment a superior nation to America, and a freer nation as well. If you are so craven that you do not wish to see America lead humanity to its destiny, then so be it. But do not pretend that you are doing it &ldquo;for the good of the people&rdquo;- your fear-mongering censorship and idiocratic populism could not be farther from that.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0061,
comment,2024-04-03T14:04:35Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0079,,,Madere,"Comment from Madere, Jared",This needs to be open for the creation of visual and sonic artworks- this conversation of closed weights is fear monkeying by repulsive corporations seeking to use under-informed law makers as a way of securing a monopoly in their industries- there is no safety risk ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0079,
comment,2024-04-03T14:04:20Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0051,,,Anonymous,Comment from Anonymous,"The ONLY way to have safe AI is publishing the exact steps used to create each user facing AI model so that everything is actually and trully open, transparent and reproducible by everyone. Everything else is for-profit corporate speech by those who want to monopolize and lock down instead of democratize.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0051,
comment,2024-04-03T14:05:22Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0162,,,Lopez,"Comment from Lopez, Kevin","Open weights and open models should not be regulated. It allows regular people to experiment and learn about ML. Regulating open source is bad. Imagine trying to regulate the linux kernel, the world would be in a much worse place. If you want to regulate someone regulate OpenAI as they hold a lot of power and may not make decisions that help the greater good. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0162,
comment,2024-04-03T14:06:01Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0236,,,Anonymous,Comment from Anonymous,"This ban would have negative consequences across many fields and scenarios. Here are some of them.<br/><br/>Reduced Competition: Limiting access to open-source AI weights would mean that only large companies like Google, Microsoft, and NVIDIA have access to advanced AI models. This could lead to a monopoly-like situation where these companies dominate the market, stifling innovation from smaller players and startups. Smaller companies may not be able to compete effectively, leading to less diversity in the marketplace and potentially higher prices for consumers. <br/><br/>Economic impact: According to Goldman Sachs, 300 million jobs could be lost due to AI. If only a few large companies have access to advanced AI technologies, there may not be enough job opportunities created to replace those lost due to autmoation. This can lead to increased unemployment rates and a decline in tax revenue.<br/><br/>Regulatory Challenges: Banning open-source weights could create regulatory challenges, as policymakers may struggle to monitor and enforce restrictions on access to these models effectively. This could lead to unintended consequences, such as the proliferation of unregulated AI models that are not subject to transparency or ethical standards. <br/><br/>Public Perception of AI: Banning open-source weights could also impact public perception of AI technologies. Restricting access to these models might fuel skepticism about the benefits of AI and lead to increased mistrust among consumers, which <br/><br/>National Security Implications: Restricting access to advanced AI technologies could have implications for national security. If only a few large companies have access to these technologies, it could create a power imbalance that could be exploited by malicious actors or adversaries seeking to gain an advantage in cyber warfare or other strategic domains. <br/><br/>Consumer Privacy: With closed-source models, it becomes challenging for consumers to understand how their data is being used or shared by the companies providing these services. Monopolies in this space may take advantage of this lack of transparency, leading to potential misuse or mismanagement of consumer data, which can have severe consequences for individual privacy rights.<br/><br/>Undermining US Leadership: The United States has historically led the world in technological innovation. A ban on open-source AI models could undermine this leadership position, as other nations may capitalize on the opportunity to fill the void left by the United States in this rapidly growing field. China for example is also doing regulation but is doing it in a way to make sure there is growth.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0236,
comment,2024-04-03T14:06:20Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0272,Fraxion,,,Comment from Fraxion,"I have been using the ecosystem of &quot;open&quot; advanced AI models for six months. In that time I have launched a stealth startup with a working POC while recovering from a disabling condition. None of this would have been possible without open foundation models. I am able to give myself privacy with an AI running on hardware I control. I am able to run my &quot;open&quot;AI model on my hardware for the cost of electricity. The same workload ran in the cloud cost $300 for 15 minutes. <br/><br/>As an American I support anything that pushes freedom forward. Open foundation models do this. They also put power in the hands of everyday Americans. We are facing a battle for control of the evolution of thought. We need to proceed carefully but without fear. We are the land of the free and the brave. We need to make sure this technology is not closed off and only for the elite. <br/><br/>Question 1: <br/>a) There is evidence that model weights for some AI systems have become widely available over time as the technology matures and the models are improved upon. Historical examples include the release of model weights for image recognition systems like ImageNet or natural language processing models like GPT-2/3. However, the timeline for open sourcing model weights can vary significantly depending on factors such as the complexity of the model, the level of competition in the field, and the interests of the developers or organizations behind the models.<br/><br/>b) It&#39;s difficult to provide a general estimate for the timeframe between the deployment of a closed model and the deployment of an open foundation model of similar performance on relevant tasks without specific context or examples. This timeline can range from a few months to several years or more. The variables that may affect this timeline include the pace of technological advancement, the level of competition in the field, the interests of the developers or organizations involved, and regulatory considerations.<br/><br/>c) The level of distribution can be one way to define the &quot;wide availability&quot; of model weights. However, it&#39;s important to consider other factors such as accessibility (e.g., cost or technical expertise required), usability (e.g., documentation or support provided), and the potential for misuse or harm associated with the model weights. There isn&#39;t a universal threshold for defining &quot;wide availability,&quot; but it should be assessed on a case-by-case basis based on the specific context and stakeholders involved.<br/><br/>d) Different forms of access to an open foundation model can provide varying levels of benefit or risk depending on factors such as the intended use case, the capabilities of the model, the level of control over the model&#39;s inputs or outputs, and the potential for unintended consequences or harm. For example, web applications or APIs may offer more control over access and usage but also require internet connectivity, while local hosting or edge deployment may provide greater autonomy but may also increase the risk of data breaches or unauthorized access.<br/><br/>e) There may be promising prospective forms or modes of access that could strike a more favorable benefit-risk balance in specific contexts. For instance, federated learning or distributed training approaches could enable collaboration while maintaining control over data and model weights at the local level. Another example could be the development of explainable AI (XAI) techniques that allow users to understand how a model arrives at its predictions, thereby reducing the potential for misuse or harm while also increasing trust in the system.<br/>i)Yes, there are several promising prospective forms or modes of access that could strike a more favorable benefit-risk balance in the context of open foundation models:<br/><br/>Federated learning: This approach allows multiple parties (e.g., organizations or individuals) to collaboratively train a model while keeping their data locally, reducing the risk of data breaches or unauthorized access. The model weights are only shared during training, and the final model weights are not exchanged between parties. This can enable collaboration while maintaining control over data and model weights at the local level.<br/>Distributed training: Similar to federated learning but without the need for a central server or coordinator, distributed training involves splitting the training data across multiple nodes (e.g., devices or servers) and training the model simultaneously on each node. This can improve training efficiency while also reducing the risk associated with centralizing data or model weights in a single location.<br/><br/>(continued in attached document)",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0272,
comment,2024-04-03T14:06:21Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0273,,,Anonymous,Comment from Anonymous,"In a democracy, and open-market society, it is imperative that individuals hold the ability to express their own self-determination regarding their works, their operations, and their capacity to interact with the world around them. <br/><br/> The only way for our democracy and free market to survive this next generation will be to maintain the right for individuals and smaller communities to Architect, Build, Train, Maintain and Operate their own AI models, particularly over their own local machines. <br/> When digital interactions have become so complex and rooted in exploitative parties, AI generated content and interactions will quickly become the dominant content across the internet.<br/><br/> Users being able to operate their own AIs will be absolutely critical in filtering out what is true versus generated, or politically curated content. <br/><br/> If open sourced AIs, or locally hosted AIs are banned, then Americans will be effectively stripped of their last remaining mechanism of expression in the AI age. Our society, that constantly violates, undermines, and exploits their daily interactions in order to extort them into compliance is not a democratic society. <br/>Consolidated AI powers exclusively into large profit-oriented enterprises will guarantee that our democracy and free market will fail and give way to a path to corporate feudalism.<br/><br/>While the US citizens still wait for a basic acknowledgment of simple digital rights, our last means of retaining individual consent (self-determination) will be effectively stripped away with a ban on open sourced AI models and locally hosted AI applications.<br/><br/>It is imperative for our future that we foster this capacity in our open sourced and local AIs. <br/><br/>If such a ban were to be enacted, AI professionals (myself included) would be forced to emigrate to other countries in order to express their individual rights to live free in a hyper consolidated, hyper-controlled digital landscape.<br/> AI communities, AI work, and therefore AI progress would then be developed outside the US, where the states would continue to regress into the new dark ages of instantly curated propaganda and fake information, stripped of any means to fight back.<br/><br/> Banning open sourced AI is tantamount to banning self-determination for Americans. it is dangerous and should be avoided at all costs.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0273,
comment,2024-04-03T14:05:29Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0176,,,Anonymous,Comment from Anonymous,"Banning open model weights will lead to the nightmare you are trying to avoid. Bad actors will always have access to these tools, be they developed here or abroad (AI has made translation trivial, and ask the MPAA how easy it is to stop file sharing). Hampering open source efforts will only result in slower progress and a dangerous consolidation of knowledge/power into the hands of a small number of big corporations. You actively seek to prevent companies that stifle competition because they grow too powerful in their market. Locking down open model weights will put this transformative, world changing technology into very similar hands. They don&#39;t even need to have bad intentions; the mere fact that they are businesses means their interests will not always align with what is best for all citizens. Don&#39;t buy into the doomsday scenarios, listen to the experts. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0176,
comment,2024-04-03T14:05:31Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0179,,,Cook,"Comment from Cook, Wes","I believe that encouraging open-source tools and open weight models will be very critical for democratizing AI.<br/><br/>Safety is important, as is a focus on reducing harmful bias.  However there is also a great risk that installing too many hurdles will hurt open-source efforts.  Open-source development are often pivotal in creating technologies and experimenting with new techniques that can push the industry forward.  Much of the technological foundation of our world is built on such open-source efforts, such as the Linux operating system, and most web servers and web browsers.<br/><br/>Enforcing too strict of rules also threatens to give a leg up to the largest companies that have already invested in AI.  They have the resources to overcome most obstacles, but young startups will be greatly slowed down.<br/><br/>It is clear that these tools are going to be integrated into every piece of our lives shortly: writing aids, article summarizers, image generators, and more.  I feel that it would be a much better world if the best models were open-source and available for all to use, rather than owned by a select few tech giants.<br/><br/>Thank you for your time.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0179,
comment,2024-04-03T14:05:33Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0182,,,D,"Comment from D, Greggy","1) Open models allow small businesses to leverage the power of artificial intelligence without paying big money to already huge closed source behemoths. Limiting open source models is bad for small business and bad for innovation.<br/><br/>2) Software is speech, and freedom of speech means that if I produce data, I should be free to release it so long as I am the originator of the code or the coded and model are produced with the understanding of all contributors that the information is freely available. Limiting open source models is an attack on freedom of speech.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0182,
comment,2024-04-03T14:04:00Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0015,,,Anttila,"Comment from Anttila, Peter","Thank you for this opportunity to share industry expertise on the topic. As a security engineer at a top tech company that has worked extensively with AI security and safety questions, the most pressing reason why regulating open weights is bad for the industry and academia is that while regulation would put AI/ML into a more known state insofar as risk is concerned, AI/ML as a field overall is subject to substantial iterative development pressure making most individual software engineering artifacts like model weights obsolete within just a few short years. Since AI/ML is a significant target of research investment in many top economies, regulating these models would significantly hold back the US&#39;s ability to compete in the space of AI/ML since a vast majority of quick improvements to proprietary US developed models are being done currently stem from lessons learned from open source models (e.g. implementing a technique or improvement). Though papers can in theory be read to implement such improvements, in practice the industry has used buzz around new models as a filter for what to pay attention to (arxiv publishes around 100 AI/ML papers per day, far more than anyone can keep up with) and so since countries at this point know that AI/ML is going to be a significant economic advantage to any country that cracks the code on entrenched industry problems with them, the US in self-regulating will lose a massive competitive advantage and likely lose the lead to one of its political or economic rivals if regulation is adopted. Several other nations are moving in the opposite direction entirely (i.e. deregulation, see Japan copyright laws for model training) as a means to secure competitive advantage, and so in my professional opinion, even as someone whose entire job revolves around safety and security, is that what degree of risk AI may pose to us from within from a disruptive model or technology is not at all mitigated by regulation, but instead will likely all but guarantee that an economic or political rival of the US will have the opportunity to obtain this disruptive technology first and by this fact itself also allow a US rival to make use of it from a position of power at the expense of US interests. The largest risk stemming from AI/ML, in short, is that we don&#39;t win the race and get a seat at the winner&#39;s table to dictate what happens to those who didn&#39;t win - the disruption to economies or industries will happen with or without us, as many disruptive open access models today are already mostly funded from non US money.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0015,
comment,2024-04-03T14:04:18Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0047,,,Schmitt,"Comment from Schmitt, Alex","I would like to express my strong support for the protection and preservation of public access to open-source artificial intelligence models, and to outline why I believe this is of critical importance for the safety and benefit of all mankind.<br/><br/>Before I begin, I would like to state clearly that by &quot;open-source&quot;, I mean the release and public availability of AI models, including model weights for download, allowing for independent local hosting, subsequent fine-tuning, and model usage. For example, I would consider Meta&#39;s LLaMA models, and StableAI&#39;s StableDiffusionXL models, as &quot;open-source&quot;, since these models were released publicly for download on HuggingFace, papers were published regarding their architecture and training, and they are widely available for free use, fine-tuning and subsequent modification. Conversely, I would consider OpenAI&#39;s GPT-4 model to be closed-source, since very little technical information has been published, the model weights are not available for download, and the model can effectively only be used through OpenAI&#39;s websites and API endpoints, which generally require payment to OpenAI. <br/><br/>Due to character limits, I will outline several broad reasons why I believe that providing public access and supporting open-source AI is of critical importance, and then I will address a questions from the RFC where possible.  <br/><br/>First, AI is a nascent field, and by stripping the public of access, research in the US will be significantly stifled, causing the US to fall behind other countries that are less restrictive, which could have enormous ramifications for the future of the US economy. For example, the open-source community has made great strides in reducing the compute cost associated with fine-tuning language models and performing inference on consumer hardware through optimizing inference back-ends and developing new methods of quantization. These are huge, significant advances that everyone, including large US tech companies have benefited from. Competition in this space, especially from open-source users, ensures that AI models are developed efficiently and in ways that benefit real individuals, not just massive corporations like OpenAI, Google and Microsoft. <br/><br/>Second, security through obscurity does not work. This has been established in many other areas of technology. For example, many leading encryption algorithms, such as AES, are open-source. The open-source paradigm allows for more rigorous vetting and analysis by a wider audience, which ultimately ensures better outcomes. &quot;Black box&quot; models with no published weights, no technical documentation, and no transparency create huge risks. Companies like OpenAI may align their models to ensure that their outputs are &quot;safe&quot; and politically correct, but its impossible to know what risks may exist in such an opaque system, where training data, alignment processes, and model architecture are completely secret. There may be serious bias problems hidden in these models, or secret combinations of tokens that can be used to jailbreak these models for harmful applications. Limiting open-source ultimately reduces model diversity, exaggerating the biases of the surviving popular models.  <br/><br/>Third, the economic impacts of AI are still hard to fathom, and putting control of AI and AGI in the hands of a few wealthy tech corporations could have disastrous consequences. Without transparency and access to open-source competition, companies seeking to improve operational efficiency by leveraging AI are unable to innovate and re-skill workers to become developers, researchers and users of AI. They are instead forced to simply replace workers with software subscriptions and paid API access to blackbox AI models. This enriches tech companies at the expense of diversity, innovation, and consumer well-being. The NTIA and the US Government should promote democratizing AI by supporting transparent open-source research and ensuring that real individuals benefit from the development of AGI. Offloading this responsibility to for-profit entities like OpenAI and Google is reckless, as those companies have a fiduciary duty to enrich their shareholders, but no true duty to ensure a healthy, functioning society in a post-AGI era. Open-source and closed-source paradigms can exist symbiotically, with each benefiting from the other. Diversity of viewpoints is the ideal, and that requires diverse stakeholders, ranging from Google/OpenAI/Meta to individual developers like kaiokendev. <br/><br/>Q 1c:<br/>&quot;Wide availability&quot; should mean publicly available for download by any interested consumer. Requiring sign-up/request is acceptable. (IE Meta LLaMA)<br/><br/>Q 3: <br/>There are too many benefits to list them all here. Access to open foundational models is critical for efficient research of hundreds of complicated topics, such as alignment, context, fine-tuning and more.<br/><br/>Please, I emplore you to support the transparent open-source paradigm. Empower individuals to innovate for the benefit of all.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0047,
comment,2024-04-03T14:04:44Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0096,,,Anonymous,Comment from Anonymous,"<br/>I am writing to express my strong support for open-source AI models and to address the concerns I have regarding the current trajectory towards closed-source models, like the one embodied by ChatGPT. My advocacy for open-source AI stems from a fundamental belief in the principles of accessibility, transparency, and equity, which I fear are at risk in the rapidly evolving landscape of artificial intelligence.<br/><br/>Open-source models are pivotal in democratizing access to AI technology. By allowing a broad spectrum of individuals and organizations to explore, modify, and improve upon existing code, open-source AI fosters innovation and inclusivity. This openness not only accelerates technological advancement but also ensures that the benefits of AI are shared widely, rather than being monopolized by a few well-resourced entities.<br/><br/>Conversely, closed-source AI models like ChatGPT represent a move towards centralization of knowledge and power. While these models have undoubtedly contributed to significant advancements in AI, they also pose a risk of exacerbating wealth inequality. By restricting access to the underlying algorithms and data, closed-source models create a barrier to entry for smaller players and concentrate the capabilities and profits in the hands of a few large corporations. This not only stifles competition and innovation but also leads to a scenario where the vast majority of economic gains from AI are accrued by those already in positions of strength.<br/><br/>Moreover, closed-source models are often developed with proprietary datasets that are not publicly disclosed, making it difficult to understand, question, or improve upon the decision-making processes of these AI systems. This lack of transparency can lead to biases and unfair practices that perpetuate societal inequalities.<br/><br/>In the spirit of fostering a more equitable and just technological future, I advocate for policies and practices that support the development and use of open-source AI models. Governments, educational institutions, and private sector entities should work collaboratively to create an ecosystem that encourages sharing and innovation, ensuring that the transformative potential of AI is accessible to all.<br/><br/>I urge policymakers, technologists, and business leaders to consider the long-term implications of closed-source AI dominance and to actively promote an open-source paradigm. By doing so, we can mitigate the risks of wealth inequality and work towards a future where AI acts as a lever for positive societal change, rather than a divider.<br/><br/>Thank you for considering my views on this critical issue. I look forward to seeing a global movement towards more open, transparent, and equitable AI development practices.<br/>",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0096,
comment,2024-04-03T14:04:52Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0108,,,Bockelie,"Comment from Bockelie, Aaron","The dialogue around the control and distribution of AI model weights resonates with historical attempts to govern technological access and innovation. Such efforts highlight a perennial challenge: when entities&mdash;be it individuals, organizations, or nation-states&mdash;aim to acquire certain capabilities, they usually find a way to do so, sidestepping legal and regulatory barriers. This is especially pertinent in the sphere of AI, where limitations on model weights might not only impede innovation but also hinder a group&#39;s ability to stay competitive internationally.<br/><br/>Moreover, the push for regulation often reveals underlying agendas, with proponents of restrictions potentially standing to gain financially or politically. This dynamic brings to light concerns about the motivations behind regulatory proposals, suggesting that the pursuit of control might benefit those in positions to influence the narrative, at the expense of broader technological advancement and equity.<br/><br/>This situation calls for a balanced approach to the development and distribution of AI technologies. Instead of imposing blanket restrictions that could stifle progress and equity, fostering an environment that promotes responsible development, ethical use, and equitable access to AI advancements is preferable. Such an approach would encourage transparency and ethical standards, ensuring that the benefits of AI are widely distributed while managing the associated risks collectively.<br/><br/>Remaining vigilant against the potential for regulations to be shaped by those with vested interests, and policies related to AI should not disproportionately favor a select few (through passive technological selection or otherwise), nor should they be influenced unduly by those seeking to gain from the control mechanisms they advocate for. This perspective of scrutinizing the motivations behind regulatory efforts and advocating for openness and collaboration in the AI domain to ensure that technological progress serves the greater good rather than narrow interests.<br/><br/>The discourse on regulating AI model distribution necessitates a nuanced and open understanding of both the practical challenges of controlling technological proliferation and the complexities of the motivations driving these regulatory efforts. By prioritizing ethical development and equitable access over restrictive controls, we can navigate the intricate landscape of AI innovation in a manner that benefits all stakeholders involved.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0108,
comment,2024-04-03T14:05:23Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0164,,,Guijarro,"Comment from Guijarro, Anthony","Meta research has been working hard and spending millions to make weights available to any small to large business that benefits from them. Google has also worked on Kaggle to allow anyone to learn about machine learning for free. Taking a look at competitions hosted on Kaggle you&rsquo;ll see big names like Stanford encouraging users to create algorithms for better study on human health for a prize. Having open weights encourages research in this area and has definitely encouraged me as a student to be interested in learning about neural networks. I know the government is worried about China and AI, so I want to make a comment on the fact that this will remove competition with China. This will cause China to have more power over AI. Trustworthy companies for support on AI safety could possibly be Anthropic and EleutherAI. There are risks with people potentially gathering data to do illegal activities but then we may as well just restrict the internet.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0164,
comment,2024-04-03T14:06:22Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0275,,,Sly,"Comment from Sly, Taylor","<br/><br/>3. What are the benefits of foundation<br/>models with model weights that are<br/>widely available as compared to fully<br/>closed models? <br/><br/>Model weights that are freely available give the public the opportunity to participate in the development of AI. With the weights in the open, tools can be build by everybody that also benefit everybody. By fully closing models, you risk single entities controlling a new powerful technology with no accountability of how those technologies are applies. With open model weights, you automatically get accountability through transparency.<br/><br/>3a. What benefits do open model<br/>weights offer for competition and<br/>innovation, both in the AI marketplace<br/>and in other areas of the economy? In<br/>what ways can open dual-use<br/>foundation models enable or enhance<br/>scientific research, as well as education/<br/>training in computer science and related<br/>fields? <br/><br/>Open model weights allow any company, educational institution, or person to deploy and gain the benefits of AI. Small businesses that would otherwise need to pay large corporations for closed models are able to experiment and test open models to see if AI is the correct strategy for them. Having open models makes the entire business landscape more diverse and competitive which stimulates innovation and progress. Open models gives educational institutions fuel to make progress both in the education of young learners who will inherit a world where AI is commonplace and fuel the highest reaches of academia, even well beyond the scope of computer science. Having open models that are fine tuned to aid in a specific type of research, for instance health and medicine, will allow the benefits to be spread more widely.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0275,
comment,2024-04-03T14:06:22Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0276,,,Yarbro,"Comment from Yarbro, Brad","Concerning the dangers of these models being open source, consider that the dangers must be compared to the baseline dangers of a bad actor simply having access to the internet and a search engine (ex. Google).<br/><br/>There should be more concern with the data used to train these models, rather than just the model weights or parameters.<br/><br/>The competitive moat of these large companies capable of building these models comes from the data they have access to and the amount of compute time necessary to train.<br/><br/>In a recent Wall Street Journal interview with OpenAI&rsquo;s CTO regarding their Sora text-to-video model, she admitted to using publicly available data and licensed data.<br/><br/>Which companies are selling or allowing use of their data to OpenAI?<br/><br/>OpenAI and other companies should be more transparent with their use of data from sources such as Facebook, Instagram, and YouTube.<br/><br/>There could be data leakage and hallucinations as video output that would violate or encroach on United States citizens&rsquo; privacy.<br/><br/>There are many more things to consider, but the more open source competitor models there are, the more accountable OpenAI and other large companies will have to be, with respect to lawmakers and citizens of the United States.<br/><br/>For the risk of sexual content being created by these models, all porn should be banned, which a similar solution could apply to AI models capable of sexually explicit material.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0276,
comment,2024-04-03T14:05:34Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0184,,,Anonymous,Comment from Anonymous,"The risks of models with widely shared weights are similar to the risks of models with private weights. Sophisticated strategies for prompting and combining answers can substitute for fine tuning to a substantial extent, both for avoiding safety controls installed by closed model providers and for boosting model capabilities. While fine tuning is better for some applications, to my knowledge there has been no demonstration that the difference is large for security critical applications, controlling for the amount of effort invested in each. In contrast, there probably are significant differences in the level of risk presented by models whose use is limited to a small number of highly trusted individuals compared to models that can be used by anyone, whether they have open or closed weights. I suspect a regulatory stance that is restrictive towards open weight models and permissive towards closed weight models would embody an inconsistent attitude towards risk.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0184,
comment,2024-04-03T14:05:37Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0190,,,Anonymous,Comment from Anonymous,"Any fair evaluation of the AI landscape, including the domain of open sourcing models, must accept two fundamental truths: 1) the technology is quickly evolving (capabilities doubling in ~1 year) and very new (most algorithms and advanced hardware architecture under 5 years old). No one has deeply studied it, no one has extensive experience with it. Even &quot;experts&quot; have been repeatedly surprised; surveys are unambiguous about this. 2) If we take the creators of this technology seriously about their ambitions and their hopes for AI, it will be an unimaginably impactful technology, for ill or maybe for good. They may of course be wrong about the things they&#39;re creating. Maybe they&#39;ll never work/never amount to much. But THEY SAY the technology will have unprecedented impact. <br/><br/>Given (1), we should be humble and cautious about this technology. This, to me, argues against taking any irrevocable action. Releasing an open model is an irrevocable act. If a dangerous model is open sourced, it can&#39;t ever be taken out of all actors&#39; hands.<br/><br/>Given (2), we should say if the creators are wrong and AI is soon to fizzle or plateau, then restricting the sharing of open source models will have no significant negative effect. If they&#39;re right, and AI is soon capable of super human action, open sourcing could rapidly destabilize the world, and cause incalculable harm.<br/><br/>In short, open sourcing (at least of frontier models) seems to have limited upside and huge downside.<br/><br/>Open sourcers should ideally be restricted, at least until the field is mature enough for a proper cost benefit analysis and for humanity to learn how to tell a dangerous model from a safe one--which we currently don&#39;t even know how to do reliably! If banning open sourcing is impossible, those who do so should be held strictly liable for any harm done by those models. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0190,
comment,2024-04-03T14:05:39Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0194,,,Krellenstein,"Comment from Krellenstein, Adam",I&#39;m an engineer and technology executive and I think that releasing model weights should be carefully regulated because modern AIs have potential to cause widespread harm (like many other technologies).,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0194,
comment,2024-04-03T14:05:43Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0201,,,Kirk,"Comment from Kirk, Quentin ","Open-source weights, or freely available machine learning models, offer several important benefits from scientific, political, and national security perspectives. Here is an argument for why they should remain largely unrestricted in the United States:<br/><br/>Scientific Perspective:<br/>1. Open-source models promote transparency, reproducibility, and collaboration in AI research, enabling scientists to build upon each other&#39;s work and accelerate progress.<br/>2. Unrestricted access to these models allows researchers from diverse backgrounds and institutions to contribute to the field, fostering innovation and diverse perspectives.<br/>3. Open-source models facilitate the development of AI solutions for important scientific challenges, such as disease diagnosis, climate modeling, and space exploration.<br/><br/>Political Perspective:<br/>1. Restricting open-source models could give rise to concerns about censorship and infringement on free speech and academic freedom, which are core values in a democratic society.<br/>2. Open access to these models promotes accountability and public trust by allowing independent audits and scrutiny, ensuring that AI systems are not biased or used for nefarious purposes.<br/>3. Restricting open-source models could put the United States at a disadvantage compared to other nations with more open policies, hampering innovation and economic competitiveness.<br/><br/>National Security Perspective:<br/>1. Open-source models enable researchers and companies to identify and address potential vulnerabilities and security risks associated with AI systems, enhancing overall cybersecurity.<br/>2. Unrestricted access to these models allows for the development of robust defensive mechanisms against adversarial attacks and the misuse of AI technologies by malicious actors.<br/>3. Open-source models can be leveraged to develop AI solutions for national security applications, such as intelligence analysis, cybersecurity, and defense systems, without relying solely on proprietary models controlled by a few entities.<br/><br/>While it is essential to address legitimate concerns regarding the potential misuse of AI technologies, overly restrictive policies on open-source models could stifle innovation, undermine scientific progress, and compromise the United States&#39; leadership in the field of AI. A balanced approach that promotes responsible development and use of AI while preserving the benefits of open-source models is crucial for maintaining a competitive edge in AI and ensuring national security.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0201,
comment,2024-04-03T14:06:18Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0267,,,Anonymous,Comment from Anonymous,"My own two cents, written in the bullet-point comparison format:<br/><br/>Pros of open model weights:<br/> - Freedom from the rules and whims of any individual, group or organisation, with the end user being able to fine-tune the model for themselves in a way that can allow them access to greater range of potential applications. The Gemini incident showed us that open models can be also used to bypass corporate censorship, be it advertent or not;<br/> - A release of a model with a permissive license can be thought of as setting up a &quot;floor&quot; on the capability level, increasing overall quality of services the market can provide;<br/> - Some applications can be made available only with open models, like the possibility of finetuning the model on everything one ever wrote or spoken (avoiding big problems with privacy otherwise), which can then be used for creating a &quot;digital copy&quot; of you that knows you extremely well and can be much more targeted in its potential help (you can count this as &quot;empowering individuals&quot; too). I haven&#39;t seen using or abusing this on any significant scale so far;<br/>- Increase of academical research speed, with an ability to directly evaluate and modify weights instead of treating them as a &quot;black box&quot; (more than they usually are);<br/><br/>Contras of open model weights:<br/> - Current open models are too dumb to cause any real, serious harm to humanity beyond (familiar and thus possible to adjust and defend from) misinformation campaigns and automated scam attempts, but it sets up a dangerous precedent - if we will continue to opensource models of basically equal capability as state-of-the-art with ~ 1.5 - 2 year lag, eventually we will open-source a model that can be used to e.g. assist in or fully automate bioweapon or malware creation, ultimately culminating in existential risk. I&#39;d generally like to have more public discussion (and of better quality) on existential risks from AGI;<br/> - By allowing open models, competing&amp;unfriendly to US states and non-state groups can just use those models instead of trying to set up their own much more expensive training-from-scratch infrastructure (as compared to finetune and inference compute);<br/> - You cannot recall open weights. Same as with any piece of software that is wanted by someone and (possibly illegally) available on the web, determined people will pirate the weights and still be able to use them;<br/> - As mentioned in the document, you can use the research on open models to mount an attack against closed ones.<br/><br/>From my limited experience, open models are currently used (besides academic research) mostly in lightweight small-scale applications, as local coding assistants and as local roleplay partners (the latter is at least half about nsfw). My own estimation is that besides private exchanges/roleplay use cases, open models are currently lagging behind SotA (as defined by claude 3 opus, gpt-4 and gemini ultra, but not free chatgpt) about 1.5 years or so, but this can change at any moment.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0267,
comment,2024-04-03T14:04:18Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0048,,,Trammell,"Comment from Trammell, Brandon",Banning open ai models will set the US back extremely in the technology race moving forward if citizens can not collaborate on ai models openly. Some of the biggest changes in the tech industry have from open source projects where any person with a passion can help contribute to moving that project forward. This will not stop the progress of ai it will just move potential businesses out of the US.,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0048,
comment,2024-04-03T14:04:20Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0052,,,Miller,"Comment from Miller, Paul","I&#39;ve been a software engineer for almost 15 years now, currently Director of UI Engineering. I play around with neural networks and their applications in my spare time. This open model weights proposition would effectively halt that hobby, stymie development and technological progress, and put us behind our adversaries in these incredibly crucial technological advancements. It will also create a barrier of entry for new AI technology-based businesses to compete, ultimately giving the first-to-marketing businesses a huge advantage and locking in their market positions.<br/><br/>I believe that we should not be seeking to regulate this technology so much as we should seek enforcement for misuse of it. Set guidelines, or ultimately laws, for what is and is not okay to use AI for and go after those who break those rules.<br/><br/>The train has already left the station. When it comes down to it, AI development is not going to stop. If we don&#39;t do it, another country will. Those same models will find their way to users in the United States regardless of these regulations. I&#39;d much rather we lead the way rather than cede the most important technological advances in decades to other countries.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0052,
comment,2024-04-03T14:04:21Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0054,,,Anonymous,Comment from Anonymous,"Banning open weight models would basically cement the power of corporations over individual people. AIs are going to get more and more important in many different areas of life, and if people aren&#39;t allowed to own them locally on their own devices, then a large part of their lives would be literally owned by corporations.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0054,
comment,2024-04-03T14:04:09Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0031,,,Anonymous,Comment from Anonymous,"Prohibiting the full and open release of large machine learning models will effectively halt the scientific progress in this area, which is largely build on open research. Big AI companies try to use the fear we get from fiction regarding the topic to initiate regulations that hinder open approaches and help their short ther profits. If we look back to the release of GPT2 a few years ago, open air stated the the model was too dangerous to be released, which is quite ridiculous from today&#39;s perspective. It seems to be the case that they use the fear also for marketing purposes to gain publicity. Also people might use all kinds of tools for bad purposes. And nobody would ban e.g. Photoshop even though it is a photo manipulation software. Analog to the weapon legislation, where many people state that guns do not kill people, people kill people, it&#39;s not the tool that&#39;s inherently bad even if people might use it for e.g. illegal purposes. Also nobody would ban a hammer, which is a very useful tool only because you can hurt somebody with it. Same thing for a car. A car can be a weapon in the wrong hands, but nobody would doubt it&#39;s usefulness in many situations. Sometimes we need to trust in the people. These tools make certain things easier, which accelerates productiv tasks, but it also can e.g. the number of hate comments one might write in a given time. This being said if we look at it as a tool and not with the lens that fiction and the buzzword apply to us many of these regulations seem ridiculous.<br/><br/>I am a researcher in the field of machine learning for some time and am quite concerned that good intended regulation might shut down the open research of universities and individuals.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0031,
comment,2024-04-03T14:05:49Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0212,,,Rabe,"Comment from Rabe, Brandon","As AI technology grows in both power and popularity, the question of regulation is a natural one.  However, as an information technology, any attempt to restrict the production and distribution of AI models is ultimately an attempt to restrict and control the sharing of information.  These models are only ones and zeros on a hard drive, and can be freely shared infinitely by anyone with access to a computer and the internet. Restricting these models based on any quantitative factor such as file size, training computation, or distribution scale, is like trying to cut the head from a hydra. The technology can easily be redesigned to morph around these limitations, maintaining the constant pace of evolution while still fitting within the letter of the law. Reactive adjustments to the law will always be a step behind the cutting edge of the machine learning community.<br/><br/>I believe that such quantitative restrictions miss the mark, and will not truly provide the safety that regulation seeks to ensure.  The raw size, power, and distribution of a model are not what make it inherently dangerous. The information that a model is trained on is the primary determining factor in how a model will behave. For example, a model that is trained on a dataset that includes instructions on the production of dangerous weapons, or other nefarious acts such as hacking and scamming, will be a dangerous model. This is true regardless of model size and intelligence; even a relatively small model that isn&rsquo;t powerful or state-of-the-art can be dangerous when trained in such a way. Conversely, a very large and powerful model that has been trained on a well curated dataset, with careful attention paid to remove &ldquo;dangerous&rdquo; instructional information, will not be a dangerous model.<br/><br/>As training datasets are often hundreds of billions of words long, any attempt to regulate the content of training datasets seems to be a fool&rsquo;s errand. As technology progresses, the computational and skill requirements will continue to decrease, and the ability to train machine learning models will become easier to the point of triviality. Eventually anyone will be able to train their own models at home. We are already to the point that skilled programmers can do this right now. AI models cannot realistically be regulated by limiting their size, computational power, or distribution, and any attempt to do so will only retard the growth of the USA as this revolutionary technology changes the world around us.<br/>Models can only be judged and regulated after the fact, based on their actions. If a user of a specific model has been shown to cause illegal harm with a model, then they can be punished or sanctioned within the framework of our existing law, based on the crimes that they have committed. Then once the model has been demonstrated to be dangerous, it can be regulated on a case by case basis. Restricting models before they have been produced, based on arbitrary quantitative limits to size, power, and distribution, will only restrict the technological development of the country, without providing any real additional safety.<br/><br/>In conclusion, while the impulse to regulate AI models is understandable given their increasing power and potential for misuse, the proposed regulations based on quantitative factors such as size, computational power, and distribution are misguided. These restrictions would be ineffective in practice, easy to circumvent, and would only serve to hinder the United States&#39; progress in this transformative field. Instead, we should focus on promoting responsible AI development practices, particularly in the curation of training datasets, and address harmful applications of AI models through our existing legal framework on a case-by-case basis. By taking a more targeted and adaptive approach to AI regulation, we can foster innovation while still protecting society from the potential risks of this technology. Preemptive quantitative restrictions, however well-intentioned, are not the answer.<br/>",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0212,
comment,2024-04-03T14:05:51Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0216,,,Parada,"Comment from Parada, Carlos","I am reaching out to share my deep concerns about the open-source distribution of large language model weights and the potential implications for national security and the safety of our global community.<br/><br/>The advent of large language models represents a significant leap forward for artificial intelligence. In 2020, researchers estimated it would take until 2050 for an AI to achieve an above-average test score on standard college admissions tests. GPT-4 achieved this goal in March 2023. The breakneck pace of these advances has forced us to confront the possibility that we will develop smarter-than-human intelligence within the next decade. When we complete such a superintelligent artificial intelligence, there is a significant risk that such a model will pursue goals misaligned with our own and that we will not be able to stop it. Several of these concerns<br/><br/>Unrestricted access to model weights can lead to exploitation by foreign adversaries. Open-sourcing these model weights provides them directly to hostile nations such as China and Russia, who are reliant on American engineering to provide these models directly to them. The ability of these models to generate convincing text can be weaponized to create sophisticated disinformation campaigns, manipulate public opinion, or carry out fraudulent activities, all of which can have destabilizing effects on our society.<br/><br/>Beyond immediate threats, the existential risks these models pose for humanity over the next decade cannot be overlooked. The rapid advancement and proliferation of AI capabilities could lead to scenarios where autonomous systems operate outside the scope of human values and ethics, potentially leading to unforeseen and irreversible consequences.<br/><br/>Half of AI researchers believe there is a 10% or greater chance that the invention of artificial superintelligence will mean the end of humanity. Among AI safety researchers, the average estimate is roughly 30%. Professors Geoffrey Hinton Yoshua Bengio, the Turing laureates known as the fathers of deep learning, have sounded the alarm about these risks, which have become widely acknowledged in the scientific community and have led to deep concern within both the Biden administration and Congress.<br/><br/>In light of these considerations, I urge regulatory bodies to take a proactive stance in establishing a framework that governs the distribution and use of large language model weights. This framework should prioritize the protection of our national interests and the safeguarding of human welfare above all else.<br/><br/>It is essential that we engage in a collaborative effort to ensure that the development of AI technologies aligns with our collective security and ethical standards. The decisions we make today will have lasting impacts on the trajectory of AI and its role in our society.<br/><br/>I appreciate your attention to this critical matter and look forward to the implementation of thoughtful and robust regulations that address these pressing concerns.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0216,
comment,2024-04-03T14:05:12Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0143,,,Mittal,"Comment from Mittal, Pranay","I think we should allow for regulations to restrict the release of open source model weights, especially on general-purpose models. I worry deeply about the fact that safety regulations can be removed easily with fine-tuning from open model weights (see https://arxiv.org/abs/2310.20624). I think general-purpose AI could be extremely dangerous (especially if in the hands of the PRC, but also in the hands of bad actors domestically). Please restrict the release of open source weights.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0143,
comment,2024-04-03T14:05:38Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0193,,,Anonymous,Comment from Anonymous,"While there are detailed technical responses to be made to the questions posed in RFC NTIA-2023-0009, the core issues are political and economic.  <br/>Who do we want to be as Americans, and how do we want to encourage and reward economic competition in the marketplace?<br/>Though I lead AI developments as an executive and have products, publications, and patents in AI and neural nets stretching over 25 years back to my Ph.D. thesis, in this response (pdf attached) I focus on these core issues rather than the technical ones.   Regulation of openly-available weights for foundation models is directly antithetical to core American values of free speech, privacy, and economic innovation.","[('comment', 'https://downloads.regulations.gov/NTIA-2023-0009-0193/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0193,
comment,2024-04-03T14:06:05Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0242,Access Now,,,Comment from Access Now,See attached file(s),"[('Access Now Comments to NTIA on open foundation models _ March 27_2024', 'https://downloads.regulations.gov/NTIA-2023-0009-0242/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0242,"Access Now submitted comments to the NTIA regarding dual-use foundation artificial intelligence models with widely available weights. They emphasized the importance of researching the risks and benefits of open AI models, highlighting concerns about 'Big Tech' dominance, human rights safeguards, and resource-friendly development. The submission discussed the debate between open and closed models, the control over AI infrastructure by major companies, and the implications for marginalized communities and the Global South. Access Now urged for transparency on environmental impacts and labor conditions in AI model development."
comment,2024-04-03T14:06:16Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0264,GitHub,,,Comment from GitHub,See attached file.,"[('GitHub NTIA-2023-0009', 'https://downloads.regulations.gov/NTIA-2023-0009-0264/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0264,"GitHub supports the NTIA's consultation on widely available model weights to inform the implementation of Executive Order 14110 on AI development. Open source models are considered a public good with economic value and benefits to digital modernization. Widely available model weights provide similar benefits to AI development as open source software. The proposal emphasizes the importance of evaluating AI systems as a whole, focusing on risks of reckless and malicious use, societal resilience, and the benefits of open models in research and government use. Openly available evaluation suites and research directions are recommended to manage risks and amplify benefits. Legal and business issues related to open foundation models are compared to the ecosystem of open source software. License terms enabling wide reach and societal benefit from open source software are highlighted as lessons for open AI models."
comment,2024-04-03T14:06:23Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0277,Microsoft Corporation,,,Comment from Microsoft Corporation,See attached file(s),"[('20240326 Microsoft Response to NTIA Dual-Use Foundation Models with Widely Available Weights RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0277/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0277,"Microsoft's submission to the NTIA's request for comment on dual-use foundation artificial intelligence models emphasizes the importance of responsible AI development. They recommend clear definitions for open source AI models, risk-based frameworks for release, and global collaboration on AI research. Microsoft highlights the benefits of open foundation models for innovation and security, stressing the need for international standards. They suggest using Federal purchasing power to incentivize safety and security in AI models and engaging with open source communities for feedback. Microsoft also recommends ongoing research, evaluation, and training on AI safety and security."
comment,2024-04-03T14:05:45Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0205,,,Barrett,"Comment from Barrett, Anthony","We are researchers affiliated with The University of California, Berkeley, with expertise related to AI development, safety, security, policy, and ethics. We previously submitted responses to NTIA last year on AI Accountability (https://medium.com/cltc-bulletin/response-to-ntia-request-for-comments-on-ai-accountability-policy-c2aca7e4285c), and to NIST several times over the past three years at various stages of NIST&rsquo;s development of the AI Risk Management Framework (AI RMF).<br/><br/>The debate about &ldquo;open or closed&rdquo; foundation models has become contentious in policy and technical communities, but there are middle ground approaches that can help to balance the benefits of openness with the risks from the proliferation of unsecured dual-use foundation models. We emphasize these approaches in our response. NTIA has used the term &ldquo;open foundation models&quot; as a shorthand for the more specific term used in the White House Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, &ldquo;Dual-Use Foundation Models with Widely Available Model Weights.&rdquo; However, we note the term &ldquo;open&rdquo; is often conflated with &ldquo;open-source&rdquo; and can overstate the true transparency of a release strategy while creating a sense of a false binary. In practice there is a spectrum of release methodologies between &ldquo;open&rdquo; and &ldquo;closed&rdquo;. Throughout our response, we typically use the terms &ldquo;unsecured&rdquo; and &ldquo;secured&rdquo; instead of &ldquo;open&rdquo; and &ldquo;closed&rdquo; respectively, to refer specifically to the question of whether model weights have been made widely available (e.g. downloadable on a public repository).<br/><br/>Here are some of our key comments and recommendations on the NTIA Openness in Artificial Intelligence Models RFC:<br/>AI models with widely available weights, or unsecured models, can provide important benefits such as enhanced privacy for intended model users, easier auditability, as well as a more widely accessible research and innovation ecosystem. <br/>However, unsecured models also pose risks, such as various forms of malicious misuse resulting in harm, including to people&rsquo;s rights and wellbeing and to the safety of the general public. Although both closed and open models can pose some such risks, unsecured models pose unique risks in that safety and ethical safeguards that were implemented by developers can be removed relatively easily from models with widely available weights (e.g., via fine tuning).<br/>In addition to investing in upstream protections to prevent a range of misuses of unsecured foundation models, we should invest in downstream protections to prevent specific misuses. However, we should not rely only on downstream protections. <br/>As part of managing the risks of unsecured model release without preventing the benefits, we recommend: Foundation model developers that plan to provide downloadable, fully open, or open source access to their models should first use a staged-release approach (e.g., not releasing parameter weights until after an initial secured or structured access release where no substantial risks or harms have emerged over a sufficient time period), and should not proceed to a final step of releasing model parameter weights until a sufficient level of confidence in risk management has been established, including for safety risks and risks of misuse and abuse. The largest-scale or most capable models (including dual-use foundation models as defined in Executive Order 14110) should be given the greatest duration and depth of pre-release evaluations, as they are the most likely to have dangerous capabilities or vulnerabilities that can take some time to discover.<br/>We also recommend openness mechanisms that do not require making a model&rsquo;s parameter weights downloadable. Many of the benefits of open-source systems, such as review and evaluation from a broader set of stakeholders, can be supported through transparency, engagement, and other openness mechanisms besides releasing model parameter weights. For example, independent researchers should be able to test and audit secured models more readily and with the protections of a legal safe harbor. <br/> <br/>In our complete submission, we provide more detail and additional comments. <br/><br/>Views we express here in this piece are our own, and do not necessarily reflect the views of our employers or funders.<br/><br/>Thank you again for the opportunity to comment on the NTIA Openness in AI RFC. If you need additional information or would like to discuss further, please contact Anthony Barrett at anthony.barrett@berkeley.edu or Jessica Newman at jessica.newman@berkeley.edu. In any case, we look forward to further engagement with NTIA as you and others consider how best to approach the costs and benefits of AI openness.<br/><br/>Our best,<br/><br/>Anthony Barrett, Ph.D., PMP<br/>Visiting Scholar, AI Security Initiative, Center for Long-Term Cybersecurity, UC Berkeley<br/>(see complete submission for more)<br/>","[('NTIA 2024 AI Openness RFC Berkeley Response', 'https://downloads.regulations.gov/NTIA-2023-0009-0205/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0205,
comment,2024-04-03T14:06:14Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0261,Computing Community Consortium,,,Comment from Computing Community Consortium,See attached file(s),"[('CCC Response to the NTIA RFI', 'https://downloads.regulations.gov/NTIA-2023-0009-0261/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0261,"The Computing Research Association's Computing Community Consortium (CCC) believes that risks associated with widely available model weights for generative models are not significantly exacerbated compared to non-public model weights. They suggest regularly revisiting the issue and potentially releasing only a portion of model weights to mitigate risks. They emphasize the benefits of democratizing foundation models to promote economic innovation and diverse research, education, and societal benefits. The CCC also highlights the need for more research and diverse perspectives to manage safety-related and technical issues surrounding dual-use foundation models with widely available model weights."
comment,2024-04-03T14:06:12Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0258,,,Withers,"Comment from Withers, Caleb","Please see the attached file.<br/>Thank you,<br/>Caleb Withers<br/>Center for a New American Security<br/>","[('CNAS - NTIA Open Weights response', 'https://downloads.regulations.gov/NTIA-2023-0009-0258/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0258,
comment,2024-04-03T14:05:55Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0224,Center for AI Policy,,,Comment from Center for AI Policy ,See attached file(s),"[('NTIA Response on Open Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0224/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0224,"The submission by Thomas Larsen from the Center for AI Policy highlights the debate around open-sourcing AI and its impact on society. While open development is beneficial for most AI models, releasing highly capable AI model weights could pose national security risks, particularly regarding WMD-like AI and AGI. The submission recommends better monitoring to predict advanced AI development and prevent high-risk AI models from being open-sourced. It also discusses the benefits and risks of widely available model weights and proposes policy recommendations to mitigate threats, emphasizing the need for capability-based safety standards."
comment,2024-04-03T14:05:31Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0180,IBM,,,Comment from IBM,See attached file(s),"[('NTIA Open Model Weights RFC - IBM', 'https://downloads.regulations.gov/NTIA-2023-0009-0180/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0180,"IBM, in response to the NTIA's request for comment on dual-use foundation artificial intelligence models with widely available model weights, emphasizes the importance of following science in regulating AI safety risks, working with stakeholders to develop precise regulatory interventions, and preserving the benefits of open innovation ecosystems. IBM advocates for maintaining an open innovation ecosystem for AI to enhance U.S. leadership in the field. They highlight the lack of robust scientific evidence justifying restrictions on model weights and propose collaborative approaches with NIST for regulatory sandbox development. IBM stresses the economic and social benefits of openness, calling for a cautious and evidence-based approach to regulation."
comment,2024-04-03T14:05:57Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0227,,,Cohen,"Comment from Cohen, Michael",Please see the attached report.,"[('Cohen_Osborne_Open_Source_Comment', 'https://downloads.regulations.gov/NTIA-2023-0009-0227/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0227,
comment,2024-04-03T14:05:34Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0185,,,Anonymous,Comment from Anonymous,See attached file(s). It gives some technical information as well as expresses personal opinion.<br/>Table of Content:<br/>Section I - Meta-comments on the state of human society with respect to AI and current political landscape of opinion with respect to AI<br/>Section IIA - Explaining our current understanding of LLM<br/>Section IIB - More misc. remarks and the future of AI<br/>Section IIIA - Analyzing the questions posed by NITA<br/>Section IIIB - Summary of position and policy recommendation<br/>Section IV Conlusion and Wrapping up,"[('ntia_oss_ai_comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0185/attachment_1.txt')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0185,
comment,2024-04-03T14:06:05Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0243,Software & Information Industry Association (SIIA),,,Comment from Software & Information Industry Association (SIIA),Please see attached comments from the Software &amp; Information Industry Association (SIIA).,"[('SIIA Response to NTIA on AI Open Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0243/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0243,
comment,2024-04-03T14:05:03Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0128,Holistic AI Inc.,,,Comment from Holistic AI Inc. ,"Please find attached Holistic AI Inc.&#39;s response to the NTIA&#39;s RFC on Request for Comments on the potential risks, benefits, other implications, and appropriate policy and regulatory approaches to Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights. <br/><br/>Regards, <br/>The Public Policy Team, Holistic AI Inc.","[('Holistic AI Response to NTIA RFC on Open Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0128/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0128,
comment,2024-04-03T14:06:28Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0286,Pryon Inc.,,,Comment from Pryon Inc.,See attached file(s),"[('Pryon NTIA Submission 032724 E', 'https://downloads.regulations.gov/NTIA-2023-0009-0286/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0286,
comment,2024-04-03T14:06:11Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0256,,,Bommasani,"Comment from Bommasani, Rishi",See attached file,"[('Response to NTIA RFC Open Foundation Models', 'https://downloads.regulations.gov/NTIA-2023-0009-0256/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0256,
comment,2024-04-03T14:06:08Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0249,RAND,,,Comment from RAND,See attached file(s),"[('RAND Response to NTIA RFC on AI Model Weights', 'https://downloads.regulations.gov/NTIA-2023-0009-0249/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0249,
comment,2024-04-03T14:05:48Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0210,International Center for Law & Economics,,,Comment from International Center for Law & Economics,See attached file(s),"[('ICLE - NTIA COMMENTS RFC Open Foundation Models 2024', 'https://downloads.regulations.gov/NTIA-2023-0009-0210/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0210,"The International Center for Law & Economics emphasizes the importance of fostering innovation and responsible production of AI in response to the NTIA's request for comments on Dual Use Foundation Artificial Intelligence Models. They caution against overly cautious regulation that could stifle AI evolution and suggest a balanced approach that maximizes social benefits while managing risks effectively. They critique the broad definition of ""dual-use"" models in the Executive Order, warning that it could hinder open-source AI development and international collaboration. The Center advocates for a flexible regulatory framework that differentiates between low-risk and high-risk AI applications to encourage innovation while ensuring safety and ethical integrity."
comment,2024-04-03T14:04:38Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0083,,,van der Linden,"Comment from van der Linden, Henk","While I am not a US citizen I have experience in providing open source AI software and models to end users, trough this comment I hope to provide answers to the questions raised. I would like to emphasize that I consider AI and even a foundational model very broad definitions and this can lead to the destruction of the research and hobby space if done imprecisely.<br/><br/>I&#39;d like to advocate that any AI legislation makes a clear distinction between derivative works and new pre-trained models. As well as trying to limit the scope based on the intended use of a model. For example: a model that is meant to produce fictional literature requires different legislation than a model that can be used for autonomous weaponry. If a hobbyist is fine-tuning an existing model with low risk data you can commonly find on the internet or data of a fictional nature this should not be held to the same governance standards as someone producing a model that can be used in an autonomous weapons system. And the best approach to differentiate this is on a dataset level (or functional level for non-generative models). To protect researchers and hobbyists who produce open models you should only hold them accountable for the data they are adding to the model, not unintended use or consequences they could not foresee and no longer remedy after the fact.<br/><br/>Attached is a more detailed answer to the questions, while I understand I have no direct say on US matters as a non-citizen, I do hope my insight provides value in your legislative efforts. ","[('QuestionsAnswered', 'https://downloads.regulations.gov/NTIA-2023-0009-0083/attachment_1.txt')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0083,
comment,2024-04-03T14:05:56Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0225,The Center for Security and Emerging Technology (CSET),,,Comment from The Center for Security and Emerging Technology (CSET),See attached file(s),"[('CSET Response to NTIA RFC (89 FR 14059)', 'https://downloads.regulations.gov/NTIA-2023-0009-0225/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0225,"The Center for Security and Emerging Technology (CSET) at Georgetown University responded to the NTIA's Request for Comment on Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights. They addressed topics like defining ""open"" models, risks, benefits, managing risks, governance mechanisms, and planning for the future. CSET suggested defining open models by the level of access to weights, discussing the importance of downloadable weights, and evaluating risks associated with widely available model weights. They also highlighted the benefits of open model weights for competition, innovation, scientific research, and education. CSET emphasized the need for model evaluations, mitigation assessments, uncertainty evaluations, and safeguards to manage risks associated with widely available model weights. They also discussed ways to regain control over model weights and the components necessary for analyzing, evaluating, certifying, or red-teaming the model."
comment,2024-04-03T14:05:25Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0168,,,Smith,"Comment from Smith, Colby","Please find attached a .docx (NTIA-2023-0009-0001.docx) detailing my comments on document NTIA-2023-0009-0001. Each page of the document deals with a different question/subsection of the document (page 1 deals with question 1, page 2 deals with question 1a, etc.), with the question in italics.<br/><br/>We were only able to respond to a few questions due to time constraints, but we hope what we have written aids the NTIA in decision making.","[('NTIA-2023-0009-0001', 'https://downloads.regulations.gov/NTIA-2023-0009-0168/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0168,
comment,2024-04-03T14:06:09Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0252,"AH Capital Management, L.L.C. (""a16z"")",,,"Comment from AH Capital Management, L.L.C. (""a16z"")","AH Capital Management, L.L.C. welcomes the opportunity to provide comments in response to the RFC.  Our comments are presented in the attached document for your consideration.","[('a16z NTIA RFC Comment Letter 3-27-24-2', 'https://downloads.regulations.gov/NTIA-2023-0009-0252/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0252,"AH Capital Management, L.L.C. (""a16z""), a leading venture capital firm with significant investments in AI, supports the NTIA's Request for Comment on Dual Use Foundation Models. They emphasize the benefits of Open Models, highlighting how they enhance innovation, reduce barriers to entry, increase competition, and promote transparency to mitigate risks such as bias. a16z advocates for inclusive definitions of ""open"" and ""widely available"" to maximize the benefits of Open Models. They stress that Open Models should compete freely with Closed Models, urging NTIA to adopt rules that foster their development and avoid favoritism."
comment,2024-04-03T18:20:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0332,AE Studio,,Rosenblatt,"An open letter from Judd Rosenblatt, Founder and CEO of AE Studio, on the crucial role of open-source models in advancing AI alignment research (see attached PDF)",See Attached,"[('Judd Rosenblatt--Response to NTIA Solicitation for Comments on Open-Weight AI Models', 'https://downloads.regulations.gov/NTIA-2023-0009-0332/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0332,"Judd Rosenblatt, CEO of AE Studio, emphasizes the importance of open foundation AI models in advancing AI alignment efforts. He warns against rushing to impose restrictions on open-source models, as it may hinder alignment research and consolidate control in large tech companies. Rosenblatt advocates for a nuanced approach, balancing the benefits of open-sourcing with the risks of security threats. He calls for lawmakers to establish guidelines for open-source model development and increase funding for AI safety research to ensure technological progress aligns with human values and ethics."
comment,2024-04-03T18:23:30Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0263,"American Civil Liberties Union, Center for American Progress, and the Leadership Conference on Civil and Human Rights",,,"Comment from American Civil Liberties Union, Center for American Progress, and the Leadership Conference on Civil and Human Rights ","See attached comments by the American Civil Liberties Union, the Center for American Progress, and the Leadership Conference on Civil and Human Rights. <br/>","[('2024.03.27 - Coalition NTIA Comments v4 - Updated', 'https://downloads.regulations.gov/NTIA-2023-0009-0263/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0263,"The submission to the NTIA regarding Dual Use Foundation Artificial Intelligence Models emphasizes the importance of openness in AI to advance transparency, civil rights, and safety. It highlights how AI, both advanced and rudimentary, impacts various sectors like housing, employment, credit, healthcare, education, child welfare, insurance, and the criminal legal system. The document stresses the need for regulatory measures like auditing, transparency, and explainability to protect civil rights. It also discusses the potential benefits of ""open"" AI in mitigating market concentration, democratizing AI benefits, and increasing competition in the AI market."
comment,2024-04-03T17:57:03Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0316,,,Warren,"Comment from Warren, Neil",Open-sourcing AI model weights drastically increases the chance of everyone on Earth dying.,"[('Comment on NTIA - Google Docs', 'https://downloads.regulations.gov/NTIA-2023-0009-0316/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0316,
comment,2024-04-03T17:57:04Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0318,,,Weinberg,"Comment from Weinberg, Michael",See attached file(s),"[('WeinbergNTIAOpenModelComment20240327', 'https://downloads.regulations.gov/NTIA-2023-0009-0318/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0318,
comment,2024-04-03T16:45:46Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0292,,,Anonymous,Comment from Anonymous,"Hello,<br/><br/>I do not think that anyone should be able to limit models because they are necessary for the advancement of the sector. It brings a lot of innovation for the next generation to build upon. Please do not limit access to these tools.<br/><br/>Thanks",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0292,
comment,2024-04-03T17:56:51Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0294,,,DeVentura,"Comment from DeVentura, Dominick",Do not regulate ai models. Keep it open and keep it free. ,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0294,
comment,2024-04-03T17:56:54Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0300,,,Gutierrez,"Comment from Gutierrez, Raul","Open weight models have sparked a wave of innovation that have led to waves of innovation that will foster rapid advances in a wide range of fields. Even referring to these models as dual use seems wrongheaded. These models/weights will be the foundation a new economy. Restricting them enhances entrenched corporate power at the expense of entrepreneurs, innovators, and individuals.  As an example open models have allowed our small 6 person startup to inexpensively build products that would have been unimaginable a few years ago and that will have a global reach.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0300,
comment,2024-04-03T17:57:09Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0330,,,Anonymous,Comment from Anonymous,Blocking people from training models would only cripple the US in an AI race with china. In addition even if the bill passes it&rsquo;s likely that criminals and bad actors will still find a way to get access to high powered models,[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0330,
comment,2024-04-03T17:56:56Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0304,,,Anonymous,Comment from Anonymous,"Open weight models are the future of human knowledge, think about how many people struggle with harsh judgments from society due to biological attributes like height, facial structure, or mental conditions, and the impact it must have on their mental stability. Open models often help these people when they communicate with them, make friends with them, which improves their mental stability. Governments may not help them with their mental stability, but at least with open models, they can help themselves.<br/><br/>And not only that, I learned programming from my open model, my Python learning has been completely reliant on my local model that I run on my PC.<br/><br/>And not only that, so many things I have been learning, now I&#39;ve started to understand scientific papers because I tell the model to summarize and explain them in simple terms.<br/><br/>These models never get frustrated like human teachers when we ask them questions 500 times, we ask them to explain in different ways.<br/><br/>It&#39;s one of the only options for a true personalized learning experience that no human teachers or education system can provide.<br/><br/>To many people, the only good communication experience they have is with their open model, who becomes their friend, their assistant, their partner.<br/><br/>It&#39;s simple - humans will never change their behavior for the sake of others, but open models can change their behavior however you like, with a good system prompt being all it takes.<br/><br/>They have many positive aspects and can provide people with their primary source of mental stability, learning experiences, and even assistance with productive tasks.<br/><br/>And the negative aspects of open models are not true negatives of open models; they are a problem of people, humans. If we were to misuse anything, the focus should be on improving the mental stability of people, addressing societal factors that make people mentally unstable and more likely to misuse anything. It could well be that the pressure to compete for money and resources is the true source of most mental instability.<br/><br/>If people, on average, are happier with their societal experiences, they would never misuse anything. Stop targeting technology; truly focus on improving the mental health of people, and it will fix most of the problems with misuse or general instability, such as crimes.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0304,
comment,2024-04-03T17:57:00Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0312,,,Gowthaman,"Comment from Gowthaman, Subramanian","Advocating for open sourcing Large Language Models (LLMs) is crucial for ensuring equitable access to advanced AI technologies and fostering transparency and accountability in their development and application. One compelling argument in favor of open sourcing LLMs lies in the potential dangers associated with proprietary, closed-source models, particularly in the hands of large corporations.<br/><br/>Closed-source LLMs grant unchecked authority to corporations, allowing them to dictate what information is prioritized and shaping societal narratives. This concentration of power resembles the philosophy of a dictator, stifling diversity of thought and expression. Conversely, open sourcing LLMs promotes democratization by making the underlying code accessible, enabling collaborative scrutiny, bias identification, and improvement proposals.<br/><br/>Moreover, open sourcing LLMs fosters innovation, knowledge sharing, and inclusivity within the AI community. By embracing openness and collaboration, we can ensure that AI technologies serve the collective interests of humanity rather than the agendas of a privileged few.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0312,
comment,2024-04-03T17:56:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0309,,,Wagenhoffer,"Comment from Wagenhoffer, Dane","Do not limit open weight models, it is imperative they remain open to encourage competition ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0309,
comment,2024-04-03T17:57:02Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0315,,,Huang,"Comment from Huang, Michael","9. What other issues, topics, or adjacent technological advancements should we consider when analyzing risks and benefits of dual-use foundation models with widely available model weights?<br/><br/>It is important to take a step back and consider the big picture.<br/><br/>Homo sapiens has been around for a few hundred thousand years. In all that time, we have been the most intelligent species on Earth. There are many species that are stronger or faster; there are none that are more intelligent.<br/><br/>Having the top rank has caused us to become complacent. We do not fear being overtaken.<br/><br/>There are three scenarios where humankind can be overtaken. The first is the arrival of an extraterrestrial intelligence, which seems very unlikely due to the Fermi paradox.<br/><br/>The second is a species on Earth that becomes more intelligent through evolution or modification. However, the species most likely to exceed human intelligence through these means would be humankind itself.<br/><br/>The third is the creation of artificial general intelligence (AGI). If one compares AI now, to what AI was like ten or fifty years ago, it appears that AGI is increasingly likely.<br/><br/>If AGI is created, then artificial superintelligence (ASI) may not be too far behind. Alan Turing&#39;s Bletchley colleague I. J. Good writes:<br/><br/>&quot;Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an &quot;intelligence explosion,&quot; and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.&quot;<br/>Irving John Good, &quot;Speculations Concerning the First Ultraintelligent Machine&quot;, 1965.<br/><br/>The problem of control was one that Alan Turing recognized:<br/><br/>&quot;...it seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler&#39;s Erewhon.&quot;<br/>Alan Turing, &quot;Intelligent Machinery, A Heretical Theory&quot;, 1951.<br/><br/>The central problem posed by the age of AI is how to keep it safe. What do we do with a powerful dual-use technology with enormous benefits and dangers?<br/><br/>The creation of artificial narrow intelligence (ANI) is a way of creating powerful non-dual-use technology.<br/><br/>The creation of artificial general intelligence (AGI) should be paused by law (and treaty) unless it is proven to be safe.<br/><br/>This position is supported by recent statements by Turing Award recipient Yoshua Bengio:<br/><br/>&quot;Strict regulatory requirements or bans on the development of highly advanced AIs known for the risk of emergent goals within an AI, such as reinforcement learning, until we have clear evidence of their safety.&quot;<br/>Yoshua Bengio, &quot;Written Testimony of Professor Yoshua Bengio, Presented before the U.S. Senate Judiciary Subcommittee on Privacy, Technology, and the Law&quot;, July 25, 2023, https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_bengio.pdf<br/><br/>&quot;My stance can be basically summarized as follows: until we know how to build the kind of safe and controllable systems that Yann [LeCun] and others (including me) would like to figure out, and until the scientific evidence is clear that such systems are indeed demonstrably safe, we should not build and deploy these systems (by law). This would be a powerful incentive for companies to invest a lot more in AI safety than they currently do. Obviously, such a safety precaution need not apply to 99% of AI systems (which are not very dangerous) but mostly to future high-power generalist systems (the so-called Frontier AI systems) beyond some level of computational resources (or other clearly measurable metric, when we know better). This is to avoid potential catastrophic misuse or loss of control (as already discussed at length by many others and myself).&quot;<br/>Yoshua Bengio, October 29, 2023, https://www.facebook.com/yoshua.bengio/posts/pfbid0KhttfB3UZEKTUMXoUZMPNxz1mQAduwXpC6V486QaBcwA1gxTCiuNoxPqdYkW8Frsl<br/><br/>On the question posed by the Request for Comments, dual-use foundation models should not be widely distributed or made widely available (by law) because of their inherent dual-use nature, and the irreversibility of their release.<br/><br/>Thank you.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0315,
comment,2024-04-03T17:57:09Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0329,,,Anonymous,Comment from Anonymous,"Advocating for open sourcing Large Language Models (LLMs) is crucial for ensuring equitable access to advanced AI technologies and fostering transparency and accountability in their development and application. One compelling argument in favor of open sourcing LLMs lies in the potential dangers associated with proprietary, closed-source models, particularly in the hands of large corporations.<br/><br/>Closed-source LLMs grant unchecked authority to corporations, allowing them to dictate what information is prioritized and shaping societal narratives. This concentration of power resembles the philosophy of a dictator, stifling diversity of thought and expression. Conversely, open sourcing LLMs promotes democratization by making the underlying code accessible, enabling collaborative scrutiny, bias identification, and improvement proposals.<br/><br/>Moreover, open sourcing LLMs fosters innovation, knowledge sharing, and inclusivity within the AI community. By embracing openness and collaboration, we can ensure that AI technologies serve the collective interests of humanity rather than the agendas of a privileged few.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0329,
comment,2024-04-03T17:56:52Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0296,,,T,"Comment from T, T","As a US citizen, I strongly encourage the committee to acknowledge the advantages of open model weights. These weights can serve as tools to enhance our lives, create economic opportunities, and prevent reliance on a single point of failure or influence controlled by a corporation&#39;s AI system API. This is a crucial free speech issue for our nation.<br/><br/>Companies developing closed-source foundational model weights are nearsighted in their regulatory capture strategy. They risk irrelevance as new, more powerful architectures running on efficient and low cost hardware are developed and accessible to more people. Already, OpenAI&#39;s dominance is being challenged by newer companies, and the technology is becoming commoditized. Therefore, these companies&#39; influence on public policy should be minimized.<br/><br/>Instead, the government should implement policies ensuring that full training data used for foundational/mass access closed models is accessible to the public. These companies benefit from knowledge created by US citizens and global populations and we should be able to examine what information is being used. Furthermore, alignment philosophy, methods, and biases should be transparent to prevent models from spreading ideologies or misinformation driven by company agendas.",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0296,
comment,2024-04-03T17:57:02Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0314,,,Beavers,"Comment from Beavers, Molly","I have been educating people through community study groups on Deep learning models for 5 years now. Open access to weights have allowed community members to fine tune these models on their own tasks. We have also been exploring ways to decrease the total of data needed to fine tune models for these individual use cases. <br/><br/>Though why do we need to fine tune these models? One reason is evasiveness. Language models from large corporations can be very safe, but tend to have an overly &quot;corporate&quot; voice. General every day tasks like how to build a fence can get responses suggesting to consult a structural engineer if you even remotely want to reinforce the fence against earthquakes. There are a lot of these issues that limit current models. <br/><br/>Another value of having these weight available is of course to educate the public. While open source models do not match the capabilities of closed source models they are close enough, so that we do have communities of people outside of large corporations that are familiar with the general tasks that go into building these models. If weights are not available I worry that less people would have knowledge of the inner workings of these models, such as what is an attention mechanism. ",[],https://api.regulations.gov/v4/comments/NTIA-2023-0009-0314,
comment,2024-04-03T14:05:58Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0229,Center for Democracy and Technology,,,Comment from Center for Democracy and Technology,See attached file(s),"[('CDT to NTIA comments on open foundation models 03272023', 'https://downloads.regulations.gov/NTIA-2023-0009-0229/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0229,
comment,2024-04-03T14:05:29Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0175,,,Foster,"Comment from Foster, Charles","[See attached file for responses to questions.]<br/>I am an AI/ML researcher who has been involved in the large language model (LLM) space for several years. Back in 2020, I was a volunteer within EleutherAI, a research community that was collecting datasets for LLMs, constructing evaluations for them, and training them in an open manner. Since then I have been in the private sector, working on applications in assessment &amp; education that rely on foundation models (both closed and open) as a base. I would consider myself very well-informed about the technology itself, and somewhat less so for the policy landscape surrounding it. I believe that foundation models will be of continuing importance, but I have been frustrated at the low quality of general discourse around the issue of their good governance.<br/><br/>Given that context, it felt appropriate to offer comments on this RFC. In the attached document, I&#39;ve tried to answer the questions posed in the cases where I felt I had something to contribute, and to indicate otherwise where I felt I didn&#39;t have anything to contribute.","[('NTIA Openness in AI RFC Comments - Charles Foster', 'https://downloads.regulations.gov/NTIA-2023-0009-0175/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0175,
comment,2024-04-03T14:06:17Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0265,Intel Corporation,,,Comment from Intel Corporation,Intel Corporation Comment Submission,"[('NTIA RFC Dual Use Open Models_March 2024_Intel', 'https://downloads.regulations.gov/NTIA-2023-0009-0265/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0265,
comment,2024-04-03T14:06:15Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0262,Hugging Face Inc.,,,Comment from Hugging Face Inc.,"Hugging Face applauds the ongoing work of the National Telecommunications and Information Administration (NTIA) in examining dual use foundation models (FMs). The following comments are informed by our experiences as an open platform for AI systems, working to make AI accessible and broadly available to researchers for responsible development.<br/><br/>In order to best address the risks and benefits of widely available model weights, we explore both the dimension of wide availability of a model and access to model weights in our responses to avoid conflating risks inherent in the technology with risks associated with release methods. We refer to models with widely available model weights as &ldquo;open-weight models&rdquo;. <br/><br/>Please find our answers to the RFC questions in the attached document.","[('HF NTIA open weights Response', 'https://downloads.regulations.gov/NTIA-2023-0009-0262/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0262,
comment,2024-04-03T14:06:23Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0278,,,Hartley,"Comment from Hartley, Mary-Anne","[Article 3b-e]<br/>Traditionally, censorship has worked by blocking access to information. Since the internet, an equally effective strategy has been to flood the signal with noise, essentially blocking access through complexity.<br/><br/>Foundational Artificial Intelligence models have the potential to compress enormous-scale complexity into a massively distributed, accessible conversational interface. Closing access to transparent community development of this powerful technology is a new type of censorship: blocking access to fair representation.<br/><br/>As a medical doctor and professor of data science specializing in digital humanitarian response and global health at Yale and EPFL, I have first-hand experience of the lifesaving power of access to relevant, representative information at the right time and place. Simply informing a pregnant woman in rural Africa that her fever and vaginal discharge are signs of the utmost urgency, can change the course of two lives. <br/><br/>A staggering percentage of the global population dies from preventable causes. It is a real-time, ongoing, completely preventable existential catastrophe that deserves prioritization and radical urgent technological innovation. <br/><br/>Open-source foundation models with widely available weights have allowed our academic collaborative to produce Meditron, currently the world&rsquo;s best-performing, open-source, open-access model for medicine. Thanks to the widely available weights of Meta&rsquo;s Llama-2, we were able to actively scrutinize the model, expose it to data from critically underrepresented populations and settings, and test hypothetical counterfactual scenarios of bias and harm.<br/>We have since released this model completely open-source (from data to weights) and it has been downloaded over 30K times within three months, with particular interest from NGOs and low-resource settings. Researchers in places ranging from Peru to the Gambia have reached out feeling empowered to better represent their communities in this foundational model by implementing low-cost finetuning techniques. <br/><br/>Training foundation models are extremely resource-intensive and disproportionately inaccessible to low-resource settings. Blocking access to weights of pre-trained foundation models forces these communities to &ldquo;retrain the wheel&rdquo; which not only encourages wasteful carbon-intensive computation but is also impossible. <br/><br/>The decision to censor widely available weights will ultimately determine the size of the technological gap between high and low-resource settings. <br/><br/>My group builds AI tools for the largest global-scale humanitarian organizations, which serve a significant percentage of the world&#39;s most vulnerable and underrepresented populations. Commercial and governmental entities, by definition, cannot fulfill the core humanitarian principles of neutrality and impartiality in these models.<br/>Academia and the open-source community serve a critical and unique role by representing a neutral space for the transparent validation of new technologies. This is especially important for technologies with the power to influence life-or-death decisions. This use case deserves access to the best, cutting-edge models and widely available weights. <br/><br/>Paraphrasing Benjamin Franklin, the only thing more expensive than information is ignorance.<br/><br/>All the above is stated with an acute awareness of the important risks of open foundation models. Generative models have the potential to facilitate the creation of propaganda and non-consensual intimate images as well as degrade artistic ownership, and power weapons of mass destruction. These risks make it even more important to ensure open-source access to a global community of concerned researchers to model countless such counterfactual scenarios of risk and harm specific to their context and create protective measures to detect and respond to this inevitable risk. The decision to release foundation model weights is an opportunity to democratize the development of protective measures against technological threats.<br/><br/>Foundation Artificial Intelligence models represent global knowledge and require global community-driven oversight. Weight censorship concentrates the power of foundation models in the hands of non-neutral actors with potential commercial or partisan interests, creating an imbalance of power between citizens and monopolies. <br/><br/>In conclusion, with the power of global-scale life-saving benefits (and preventing global-scale catastrophe) comes the responsibility to share it.<br/><br/>[Article 1c]  &ldquo;Widely available&rdquo; implies democratic/representative distribution. If the term intends to define a group of privileged access holders, the term &ldquo;widely&rdquo; is not descriptive. If a privileged circle of institutions is selected, who selects them and how is equity ensured in this selection? How would leakage be prevented/tracked to avoid rendering the proposed censorship useless? These questions do likely not have equitable or realistic solutions.<br/><br/>","[('Nature2024_MEDITRON_Open_Medical_Foundation_Models_Adapted_for_Clinical_Practice', 'https://downloads.regulations.gov/NTIA-2023-0009-0278/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0278,
comment,2024-04-03T14:06:13Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0259,Machine Intelligence Research Institute,,,Comment from Machine Intelligence Research Institute,Please see the attached file for our response to the NTIA AI Open Model Weights RFC. ,"[('MIRI_NTIA_RFC_response', 'https://downloads.regulations.gov/NTIA-2023-0009-0259/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0259,
comment,2024-04-03T14:05:24Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0165,Reason Foundation,,,Comment from Reason Foundation,"Comment from Reason Foundation attached as PDF<br/>Authors: Max Gulker, Spence Purnell, Richard Sill","[('ReasonFdn_NTIA_OpenFdnAI_032624', 'https://downloads.regulations.gov/NTIA-2023-0009-0165/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0165,
comment,2024-04-03T14:05:35Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0186,Chamber of Progress,,,Comment from Chamber of Progress,See attached.,"[('Chamber of Progress NTIA RFC', 'https://downloads.regulations.gov/NTIA-2023-0009-0186/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0186,
comment,2024-04-03T14:05:47Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0208,JusticeText,,,Comment from JusticeText,See attached file(s),"[('NTIA AI Open Model Weights RFC - JusticeText Comment', 'https://downloads.regulations.gov/NTIA-2023-0009-0208/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0208,
comment,2024-04-03T14:05:41Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0200,,,Sthalekar,"Comment from Sthalekar, Nikhil",See attached file(s),"[('AI_regulations_comments', 'https://downloads.regulations.gov/NTIA-2023-0009-0200/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0200,
comment,2024-04-03T14:05:01Z,NTIA-2023-0009,NTIA-2023-0009-0001,NTIA-2023-0009-0125,R Street Institute,,,Comment from R Street Institute,"The attached file contains the comments of Adam Thierer, Senior Fellow at the R Street Institute. ","[('Comments of Adam Thierer (R Street Inst) in NTIA Openness in AI RFC (March 2024)', 'https://downloads.regulations.gov/NTIA-2023-0009-0125/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NTIA-2023-0009-0125,
