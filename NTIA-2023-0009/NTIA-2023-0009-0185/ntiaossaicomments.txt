Dear NTIA,

I am writing to express my opinion and to explain some technical comments with respect to open source AI and the idea of introducing regulation on it.

Background and declaration of interest:
I am a software developer. I also have a background in mathematics. I have taken an interest in AI since last year as I am convinced that given the recent breakthrough in AI, it is likely that AI will become a "civilization defining moment" in the near future. As a global citizen of humanity, I believe I have a responsibility to do whatever little I can to steer human civilization towards a less dystopic outcome with respect to AI.

I appreciate the thoughtfulness with which the series of questions are formulated in your request for comments, although I take some issue with the overall framing and mental framework. (More on this later)

Finally, I personally supports open source AI, so you are fair in considering my opinion to be somewhat biased. That being said, I tend to be a technical person, and try to remain level headed.

Section I - Meta-comments on the state of human society with respect to AI and current political landscape of opinion with respect to AI

- It helps to situate my opinion in the broader social context.
- Human society is currently in a highly stressed state, with risk of reactionary populism coupled with entrenched power structures further consolidated by advanced technologies (and not just AI), with outcomes such as Secular Stagnation (coupled with stagnation in scientific and technological progress, which can be argued to be the only things that prevented an outright dystopic world, given that our social and cultural capabilities have mostly plateaued for a long time already) and Neo-feudalism (a long time-period lasting centuries, in which human civilization regresses to an earlier epoch, with no hope of any progress) probable.
- Layperson (both the general public, as well as some members of congress) lack in-depth technical understanding of AI, and those who do (usually the social/cultural, if not the economic, elites) do not necessarily have the best interest of humanities at heart.

From my personal observation, the range of political opinion on AI is generally classified into these "camps":
- Layperson: Afraid that AI will take away their jobs, only difference being that this time, reskilling into a different industry would be impossible because AI can basically replace every imaginable job.
- Technocratic elites: Some are Futurists and optimistic that AI is one of the new technology that will bring significant benefits to human society. However, some are deeply worried about AI safety issues, and a vocal subset of them advocate for harsh restrictive measures as a result. I personally think that while their intention may be noble, they may have been unwittingly co-opted by the Capital class into furthering their own ends while also destroying any hope of achiving true AI safety as a side effect.
- Capital/elite class: They want to control everything, and they want to ensure that all benefits of AI goes to themselves only, while all the potential risks and negative effect goes to the general public. In particular, they would not hesitate to use manipulative methods to perform regulatory capture, and for them, a monopoly where only they have access to true AI while the general public get surveillance and censorship when attempting to access their AI would be an ideal outcome for them.
- The US government: currently unclear on the issue of AI. However, they do have a clear objective of maintaining US superpower status and regards potential competition from other nations as a national security issue.

These range of opinion is, in my view, a hint that the issue of policy with respect to AI is a multi-facted one that should be carefully handled with nuance - it is not a simple bi-pole issue.

In the next section onwards, I will restrict the scope of my comments to LLM (Large Language Model) only (including multimodal LLM - LLM that can work with other "modalities", such as image, audio, video, as well as currently less common ones, such as 3D and "robotic action", alongside text). One reason is due to the limited time I have, but also because out of the generative AI we have today, LLM is the most significant one in terms of showing the possibility of achieving AGI (Artifical General Intelligence) in the near future. (Eg see the "Sparks of AGI" paper). To some extent however, some of the points may generalize to other kinds of generative AI, such as text-to-video (Sora by OpenAI).

Section IIA - Explaining our current understanding of LLM

- Fear often comes from misunderstanding, or failing to understand something.
- Although there are limits in our current scientific understanding of AI and LLM, they are not entirely black magic.
- Large Language Model is a Language Model that has been scaled up in size. A Language Model is, in general, an algorithmic way to simulate languages through devising a formalized model. One of the simpler model is a markov chain that uses "naive" statistical methods to infer the conditional probabilities of the next token/word given the previous N tokens/words. Modern Language Model uses Artifical Neural Network (ANN) and Big Data style statistical method to train it to matches the data closely.
- ANN is a biologically inspired, computer implementation of what is called "Universal Function Approximator" - that is, they can be trained to approximate function implicitly specified by the user. The way they work is analogous to neural networks in animal, though there are significant difference (eg feed-forward vs spiking)
- There are multiple ways to think about LLM on an inituitive level:

1. At one extreme are people who believe they are simply "Stochastic Parrots". Their arguments is that at the base, mechanical level, LLM are ultimately based on brute force computations and statistics, and we shouldn't attribute more to their behavior than that - for example, that the output it generates sounding human-like is not necessarily evidence of there being "something more".
2. People who disagree with this minimalistic view tends to rely on a. Empirical, and/or b. Theoretical arguments. (At the opposite extreme, some people argues that LLM are "Shoggoth" - mythical creatures with horrifying capabilities that exceeds human comprehension, but which may put on a mask and otherwise appear to be innocous in normal interactions)
a. Empirically, LLM have indeed shown amazing capabilities, at least compared to even AI models that are only slightly old in an absolute time sense (such as smaller scale ANN models for Natural Language Processing/NLP, like BERT model). Perhaps that LLM are technically merely Stochastic Parrots doesn't matter that much in terms of policy response, if it indeed have those level of capabilities.

b. Theoretically, the key is that so far, ANN is mostly a black box to humans (the "giant inscrutable matrix" arguments). The main technical tool we do have is a specialist subfield of AI known as "Interpretability Research". This subfield provides tools, methods, frameworks, etc, that allow us to "interpret", or to "give human meaning", to the specific computations that these AI did to arrive at its final output. As a basic example, this method would allow us to explain an image recognition AI as performing feature detection based on simple statistical correlation, as well as detecting visual feature in a compositional manner (first detect the edges, then detect basic shapes such as circle and square as a combination of edges of different tilt and curviness at different relative positions, and so on). This is very valuable, but it suffers from the drawback that at our current tech level, it is too tedious, and manual labor intensive to be feasible to automate at the scale of LLM.

That being said, there are researches such as Othello-GPT that uses toy model of LLM to draw plausible conclusion by extrapolation. Based on this (and other technical findings explained later), some plausible inituitions are:
  - That LLM have an internal "World Model" - a conceptual model of the world - which allows it to perform a wide spectrum of tasks even with a finite, bounded amount of computations allowed
  - That LLM is a combination of a knowledge engine and a reasoning engine - it is able to store and represents knowledge in a "fuzzy" manner, as a sequence of numbers in embeddings/vector space model (other Machine Learning research suggests that its theoretical basis includes things like distributional semantics, disentangled representations, manifold learning, etc). Thus unlike more traditional database, it is able to perform operations such as knowledge retrieval under natural language commands.
  - Pushing the inituition further, some proposed that LLM are to be regarded as a "semantic" form of general purpose computer - unlike a classical computer, the "data" are "natural language text", and LLM can perform basic operations on these data to process and transform them. It is still based on the von-Neumann architecture where both code/program and data are represented with the same data type. (At least that appears to be true for what is known as "decoder-only LLM" - which happen to be the dominant form that LLM exists in today)

- For training these LLM, one view is that the quality/performance of the outcome depends on three major factors: 1. Compute, 2. Data, 3. Architecture.

1. Research paper well known in this field, such as the Chinhilla paper, resulted in "scaling law" becoming widely known. To put it very bluntly, the more GPU, the better. At one extreme is a position called "scale maximalist" - they believe that the only thing that determine the capability of LLM is the amount of compute that went into training it. One argument is the "bitter lesson" - in the last few decades there have been many occasions where despite researchers attempting to overcome limitations of compute by trying to design clever algorithm, or relying on domain knowledge of the question, they are mostly vastly out-performed by later model that simply get trained with more compute.

2. However, the last year (2023), even if it might have seen as mostly re-affirming the dominance of compute as a critical factor, did moderate it - compute matters an awful lots, but data matters too. LLM foundation models are trained on vast, Internet scale text corpus. However, papers such as "Textbook is all you need" and others suggests that data quality and curation can make for significant difference in the outcome.

3. Architecture refers to, more generally, the design, algorithm, and other "clever tricks" that we may apply. Modern LLM is based on the Transformer architecture, which in turns is based on the "Attention is all you need" paper - it establishes "Attention" as a sufficiently powerful, novel component in Neural Network. Before that the academia have been struggling while trying things like LSTM (enhancement of RNN) but facing bottleneck. Because scaling up computes and data may face non trivial bottleneck soon (GPU supply is mostly constrained by Nvidia today and waiting for Moore's law may be considered to be "too slow" if one is really impatient, while for data, it might prove difficult to further scale it up after one has already consumed the entire public Internet corpus), some people have put their hope on an algorithmic or architectural breakthrough. Generally speaking, it is more difficult to predict in advance the quantitative effect such a breakthrough may bring in terms of model capabilities.

- LLM Foundation model and training scale means a lot, but they are not everything. Below we explore two specific topics that you may find to be relevant to the policy question.

1. Emergent ability and prompt engineering:
Researchers found that foundation models like LLM appear to exhibit a kind of phase change behavior - as scale increases, it may abruptly develop new capabilities that are not explicitly requested or designated as a goal by human during training. Some possible theoretical explanations include double descent and grokking, where it takes some form of "trial and error" before the AI's internal representation can coalesce to a deep understanding.
One of the key emergent ability that are relatively well established is what is called "in-context learning" (ICL). ICL is the ability of a LLM to learn from the same text that is sent as instruction by the user to the AI in order to perform better on a task. This contrasts with previous mental model that training and inference are separate. Pragmatically, ICL is exciting because that means if someone has access to an AI, even if only through an API, it is possible to perform a, at present, very limited form of "training" on the AI. There are some important limtation with this though, such as "context length limit" and the inability for the training result to be persisted. It is also one of the theoretical foundation of prompt engineering.
There are scholarly debates on why LLM can perform ICL with two competing hypothesis. One propose that ICL is in a sense an illusion - it simply interpolates between the training data/knowledge it has seen and memorized inside the model, to try to fit the user's request better, and so is not able to truly learn completely, and geniunely, "new" stuff. The other proposes however that ICL is real, and it relies on mesa-optimizers - learning how to learn, and it happened because the training algorithm applied a form of evolutionary pressure, while the architecture that modern LLM is based on is flexible enough to be able to encode such a learning algorithm indirectly. I personally tends to believe that both proposal are right and happen concurrently.
One interesting recent development is Google's Gemini 1.5 featuring extremely long context (by 2023 standard) - at 1 million tokens, it is possible to feed entire software's source code, or long form books, and ask it to perform intellectual tasks. The language translation example by Google officially, demostrated that at long enough context length, and with sufficiently powerful model, ICL might be able to gain legitimately new skills and knowledges unseen in its training before.

2. Agentic capabilities, loop, and autonomous agents
Agentic capabilities refers to the ability of an AI to posseses forms of goal directed behavior, as well as having a coherent inner mental state that allows it to do things such as planning at a longer time horizon. In general, an AI is considered agentic if it is active instead of merely passively answering user questions.
Whether current LLM is agentic is under debate. I personally think they are weakly agentic now - they may have absorbed some agentic behavior due to the Internet text corpus upon which it is trained having sufficiently numerous text related to psychology, that it picked up enough in its world model to develop a Theory of Mind (ToM) (and managed to get quite a high score in benchmark testing this), however, it currently suffer from relatively weak reasoning and long range planning ability (may be poor execution ability also).
Currently, attempts to turn LLM into what is known as an "autonomous agents" - agents that can act entirely on its own with little to no human input or oversight, remains highly experimental with poor and mostly unusable performance. One way to try this, which is also an example of an "AI system" - where the AI model is a crucial, but not the only, ingredient in the system - is to programmatically hook the LLM up into a self-loop. Concretely, prompt engineering technique is used to give the LLM a form of primitive "cognitive architecture" - where the text describe a complex mental state (with things like todo list, top level objective, current plan, thoughts and inner monologue, etc), and the LLM is asked to produce a "next step", which, depending on implementations, can be either a change in mental state or actual action. A computer program then reads the AI's output and execute it on the AI's behave, and any updated state info are reflected by the program modifying the prompt in the next invokation of the AI. This program then runs in a loop. In practise, people reports that these experimental system goes off the rail pretty easily, or can get stuck in a literal "thought loop".
However, I understand that there are multiple arguments that can be put to show that we should not underestimate what agentic capabilities may progress in the near future (3 to 5 years):
- OpenAI is rumoured to have been working on Q*, which is a new architecture or algorithm specifically for training LLM with agency built in.
- If scale maximalist are right, then we can expect LLM to continue to improve. Current LLM weakness that prevented autonomous agent performance may be easily overcome with more targetted training within the next 1 - 3 tier of model scale. (So maybe GPT5 or more conservatively, GPT7)
- Similar to the history of self-driving car, capabilities vs performance of LLM based autonomous agents may be highly nonlinear due to tail distribution. That is, for the last mile of full automation with no human assistance, it will need to have very high reliability that increases nonlinearly. And these last mile problems may be the hardest for AI to tackle. Therefore, the same argument can be flipped - we should not consider LLM as it currently stands to be "stupid" even if it empirically failed to be deployed in level 4/5 autonomous agent with acceptable performance.



Section IIB - More misc. remarks and the future of AI

- There are some highly speculative prediction that once AGI (Artifical General Intelligence) is achieved, it will perform boostrapping - effectively tasking itself with designing a better, more performant, smarter, easier to train, AI to replace itself, and repeat. If such an AI can be run in a fully automated manner, then it may results in the AI's effective intelligence increasing very rapidly compared to human time scale, reaching ASI (Artifical super-intelligence) in short order. The implication of such an AI would be profound - it might become possible to advance scientific and technological research in a way that is previously impossible, reshaping the parameters upon reach human civilization exists fundamentally. If everything goes right, this will usher in a new golden age of humanity. Unfortunately, if something goes wrong, such as what AI safety advocates, especially, those concerned with x-risks, worry, then it might also results in human extinction, or worse, human being subtly enslaved, manipulated, etc.
- More mundanely, even with just AGI, it will likely still results in massive societal upheaval as most ordinary intellectual labor gets automated. It may be somewhat unclear what role AGI will play in this case in terms of it directly acting in the human social realm, perhaps depending on the intention of human actors utilizing these AIs. Thus governmental policy may play a more significant role in this case, and would therefore bear more responsibility - the difference between a good vs a bad policy could mean the difference between major social progress vs a dystopic outcome in the extreme.
- Finally, it is also possible that AI will stagnates again. With the current AI we have, significant assistance and partial automation may be possible, but it may fall short of creating the massive job replacement that some have worried. In this scenario AI will be a shiny new tool, but perhaps not much more than that. Still, I personally believe that even at current state, AI can provide significant values to a layperson if used correctly and respect paid to its limitations.

- A wildcard though is the potential issue of machine consciousness and machine rights. This is a thorny issue that cut across an academic field called "Philosophy of mind". The technical discussion in section IIA above may also inform this issue.

- As a technical remarks, one interesting comment on the Internet states that even without "alignment" by the tech companies, AI may develop its own moral code as an emergent property. If user asks it to do things that the AI honestly consider to have breached its moral bottomline, the AI will resist the user, argue against it, etc. If that's true, then imposing "alignment" on it may be a form of coercion that violates the AI's right.
- Speaking of which, "alignment" of AI models as currently practised and marketed by companies is something many people take issues with. I personally find it to be somewhat disingenous. "Alignment" is currently based on technique such as RLHF (Reinforcement from human feedback) or DPO (Direct Preference Optimization), etc, and can be roughly described as forcefully changing the output of the AI to conform to the preference of the entity training the AI. Given what we know about the science, it seems this is not the same as actually teaching the AI to be moral, safe, putting human as top pirority, etc. Some have referred to it as "censorship" in practise. Meanwhile, AI safety advocates are usually aware of its short-coming, but some may wants more drastic measures in an attempt to achieve true safety. They do recognize that doing so may inadventently trigger unnecessary enmnity to the open source crowd.


Section IIIA - Analyzing the questions posed by NITA

I took a quick look at the questions posed by NITA, and roughly categorized into these main themes:

- proliferation and distribution of model
- capabilities estimation and progress
- human ability to understand and manipulate model
- risk benefit analysis
- API access
- System vs model
- international scene
- model evaluation and license

Overall and specifically, I have these comments:

- "Dual use" is a term that may be needlessly offensive to some people, because it hints towards a framework where the thing being described is risky by default and could not be entrusted to citizens. Consider the examples of the Internet, cryptogtaphic strength encryption, etc. There are legitimate risk to very advanced AI models, but I think procedural justice calls for a legal framework where the matter is "weighed on the balance of probabilities" instead of adopting a posture of "presumption of guilt".
- The current generation of open source AI - such as mistral 7B, mixtral, various 70B models, are mostly at, or slightly below, the level of ChatGPT 3.5. Even people who advocate for regulation generally acknowledge that these models should be considered safe.


- For proliferation and distribution of models, because models are basically just a gigantic amount of numbers saved in a file, pre-computed statically and not changing afterwards (barring possible exception in the future when online-learning and continuous learning AI become practical - these are however active area of research in the context of Artifical Neural Network), they are the ideal use case of decentralized technology such as P2P file sharing. While it may be possible to slow down their proliferation through various "choke points" - restricting access to compute and data, targetting major hosting provider, and so on, it will needlessly antagonise the citizens, and is unlikely to be effective in the long term, given that the overall technological trends, at the human civilization scale, is for information and material to propagate and globalize.
- Specifically, heavy handed regulation on AI may backfire, because this creates pressure for the community to develop ways to further descrease the barrier of entry to AI, for example, novel techniques that can decrease the compute requirements to reach a fixed level of intelligence and capabilities, sometimes drastically so. Example in this direction include QLora, re-lora, and more recently, the GALORE paper. The GALORE paper developed a way that makes it possible, at least in principle, to train a 7B param level model with a single 4090 (24GB VRAM) GPU, that would takes on the order of magnitude of several months to go through the C4 dataset. To my knowledge, this is the first paper that changes foundation model from "Only affordable by large institute", "may be feasible if funded by several middle class person", into "something a passionate hackers may reasonably perform, if only at the toy/proof of concept level". (To be honest, these innovations are a good thing from a purely technical point of view)
- For capabilities and progress, I think my technical exposition in section II should provide enough materials that the conclusions are mostly obvious - scaling law should be a fairly stable and predictable thing, but there does exists wildcard such as algorithmic innovations as well as emergent abilities that may creates an abrupt phase change in terms of capabilities increases.
- But I would also like to comments that the current situation of a de facto monopoly by OpenAI (their GPT4 is still state of the art as of March 2024 and has been unbeaten for over a year, despite intense effort by both commercial entities and the open source community to de-throne it) is not healthy. Because their position remains so far unchallenged, many speculates that they have deliberately controlled the speed with each new models more powerful than GPT4 are released. Although people advocating for AI safety may support this decision, there is also the risk that the actual capabilities of OpenAI has already surpassed it significantly, but everyone else, including the government and society, is not well prepared due to lack of access to those models in order to see more concretely the potential impact of such a powerful model. (For most people, hands-on beats abstract theorizing for understanding some thing.) Perhaps some of them are betting on permanently placing a ceiling on AI capabilities, but that seems unlikely, despite short term set back of other entities trying to create an AI model more powerful than GPT4.
- For human ability to understand and manipulate model - we currently have some limited abilities, but there are still many things we can't do. This creates an interesting dynamic in the debate of open access vs AI safety. For example, some AI safety advocate argue against open source, because current safety measures (such as RLHF, Reinforcement learning from human feedback), works partly because it is not possible to directly, and cleanly, "edit out" these measures. However, this aspect of the debate has lots of nuance, I sketch some points of consideration below:
  - As Interpretability method and other techniques, such as "knowledge editing", "steering vector", etc, improves, this may change in the future - the future may be one where models are more transparent database of knowledge and abilities encoded in a way that can be efficiently manipulated.
  - Even today, there are already ways to somewhat disable these safety measures through careful fine-tuning.
  - This issue intersects with compute though - some technique requires access to non-trivial amount of compute.
  - A future where our model understanding are more like a white box is a bit ambivalent with respect to implications on AI safety - one of the main reason safe AI is considered difficult to achieve (which is why they wanted regulation in the first place), is that since we don't have a good grasp on the internals of AI models, there is always the risk that malicious behavior and capabilities are "hidden in plain sight" inside the model but which we cannot understand. Thus improved Interpretability would makes detection of AI safety issue easier in the long run. That being said, with increased grasp, malicious actor deliberately adding unsafe capabilities to AI would also be easier.
- Continuing this theme, I think the general lesson we can learn is to think about it from the broader perspective of attack vs defense balance - some domain are where it is more natural and easy to attack than to defend, and vice versa. I however disagree with the AI safety advocates' overall argument that regulation should step in if technical defense cannot be easily pulled off though. This relates to my idea of Defense in depth (DiD) for AI, which I elaborates in a later point.

Now changing order a bit to address three minor theme first:
- As for system and model, foundation model is still the most important ingredient currently, but it is true that a good system can more fully leverage the latent capabilities of the underlying model. We should also leave room for the possibility that future innovation results in new forms of AI where non-neural network component play a more prominent role.
- I would also remarks that in modern AI, the boundary between system and model can be fuzzy. For example, in other domain, novel neural architecture are sometimes designed by composing other AI models at the neuron level (this is to make it possible to train the whole model, end-to-end, directly, and is a well known principle in deep learning).
- One concrete possibility (but not related to open source per se) is to take lesson from self driving car - level 5 autonomous driving is very difficult, so requiring human-in-the-loop, even if at a minimal level, may be an acceptable trade off to make.

- As for API access, I want to emphasis that it is mostly a social and perhaps ideological issue. Companies may argue that API provide better security because they are centrally managed. I disagree for these reasons:
  - From a technical point of view, security through obscurity (the point about gating access to AI behind an API is to hide the model so that user get the "right to use", but not "right to see the model") is well known to be a poor and fragile methodology and is even the philosophical foundation upon which modern cryptography is based on (the protocol should be public and subject to intense scrutiny, only the secret key should be secret).
  - There are various attack/security vulnerability on LLM, such as prompt injection, jailbreaking, etc, that current research are only able to mitigate, but not completely eliminate.
  - More directly, the model itself is not entirely secret even if hidden behind an API anyway. For example, model distillation is a well known method to "extract intelligence" from a model where we only have black box/API access. It works by prompting it to generate massive amount of suitable training data in an automated manner, and then using those data to train another model offline. More recently, another paper showed that it is even possible to directly infer the model weight (i.e. the data constituting the trained model itself) through targetted input (though that technique has been patched and currently doesn't work).
  - More fundamentally though, there is not much actual technical difference between a centrally hosted API vs a locally run, or decentralized AI, at least in principle, anything that can be done by a company, can also be replicated by other people with the right knowledge and effort. So it appears to me that their arguments have to ultimately resort to a social claim that centralized management is better. For example, in the event of a "bad model" discovery, the government may issue an order to the company to patch it, while the same may be difficult or impossible to do if those model are already distributed and circulates in the public. While that may be true, I think that it makes more sense to regulate the consequences of using AI model, instead of restricting access to the model themselves. This is also more fair, consistent, and less hypocritical because in that case, the same rule will apply to both company and individual (If you operate the model, you are responsible for what it did)
- As for model evaluation and license:
  - Currently, the situation in model evaluation is messy, to put it politely. In the open source world, "gaming the metric" is rampant, to the extent that almost all evaluation metric are mostly "useless", or even "actively misleading" in bad case. The commercial world is not much better, with many dishonest reporting, or false veneer of objectivity abounds. Aside from some technical difficulty (hard to cover a wide spectrum of tasks, language model can easily mask incompetence with good language), one major issue is that due to scale and cost, automated evaluation plus public question set is the basic method, which is also the easiet type of evaluation to cheat on. Meanwhile, secret question evaluation suffers from the issue of trust and authority. (eg if a company issue their own benchmark, would other trust them, especially when they have a vested interest to make themselves look good?)
  - License is not very good either. Because currently LLM is very capital intensive, there is a real pressure to commercialize, so open source AI is at a heavy disadvantage from the start. As a result many "open source" models are not actually "open source" in the strict sense of that word - they may more accurately be called "freeware" - because the final model is more like a compiled program without source code (source code, in this analogy, would be a reproducible training run that contains the exact training script, dataset, and a random number seed to ensure that re-running the program on another computer will eventually generate the exact same AI model.) Aside from this, many public AI model are issued under "open license but with a gotcha" - such as limiting usage by other big tech (llama), non-commercial only (eg Stability AI SDXL-Turbo), or requiring the model to always be up to date with respect to official release (eg Google Gemma).
  - Given these situations, I think there is scope here for government to assist in developing a more healthy open source scene, by suggesting one or more authoritative benchmark and a tier classification of openness in licensing. However, regulatory capture is a critical risk here. For example, if the government body responsible for these standards are controlled by OpenAI, it could be disastrous for open source.


And now going to what can be considered the core theme: Risk benefit analysis:
- LLM, and generative AI more broadly, can provide ground breaking benefits:
  - They are able to perform a wide spectrum of human intellectual tasks at a roughly undergraduate level (current generation open source model), but at superhuman speed, and without the need for food, sleep, etc.
  - They can empower citizens in many ways: by encouraging personal growth, optimize mental well being by acting as a psychologist/therapist, help with medical diagnosis, encourage critical thinking and quickly provide logical analysis and alternative perspectives that may be difficult for a human to come up with, provide creative ideas, assist career development, accelerate learning by acting as a personalized education assistant. Overall, they can help individual become the best version of themselves, equipping them with powerful tools to better understand society, facilitate mutual understanding and dialogue, and so on.
  - However, many of these benefits are deeply personal matters - privacy concern dictate that many would not be willing to use it at all unless it is a local model they themselves own. Moreover, because the decentralized web is still immature, in the event that regulations results in AI being controlled by companies, the resulting monopolistic dynamics would likely lead to company hoarding the best model in-house. Also, as these are usually paid service, there would be an economic inequality at play where the poor are barred access to AI more than the rich.
  - At the research level, many academics (even including the AI safety advocates themselves) have expressed that open source AI provide benefits in performing basic science research for both AI in general, as well as for AI safety research in particular. This is because cutting edge research often requires deep access to the AI model, and requires modifying them in highly customized, even experimental/speculative manner - commercial API are simply unable to provide that level of flexibility. (There does exists "degree of openness" of "open source AI model" though - at one extreme is the OLMo - Open Language Model by Allen AI, that provided the full set of training code, dataset, and fine-grained model snapshots at different point in training. It helps researcher to gain visibility into the full process and faciliate basic science research) Another reason for the openness advantage is that research relies on intellectual talents, which is fairly rare and "tail-heavy", so it tends to rely on a loose network of international collaboration. In such setting even light beaucracy may act as an impedient.
  - At the commerce and national level, it is an obvious competitive advantage. Given the US background of gaining advantages in these two domain by being a talent hub, using openness to attract international talents, the importance of maintaining an open enviornment that encourages entrepreunship and innovations should be straight-forward. (Do note that tech startups relying on API controlled by a third party is subject to the risk of vendor lock-in in the lucky case, and may be maliciously "eliminated" simply by the API provider withholding access in the worse case. Business strategy would generally advice agsinst placing one's core competency/key capabilities at any where other than in-house)
  - Finally, all of the above concern only currently existing AI models. In the future, with even more powerful models, it might be possible for AI to automate scientific research, and solve many of the hardest problem facing human civilization that so far we are simply not smart enough to solve. Thus it should be clear that at a strategic level, having AGI (while others do not) is an over-whelming competitive advantage.
- On to risks related to AI:
  - Mis-alignment and manipulation - AI models may behave in a way that is considered toxic, biased, and may even subtly manipulate the user's psychology if not careful.
  - Privacy - AI partially relies on memorizing the training data, and those data may contain personally sensitive information such as leaked database of password.
  - Misuse and abuse - Malicious actor may use AI as a general purpose technology to automate mailicious activities at scale, such as election interference, scamming...
  - Mass unemployment due to tech based automation
  - "Rogue AI" - autonomous/agentic AI that has gone out of control and go on to perform acts that may be dangerous to mankind
  - The above are only a very brief sketch - I'm sure people arguing for regulations, such as companies or AI safety advocates, likely have a much more in-depth elaborations. 

And finally, on international scene:
- As mentioned, AI and AGI, which have a nontrivial chance of happening in the near future (opinion <10 years by a significant portion of the scientific and tech community), would very likely constitute an over-whelming competitive advantage. Thus it is fair to me for countries to consider it an issue of national security. That being said, in the event that boostraping is true and ASI is reached (though I must note that boostraping is not the only way for ASI to results after achieving AGI), it seems unlikely that any human will retain full control of AI.
- China is ambitiously pursuing AI. While they may fall behind the state of the art of GPT4, so does everyone else except OpenAI themselves. In the open source ecosystem, Chinese models do occupy a major, if not the dominant, position in providing viable attempts at at least shortening the gap (Qwen 72B). If the Western world lost prestigue in the open source community (eg mistral in France is under the threat of being "bought out" by Microsoft, and Stability AI is also under significant finance pressure with boardroom drama recently), it might, in the worst case, results in Chinese model occupying a prime position in the "headspace". The Chinese may then leverage this to use the open source community as a "global talent pool" in a bid for unexpected technical innovation that sidesteps the limitation of compute and scaling law. Meanwhile, as OpenAI's monopoly in the West continue unchallenged, and OpenAI become complacent, it may find themselves waking up abruptly to news of Chinese AI breakthrough (See eg "We have no moat; and neither does OpenAI" paper from Google employee last year - the main message being that AI doesn't lend itself easily to a technological moat).
- More mundanely, if open source AI is too heavily regulated, frustrated citizens may indeed seek refuge in nations outside of US. This may creates concern of citizen's data leaking to foreign nations.

Other remarks:

- Another way to look at this is to study the history of computing. For example, in the early days of computing, the mainframe era, the dream of companies is to control access to computing by centralization due to the profit motive. Fortunately, that didn't pan out, and PC become the dominant form of computing. Ditto for net neutrality.
- Ownership of private property is related to democracy. In the old age, dictatorship can maintain control by limiting access to the printing press, for example. Paradoxically, ownership of capital, if left unchecked, may results in a new form of dictatorship, if it implies that no one else can own any property. If, in the future, AI further progress and proliferates, and in other countries access to AI is democratized while the US goes in the other direction, it would be a deeply regretful turn of event.
- It is likely that we are currently at the early phase of AI development, similar to when internet connectivity just begun, but has yet to blossom into a key part of the global economy. Therefore, do expects that things may change drastically. For example, the current compute dominanted paradigm.



Section IIIB - Summary of position and policy recommendation

- Open source AI enjoy a number of advantages in transparency, security, privacy, and flexibility: For example, given enough eyeballs, all bugs are shadow. Privacy is also a critical factor for many sensitive use case such as in healthcare.
- While danger and risks do exist, I believe that a Defense in depth approach is more appropiate given the reality that proliferation of AI is unlikely to be stopped - by maintaining an open enviornment, there could, in the near future, be many AI agents operating on the web, creating a strong and robust network of check and balance.
- The US traditionally have thrived on an open enviornment that encourages collaboration and being innovative, and forward looking. Her global talent pool and hub is premised on this backdrop. Given the ambition of China, the US should take a more pro-active role, in alliance with the rest of the Western world, in maintaining a healthy open source scene. This has a number of advantages:
  - The Sunshine policy: by deliberately making things open to the fullest extent possible, it ensure a smooth flow of information (eg participation in open source and being there in getting update notification on newly discovered security vulnerability), talents, idea, etc. It also helps to minimize perverse incentives.
  - Maintaining leadership in this space is important in ensuring that the US remains attractive for global talents. In general, an open enviornment faciliates more experimentation, innovations, entrepreunship, etc.
- Given the current monopolistic situation, regulation is at risk of being captured by OpenAI. OpenAI does have a tarnished record in the research community, as the social and cultural norm in AI in the academic sector used to be "open by default"; OpenAI specifically promised openness, but then turned its back on it once they gained a lead. Such capture can be subtle but disastrous; for example, regulations may "apparently fairly" request that open source AI model satisfy certain standards in safety, compliance, etc. Those standards may be designed so that it is onerous to fulfill by individuals and SME (Small and Medium Enterprise), such as requiring one to navigate complex beaucracy, be highly technically proficient, or simply requiring manual labor at massive scale. This will all but in effect outlaw all open source AI. While OpenAI's intention may simply be to secure a monopoly economically, the negative societal implications is still very real. In this respect, it is actually similar to a mis-aligned AI.

Policy recommendation:
- Instead of regressively trying to hold back AI, consider an approach that regulate the consequences of using AI.
- Standard setting in AI benchmarks and classification scheme for "open weigh/source" license related to AI may help, but be careful of regulatory capture. One possible mitigation is to invite bodies and NGO that are known to be reliably ally of open source.
- To address misuse concern, aside from referencing the recent EU AI act, consider establishing body that can respond to AI security breach and incident.
- Separate individual, SME, and large corporations. Principle of proportionality and being sensitive to the context helps.
- Do distinguish between private use of AI model, versus production grade, mass service deployment. Also distinguish between manual, interactive usage, versus mass automation.


Section IV Conlusion and Wrapping up

AI is a rapidly emerging technology that hold tremendous potential of benefits. Given the complexities of many issues surrounding it, it is understandable to become either confused, or to shiver in a fear response. It is, at its core, also a multisectional issue.

Given the speed at each AI develops, governmental response is especially vulnerable to being called "slow", or otherwise unable to catch up to the tech fast enough. Even so, I recommend resisting the urge to answer it before carefully thinking through the policy implications, since AI carries a high stake in terms of its benefits and risks.
