commentOrDocument,modifyDate,docketId,commentOnDocumentId,id,organization,firstName,lastName,title,comment,attachments,link
document,2024-02-03T20:19:32Z,NIST-2023-0009,,NIST-2023-0009-0001,,,,"Request for Information (RFI) Related to NIST’s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11)",,"[('Request for Information (RFI) Related to NIST’s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11)', 'https://downloads.regulations.gov/NIST-2023-0009-0001/content.pdf'), ('2023-28232', 'https://downloads.regulations.gov/NIST-2023-0009-0001/attachment_1.pdf')]",https://api.regulations.gov/v4/documents/NIST-2023-0009-0001
comment,2024-02-01T22:28:21Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0035,,,Minerva,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST AI Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0035/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0035
comment,2024-02-01T22:29:34Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0040,"Duality Technologies, Inc.",,,Comment on FR Doc # 2023-28232,"Attached please find comments from Duality Technologies, a provider of privacy technologies used to secure AI and enable AI collaborations. Thank you very much. ","[('Duality Technologies Response to Request for Information vSUBMITTED', 'https://downloads.regulations.gov/NIST-2023-0009-0040/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0040
comment,2024-02-01T22:30:12Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0041,,,Scheller,Comment on FR Doc # 2023-28232,See attached file(s),"[('Final Draft MAIC NIST AI EXECUTIVE ORDER', 'https://downloads.regulations.gov/NIST-2023-0009-0041/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0041
comment,2024-02-01T22:28:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0036,Coalition for Content Provenance and Authenticity (C2PA),,,Comment on FR Doc # 2023-28232,"Dear Director Locascio, <br/><br/>My name is Mounir Ibrahim and I am submitting comments to the Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence on behalf of the Coalition for Content Provenance and Authenticity (C2PA). <br/><br/>Please see the attached letter signed by the C2PA Chairman, Andrew Jenks. <br/><br/>Do let us know if any problems opening the document or if there are any questions. <br/><br/>Thank you for your consideration. <br/><br/>","[('Submission - C2PA NIST AI RFI Signed', 'https://downloads.regulations.gov/NIST-2023-0009-0036/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0036
comment,2024-02-01T17:13:05Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0024,StakeOut.AI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('StakeOut.AI NIST Comment_1Feb2024', 'https://downloads.regulations.gov/NIST-2023-0009-0024/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0024
comment,2024-02-01T17:03:36Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0007,"National Federation of Independent Business, Inc.",,,Comment on FR Doc # 2023-28232,"NFIB (National Federation of Independent Business) comment letter of January 10, 2024, in response to Department of Commerce/NIST Notice Titled &quot;Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5, and 11 of Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11),&quot; Docket No. 231218-0309, NIST-2023-0309, 88 Fed. Reg. 88368 (December 21,2023), is attached.","[('NFIBcommentlettertoDeptofCommerceNISTonArtificialIntellligenceJanuary10of2024', 'https://downloads.regulations.gov/NIST-2023-0009-0007/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0007
comment,2024-02-01T17:05:23Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0012,,,Delaney,Comment on FR Doc # 2023-28232,"Hello, I&#39;ve attached my response to the RFI. I&#39;m writing as an individual outside the industry. My educational background is in mechanical engineering. Thanks, Joseph","[('NIST RFI Response', 'https://downloads.regulations.gov/NIST-2023-0009-0012/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0012
comment,2024-02-01T17:12:29Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0023,,,,Comment on FR Doc # 2023-28232,"The City of New York (&ldquo;City&rdquo;), through its Office of Technology and Innovation (&ldquo;OTI&rdquo;), submits the attached response to the National Institute of Standards and Technology&rsquo;s (&ldquo;NIST&rdquo;) Request for Information Related to NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence.","[('OTI Response NIST RFC FINAL', 'https://downloads.regulations.gov/NIST-2023-0009-0023/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0023
comment,2024-02-01T17:01:10Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0004,,,Swope,Comment on FR Doc # 2023-28232,"This submission presents a focused strategy for aligning with Executive Order (EO) 14110, enhancing Governance, Risk, and Compliance (GRC) by leveraging NIST&#39;s Artificial Intelligence (AI) Risk Management Framework (RMF). The response focuses on concrete, practical enhancements in AI utilization, specifically targeting systemic issues like organizational tribalism, departmental silos, and the management of Generative AI (GenAI) and Large Language Models (LLMs). The strategy boosts performance, leverages AI advancements for growth, and improves operational efficiency. Central to this plan is extensive AI practice enhancements and the introduction of innovative GRC tools, including a specialized Microsoft Excel Tool that can be scaled across public and private GRC AI Executive decision-making systems.","[('Attachment1_SWOPE EO 14110 NIST RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0004/attachment_1.pdf'), ('Attachment2_SWOPE EO 14110 NIST RFI SUPP EXCEL TOOL', 'https://downloads.regulations.gov/NIST-2023-0009-0004/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0004
comment,2024-02-01T17:08:44Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0017,Swear,,,Comment on FR Doc # 2023-28232,"To the National Institute of Standards and Technology (NIST),<br/><br/>Thank you for the opportunity to respond to the Request for Information (RFI) concerning the management of risks and harms of generative AI, as outlined in Sections 4.1, 4.5, and 11 of the Executive Order on Artificial Intelligence. We at Swear would like to provide insights on the roles different AI actors can play in this landscape and discuss the current techniques for content authentication, provenance tracking, and synthetic content detection.<br/><br/>Roles of AI Actors in Managing Risks and Harms of Generative AI:<br/>-AI Developers: The primary responsibility of AI developers is to incorporate ethical considerations and risk mitigation strategies during the development phase of AI technologies. This includes embedding mechanisms for content authentication and traceability within the AI systems, especially those capable of generating synthetic content.<br/>-AI Deployers: Deployers should ensure that AI systems are used within ethical boundaries and comply with existing regulations. They are responsible for implementing content labeling and detection mechanisms to distinguish between authentic and synthetic content.<br/>-End Users: The end user&rsquo;s role involves being vigilant and informed about the potential risks associated with AI-generated content. They should be equipped with tools to verify content authenticity and be aware of the implications of sharing unverified information.<br/><br/>Current Techniques for Content Authentication and Provenance Tracking:<br/>-Swear&#39;s approach to content authentication and provenance tracking involves real-time digital watermarking and cryptographic signatures. This method ensures that any piece of digital content is authenticated at its point of creation and its journey is traceable throughout its lifecycle. (Please see Attachment 1_Swear Explainer.pdf)<br/>-The feasibility of this approach lies in its seamless integration into existing content creation and sharing platforms, requiring minimal behavioral change from end users.<br/>-Regarding validity and fitness for purpose, real-time watermarking and cryptographic techniques offer a robust solution against sophisticated AI-generated forgeries, ensuring the integrity of digital media.<br/><br/>Techniques for Synthetic Content Labeling and Detection:<br/>-Current techniques involve AI-driven algorithms that analyze visual and audio content for signs of manipulation. However, these techniques often lag behind the rapidly advancing synthetic content creation technologies.<br/>-Swear proposes a preventive approach where content is authenticated at the source, thus reducing the reliance on post-hoc detection methods. This method is more efficient in preventing the spread of synthetic content.<br/><br/>Reducing the Risk of Synthetic Content:<br/>-The key to reducing the risk lies in a proactive strategy where content is authenticated at the source rather than relying solely on detection after the content has been disseminated.<br/>-This approach involves a trade-off: while it is highly effective in preventing the spread of deepfakes, it requires direct consumer opt-in or integration into content creation tools and platforms.<br/><br/>Opposing Views:<br/>-Some may argue that real-time authentication could potentially infringe on privacy or creative freedom. However, Swear&#39;s method prioritizes user privacy and data security, ensuring that the content remains in the user&#39;s control.<br/>-Another opposing view is the reliance on AI for detection of synthetic content. While AI detection is a valuable tool, it should be complemented with proactive measures to ensure a comprehensive approach to content integrity.<br/><br/>Swear&#39;s approach to managing risks associated with generative AI involves a combination of real-time content authentication, ethical AI development practices, and informed use by end users. This approach not only addresses current challenges but is scalable and adaptable to future advancements in AI technology.","[('Attachment 1_Swear Explainer', 'https://downloads.regulations.gov/NIST-2023-0009-0017/attachment_1.pdf'), ('Attachment 2_Swear Workflow', 'https://downloads.regulations.gov/NIST-2023-0009-0017/attachment_2.pdf'), ('Attachment 3_Swear Screens', 'https://downloads.regulations.gov/NIST-2023-0009-0017/attachment_3.png')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0017
comment,2024-02-01T17:05:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0013,,,Anonymous,Comment on FR Doc # 2023-28232,"Submitted Comment:<br/>My response relates to responsible AI risk management for accounting and finance industry. For responsible AI risk management, the implementation of specific control activities is essential to ensure that AI systems are developed and used in a safe, ethical, and compliant manner. Here&#39;s a list of recommended control activities tailored for managing risks associated with Artificial Intelligence:<br/>1. Ethical and Legal Compliance Reviews:<br/>   - Conduct regular reviews to ensure AI systems comply with ethical standards and legal regulations.<br/>   - Implement control mechanisms to monitor changes in laws and ethical guidelines relevant to AI.<br/>2. Data Quality and Privacy Controls:<br/>   - Establish protocols for data accuracy, completeness, and relevance.<br/>   - Implement robust data privacy controls, including data anonymization and encryption<br/>3. Bias Detection and Mitigation:<br/>   - Regularly test AI models for biases (gender, racial, or otherwise).<br/>   - Implement procedures to mitigate and correct detected biases<br/>4. Model Validation and Verification:<br/>   - Conduct thorough testing and validation of AI models for reliability and accuracy.<br/>   - Implement version control and approval processes for model updates.<br/>5. Security Measures for AI Systems:<br/>   - Implement cybersecurity measures to protect AI systems from unauthorized access and tampering.<br/>   - Conduct regular security audits and vulnerability assessments.<br/>6. Transparency and Explainability Measures:<br/>   - Ensure AI decisions and processes are transparent and understandable to relevant stakeholders.<br/>   - Implement control activities like logging and reporting mechanisms to track AI decision-making processes.<br/>7. Human Oversight and Intervention:<br/>   - Establish processes for human oversight of AI decision-making.<br/>   - Develop mechanisms for manual intervention and override of AI decisions where necessary.<br/><br/>","[('Submission Comment', 'https://downloads.regulations.gov/NIST-2023-0009-0013/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0013
comment,2024-02-01T17:00:01Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0002,Robotic Technology Inc.,,,Comment on FR Doc # 2023-28232,"Robotic Technology Inc. (RTI) has been involved with autonomous AI/robot systems since the previous millennium, including performing many projects for NIST. Here we are submitting a White Paper that suggests and defines metrics for trusted autonomy. ","[('White Paper Metrics For Trusted Autonomy 3 Jan 24', 'https://downloads.regulations.gov/NIST-2023-0009-0002/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0002
comment,2024-02-01T17:01:28Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0005,OutSecure Inc,,,Comment on FR Doc # 2023-28232,The current state of AI Cybersecurity awareness and preparedness is inadequate and poses a high risk to society at large.  <br/>We require a Cybersecurity Framework focused on AI specifically.  <br/>I have been defining strategic risk based Cybersecurity &amp; Privacy programs at large Fortune 50 companies for last 25 years so know first hand that security community and leaders at most organization&#39;s have less time for complex strategy and proactive approach as is required to solve for this gap effectively.<br/>I also gave this view as a comment at a virtual NIST AI hearing.<br/>,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0005
comment,2024-02-02T13:58:04Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0044,Centre for Information Policy Leadership (CIPL),,,Comment on FR Doc # 2023-28232,"Dear Sir or Madam,<br/>Thank you for the opportunity to comment on NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11). Please find attached comments from the Centre for Information Policy Leadership (CIPL). ","[('CIPL - Response to NIST-2023-0309', 'https://downloads.regulations.gov/NIST-2023-0009-0044/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0044
comment,2024-02-02T13:58:43Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0045,"Salesforce, Inc.",,,Comment on FR Doc # 2023-28232,"See attached file(s) for Salesforce, Inc. comments","[('Salesforce Submission - NIST RFI 2.2.24 ', 'https://downloads.regulations.gov/NIST-2023-0009-0045/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0045
comment,2024-02-02T13:57:35Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0043,Kaiser Permanente,,,Comment on FR Doc # 2023-28232,"ATTN: AI E.O. RFI Comments<br/><br/>Submitted electronically to: www.regulations.gov<br/><br/>RE: Request for Information (RFI) Related to NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence [NIST&ndash;2023&ndash;0009&ndash;0001]<br/><br/>Kaiser Permanente offers the following comments on the above-captioned RFI.","[('KP Comments NIST AI 20240202', 'https://downloads.regulations.gov/NIST-2023-0009-0043/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0043
comment,2024-02-02T14:08:33Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0067,,,Balebako,Comment on FR Doc # 2023-28232,Please find in the uploaded file my comments regarding AI red-teaming.,"[('NIST Red Team comments from Balebako', 'https://downloads.regulations.gov/NIST-2023-0009-0067/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0067
comment,2024-02-02T14:05:29Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0056,,,Broyde,Comment on FR Doc # 2023-28232,"Generative Ai is predominantly built on copyrighted material. It cannot be allowed to be legally used in its current state while it&#39;s very existence requires it to learn from stolen data.<br/>As far as its use in a commercial sense, I wish for them to only be allowed use legally obtained data.<br/><br/>Beyond that it can be and has been used to create a significant amount of misinformation. It can be used to create misinformation that can ruin a massive amount of lives:<br/>-Mia Janine, a 14 year old girl whom committed suicide because AI generated nudes of her were spread<br/>-Ai generated clip of an official claiming they plan to sabotage the election<br/><br/>The reality of GAi is that it can speed up some jobs, while it&#39;s downsides can lead to a literal destruction of democracy through falsified information and propaganda.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0056
comment,2024-02-02T14:01:19Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0047,,,Billy,Comment on FR Doc # 2023-28232,"I am an individual commenting on the security risk of AI. I think robots can be very beneficial if used properly just like any other items. Basic robots for a few basic duties  are  probably safe. When it comes to programming adaptive personality and upgraded capabilities. I think we need several layers of security features from 3 different generations of human to secure the capabilities installment features. What&#39;s ethical for a 21 year, a 35 year old, and a 50 year old varies. We need panels from all corners of the content, all age groups and all races to work together on the creation, the adaptability, the boundaries, and the use.  There is no do over on this. Some features should require 3 layers of mankind affirmation from 3 generations.  The adaptive capabilities must be highly controlled from the very start. I hope this comment helps.<br/><br/>A Billy<br/>[EMAIL REDACTED]",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0047
comment,2024-02-02T14:03:47Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0051,,,Yago,Comment on FR Doc # 2023-28232,"AI in its current form is inherently harmful to humanity. Datasets have been created unscrupulously, taking data without consent. Datasets contain images of child sex abuse and other horrific and illegal material. AI models have been used to create pornography, which has led to at least one suicide. It is my strong conviction that all current models and datasets be abandoned, and for AI to be pursued in an ethical manner, with strong oversight.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0051
comment,2024-02-02T14:04:49Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0054,,,Anonymous,Comment on FR Doc # 2023-28232,"The most barebones regulation of Generative AI MUST be, for the future of Art and Culture, a complete lack of copyright protections and discouragement of use in general.  The relative savings of foregoing artists will lead to a rise in unemployment and a stark drop of the quality of any media made with it.  This doesn&rsquo;t touch on, however, the creation of Generative AI media directly requires a flagrant disobedience of copyright law as it currently stands.  No Generative AI model has been shown as possible without the theft of hundreds, if not thousands of copyright protected media that was not granted consent of use.  Thus, Generative AI must be seen for what it is, a theft machine built on the backs of art workers that seeks to unemploy and disenfranchise them.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0054
comment,2024-02-02T14:07:42Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0064,,,Deng,Comment on FR Doc # 2023-28232,"I am a student artist who has worked professionally in the illustration and concept art field. Generative AI as it is is unethical in using our labor and harmful towards our industry. <br/><br/>Generative AI is not equitable because it exists solely on the merits of its data, which are our data - our copyrighted images. Without this data, generative AI would not be able to compete with our labor. An ethical model would be a GenAI that only trains off of public domain images. This is well within the possibilities, but this would simply mean that AI companies make less money, and thus they do not do this.<br/><br/>It is extremely harmful to the creative ecosystem for AI companies to be able to take our labor and data for free to build a product that directly competes with us.  <br/><br/>GenAI companies should be required to build their technologies transparently and ethically. As it is, Generative Image AI is a violation of human creative rights, the right to own and protect our intellectual and creative works, and will contribute to a world in which humans are not able to control and profit off of their own work, as it is immediately cannibalized by AI systems to directly compete with itself. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0064
comment,2024-02-02T14:08:49Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0068,,,Campos Costanzo,Comment on FR Doc # 2023-28232,Here only to second the opinion of the commenter who submitted the stakeout.ai document on strict AI regulation.,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0068
comment,2024-02-02T14:05:57Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0058,,,Brorson,Comment on FR Doc # 2023-28232,"Good day<br/><br/>I&#39;d like to please make a comment regarding the regulation of generative AI.<br/><br/>Imagine a world where there are no new creative ideas brought to life through the arts because aspiring creatives cannot protect their work from being taken by AI corporations without consent, credit or reparations. <br/><br/>Protecting intellectual property rights ensure that creatives have the freedom to explore and express their ideas without fear of them being consumed by AI datasets created to replace/ compete with them.<br/><br/>Thank you for allowing me the opportunity to make a comment.<br/><br/>Warm Regards<br/>Dean Brorson<br/>Fine Artist and Illustrator ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0058
comment,2024-02-02T14:08:09Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0066,,,M,Comment on FR Doc # 2023-28232,"For a safe future with AI we need all datasets to be public. Large datasets are hosted in cloud storage, they need to be publicly accessible and searchable without any paywall or logins so that independent verification of the content can be done by anyone at any time.<br/><br/>A standardized system of disclosure is also essential. Most commercial projects rely on underlying larger AI models. These smaller companies must disclose which models and companies they are using in an up front and easy to access way. Not standardizing these disclosures creates a legal minefield for any potential customers.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0066
comment,2024-02-02T14:02:37Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0048,,,Anonymous,Comment on FR Doc # 2023-28232,"Generative AI is a humanitarian rights and public safety issue above all else. The potential for public harm is possibly catastrophic should it be left unsupervised in it&#39;s current state. It (generative AI) has already cost the livelihoods of several person&#39;s who have before now relied on their craft for making a living and are now being replaced by a mechanism that processes results based on the theft of these very same peoples works. Generative AI cannot continue to exist in its current theft based, unregulated form.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0048
comment,2024-02-02T14:06:06Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0059,,,Melley,Comment on FR Doc # 2023-28232,Generative AI should not be permitted to use any works/assets/media/etc. without the creator&#39;s/owner&#39;s explicit consent. <br/><br/>Generative AI should not be able to be copywritten. <br/><br/>The environmental cost of Generative AI should be accounted for when creating legislation or guidelines pertaining to it. ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0059
comment,2024-02-03T14:29:24Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0100,Anthropic,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Anthropic Response to FR Document Number 2023-28232', 'https://downloads.regulations.gov/NIST-2023-0009-0100/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0100
comment,2024-02-03T14:49:40Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0200,"Ozni AI, LLC",,,Comment on FR Doc # 2023-28232,See attached file(s),"[('OzniAI_NIST_RFI_022024', 'https://downloads.regulations.gov/NIST-2023-0009-0200/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0200
comment,2024-02-03T14:35:58Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0125,Credo AI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Credo AI Response_NIST RFI_2 Feb 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0125/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0125
comment,2024-02-03T14:48:56Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0193,Consumer Technology Association (CTA),,,Comment on FR Doc # 2023-28232,Attached.,"[('CTA comments in response to NIST RFI  2-2-24', 'https://downloads.regulations.gov/NIST-2023-0009-0193/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0193
comment,2024-02-03T14:37:00Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0129,Palantir Technologies Inc.,,,Comment on FR Doc # 2023-28232,"On behalf of Palantir Technologies Inc. (&ldquo;Palantir&rdquo;), we respectfully offer these comments to the National Institute of Standards and Technology (&ldquo;NIST&rdquo;) regarding the &ldquo;Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11)&rdquo; (Docket 231218-0309). Palantir welcomes these efforts to responsibly advance AI innovation by ensuring the safety, security and trustworthiness of AI systems. In the attached document, we provide our perspective on topics raised in the RFI and look forward to future opportunities to contribute constructively to this important policy discussion.","[('Palantir Response to NIST AI RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0129/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0129
comment,2024-02-03T14:43:25Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0167,Amor Fati Labs,,,Comment on FR Doc # 2023-28232,"4.1: There needs to be clear linkage and delineation on dual use foundation models regarding the use of parameters and floating point operations from section 4.2b that specifies: any model that was trained using a quantity of computing power greater than 1026&nbsp;integer or floating-point operations, or using primarily biological sequence data and using a quantity of computing power greater than 1023&nbsp;integer or floating-point operations. AI model weight adjustment through forward pass/back propagation, tokenization, loss calculation, regularization, batch normalization and evaluations all necessitate floating point operations (FLOPs). And that is before you get into the language of whether you&#39;re using Floating Point (FP) 32, FP16 for training or choosing to quantize models. So while this qualifier of floating point operations and tens of billions of parameters exists in section 4.2b, it directly addresses sections 4.1 (guidelines- standards and best practices) 4.5 (synthetic content generation relative to methods that implement different variations of FLOPs) and 11 (standardization for global engagement). <br/>4.5: Identifying synthetic data generation is a tough one as that is the proverbial cat and mouse. I would submit for consideration that much like NIST has developed standards for post quantum security with the FIPS, that similar methods be established for synthetic data generation that are approved and implemented. Chasing down every new way of beating detectors is an ongoing battle that is necessary- but to be effective without being overly labor intensive it needs to be paired with synthetic data generation standards that are applied to the most high value use cases (training medical ai systems, cybersec etc) that has a seal from the government of authenticity. It is most likely this industry standard seal of authenticity will need to be post quantum secure or at least post quantum resistant to bridge the gap until full migration to post quantum security on hardware and software is set. <br/><br/>11: The reality is small language models (SLMs) and Small Multimodal models (SMMs) are already becoming the standard as well as the &quot;mixture of experts&quot; training process and implementation. In this case the answer is the development of small models (1-5 billion parameter considered small as of Feb 2 2024) that can then be compiled into a mixture of experts configuration with other small models based on use cases. Like clinical subspecialists working together on one patient with multifactorial pathologies, this model of training small domain specific models that can be combined as needed can help to focus data collection/development and training protocols. For red teaming, the most promising outside of post quantum secure FIPs is the cyberseceval from Meta with LLaMA guard and purple llama. Pushing that forward and implementing it as standard rather than recreating the wheel from scratch is the right call. <br/><br/>I have attached a proposal aligned the intersection of the domains of medicine and education addressing these same three subdomains of the EO. ","[('Amor Fati Labs response to NIST RFI on EO 14110', 'https://downloads.regulations.gov/NIST-2023-0009-0167/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0167
comment,2024-02-03T14:28:13Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0096,Kolena,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Best Practices for Trustworthy AIML  Model Testing for NIST', 'https://downloads.regulations.gov/NIST-2023-0009-0096/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0096
comment,2024-02-03T14:27:56Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0094,Accenture,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Accenture Official Comments NIST AI Executive Order RFI (2) ', 'https://downloads.regulations.gov/NIST-2023-0009-0094/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0094
comment,2024-02-03T14:27:04Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0091,,,Frank,Comment on FR Doc # 2023-28232,"I write today as a person who uses or is impacted by AI or an end user of such systems, and in my personal capacity as a career Federal employee whose use of AI will be ultimately enabled or constrained according to the guidance that NIST publishes about AI safety. In the discussion of AI actors such as myself within the framework provided in NIST AI 100-1, I would be under &ldquo;People and Planet&rdquo;, and appear to be regarded as something of an afterthought. This is a grievous mistake.<br/><br/>While it may appear that NIST AI 100 and Section 4.1 of EO 14110 have focused the attention on the development and deployment of AI systems, the paradigm of development and deployment in the case of what we consider to be generative AI &ndash; which presumes highly complex end user interactions with the software &ndash; fails to account for the fact that end users are no longer merely reacting to deployed software but crossing the boundary into development. That envelope is increasing, and can be expecting to increase further, as AI tools further bridge the gap between programming languages at various levels of abstraction and human language.<br/><br/>If the closing divide between end user and developer / deployer is not promptly addressed, the NIST project to provide risk management best practices will be certain to face a significant decrease in effectiveness, as well as an increasing risk that the project will entirely fail to produce an increase in AI safety.<br/><br/>The end user, moreover, lacks the time, resources, and interest in accounting for the many broad-based risk management issues that NIST AI 100-1 describes. Like travelers at an airport &ndash; or participants in a hackathon or conference &ndash; end users require signposts and streamlined procedures that enable them to customize a range of activities for which the general outline is known in advance. <br/><br/>To that end, I encourage NIST to develop a supplement to its risk management framework that provides a framework for end user training around current major use cases, as well as examples and explanations serving to increase end user awareness of common risks and common use cases and end user association of risks with a given use case. Such a supplement could also further the cause of Section 10 of EO 14110 by helping to identify beneficial shortcuts to AI project development, allowing Federal employees evaluating new AI projects to rapidly greenlight projects that meet certain criteria for low risk while identifying common pitfalls for other projects.<br/><br/>Such a supplement would ideally be task specific. For example, the supplement could be expected to have separate chapters for computer vision / identification problems, filtering or proximity / nearest neighbor searching, and resource allocation and other optimization problems. Ultimately, end users trained on such supplemental materials could be expected to assist in developing and using AI across these distinct tasks in a manner than supports the goals of the risk management framework, and to engage with additional caution or experimentation as appropriate to the use case and information available about the AI.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0091
comment,2024-02-03T14:29:48Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0102,,,Benson,Comment on FR Doc # 2023-28232,"Artifical intelligence can be used as a tool to help nearly all industries. However, how generative artifical intelligence is not, instead it is being used in creative industries. For creatives it is stealing the jobs. Actors, writers, animators, concept artists, environmental artists. There isn&#39;t a single job that will not be affected. Those directly affected now are the artist&#39;s coming out of school. Who need job experience but cannot get it because studios, large and small business are turning to artifical intelligence to do the job of ju ior artists. If there isn&#39;t a junior artist today, they&#39;re won&#39;t be a senior artist tomorrow. This doesn&#39;t even consider the copyright infringements of millions of artists, some of whom have had their art stolen over 64 times. We need regulations in place to not only protect individual artists, but jobs as well. In a time when mass layoffs have affected hundreds of thousands of game developers, over the last month January 2024. They do not need the added stress of artifical intelligence stealing their former job.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0102
comment,2024-02-03T14:30:00Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0103,,,Alvarado,Comment on FR Doc # 2023-28232,"In order to make sure that AI develops in a fair way current models must be scrapped due to the content they had no right to use in the first place. The technology is exciting, but is being weilded recklessly. The reason for so much pushback has to deal with how all this data was mined and the livelihoods it is disrupting ,and the speed at which large companies are trying to make it a part of their workflow before any determination can be made on the sheer impact and job loss it creates. This is incredibly important because without the stolen data there would be no generative AI.The minds behind Stable Diffusion and others knew exactly what they were doing and want this to be a situation that they&#39;d rather ask for forgiveness than permission to make millions off of stolen work.<br/><br/>https://www.theguardian.com/business/2024/jan/17/big-tech-firms-ai-un-antonio-guterres-davos<br/><br/>https://www.latimes.com/entertainment-arts/business/story/2024-01-30/ai-artificial-intelligence-impact-report-entertainment-industry",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0103
comment,2024-02-03T14:34:58Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0118,,,Tolkach,Comment on FR Doc # 2023-28232,"I would like to address the issue of integration of artificial intelligence (AI) into accounting within the framework of International Financial Reporting Standards (IFRS). IFRS introduces a transformative potential for enhancing the accuracy, efficiency, and comprehensiveness of financial reporting. However, this integration also brings forth significant security concerns that must be addressed to protect sensitive financial data and ensure compliance with regulatory standards. To mitigate these risks, it is essential to implement robust security measures tailored to the unique challenges posed by AI applications in accounting. This includes adopting advanced encryption techniques to safeguard data in transit and at rest, deploying anomaly detection systems to monitor for suspicious activities indicative of potential security breaches, and ensuring that AI algorithms are transparent and explainable to facilitate audits and compliance checks.<br/><br/>Moreover, it is crucial to foster a culture of security awareness among all stakeholders involved in the AI-driven accounting processes. Training programs should be instituted to equip employees with the knowledge and skills necessary to recognize and respond to security threats. Additionally, organizations should establish clear policies and procedures for data governance, access control, and incident response, specifically designed to address the nuances of AI applications in accounting. Collaboration with regulatory bodies and industry experts can also provide valuable insights into best practices and emerging threats, enabling continuous improvement of security measures. By prioritizing security in the deployment of AI within accounting practices, organizations can leverage the benefits of technology while minimizing risks, thereby enhancing trust and reliability in financial reporting under IFRS.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0118
comment,2024-02-03T14:35:07Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0119,,,Mitterhöfer,Comment on FR Doc # 2023-28232,"Hello, Artists, Writers, voice actors etc need protection against ai training. I found my work in LAION 5b so did many other artists. Those models directly compete in my current job and I am working 18 years in a game dev studio. Ai companies need to seek consent if they want to use my work for training. Also models need to be transparent so copyright holders can find their work. Opt out doesn&rsquo;t work the damage is already done current models need to be destroyed and trained ethically from zero. It should be opt in consent, credit, compensation and transparency. I am also a mother and deepfakes ai really concern&rsquo;s me this should be illegal. Thank you king regards.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0119
comment,2024-02-03T14:37:33Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0133,,,Jones,Comment on FR Doc # 2023-28232,The greatest concern I have for AI is data collection. Many models currently use copyrighted works without the permission of their creators and then profit off of the value this data provides. <br/>Furthermore there are privacy concerns if an individuals data is being collected and fed to generative AI without their knowledge. <br/><br/>I would strongly request regulation reinforcing that AI training data be  opt in- where parties agree to allow their words or images or other data to be used by generative AI.  <br/>To allow for creators and copyright holders to sell AI data training rights to generative AI makers. <br/>And enforcement to sue makers that use data without the proper rights.,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0133
comment,2024-02-03T14:37:47Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0134,,,Stearns,Comment on FR Doc # 2023-28232,"Guidance and standards for vendor/third party procurement in the private sector would be helpful. For example, Company A may have a robust program for model validation and implementation of responsible AI. But how does this company navigate vendor due diligence and risk work when they are contracting with a third party for services that operate using proprietary trade secret AI models? The vendor would likely not be willing to share their secret sauce AI models with clients who are concerned about their customer data and processes relying on the vendor AI models without knowing if the models have been vetted to address biases and potential adverse discriminatory outputs. Thanks for your consideration of this feedback.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0134
comment,2024-02-03T14:39:10Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0144,,,Anonymous,Comment on FR Doc # 2023-28232,"As generative AI continues to expand into mainstream use, the need for regulation is urgent. AI-generated media itself is not only poisoning the information ecosystem, but the mere presence of AI makes it harder to trust what one is seeing or hearing, even when the media in question is real. For a while now, there has been a slow erosion of trust in institutions and among society generally; AI will exacerbate this problem many times over if guardrails are not put into place. Additionally, unregulated AI has made it trivially easy for bad people to cheat, steal, propagandize, and harm others.<br/><br/>However, moral responsibility should not lie only with the users, but with the AI companies who have created commercial models using data scraped from the internet--including private, copyrighted and illegal material--and at inordinate energy costs.<br/><br/>Data transparency and a basic respect for privacy and copyright should be the standard for all AI training--especially when used for commercial intent; and on the user end synthetic media should be clearly labeled as AI-generated so as to disempower scammers, fraudsters, plagiarists, and bullies. Dire legal consequences should be brought upon those who would knowingly circumvent such regulations.<br/><br/>We want better technology, but it should not be at the expense of copyright, a trustworthy media, or the well-being of the most vulnerable among us--most especially kids. Ethics with technology should be the goal, not &quot;Technology no matter the costs&quot;.<br/><br/>I don&#39;t work in the tech industry, but as a U.S. citizen deeply concerned about the state of current generative AI, I just wanted to add my voice to those urging for reasonable AI regulations, and I appreciate you giving the chance to do so. Thank you and God Bless.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0144
comment,2024-02-03T14:40:51Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0153,,,Duke,Comment on FR Doc # 2023-28232,"To ensure safe and trustworthy AI that is beneficial to every member of society, including those from underrepresented groups and the Global South, it&rsquo;s important a responsible AI framework is adopted. In Toju Duke&rsquo;s book &ldquo;Building Responsible AI Algorithms - a framework for transparency, fairness, safety, privacy and robustness &rdquo;, she explains the foundations of developing safe and responsible AI focused on the following: <br/>1. AI Principles, 2. Data ethics 3. Fairness 4. Safety 5. Human-in-the-loop 6. Explainability 7. Privacy 8. Robustness 9. Ethical Considerations / AI Ethics. The principles outlined in this book form the basic foundation for ensuring the safe development and deployment of AI.<br/><br/>Following on from the above work, Toju Duke is partnering with Paolo Giudici in authoring a book titled &ldquo;Responsible AI in practice&rdquo; which introduces an additional Responsible AI framework called SAFE-HAI (Safe Human-centered AI). The focus of this work is on the safe and accurate AI development across AI models and applications. We propose risk management frameworks across the following Responsible AI principles: AI governance, robustness, security and safety (which includes red-teaming), accuracy, privacy and data, explainability, fairness and human rights, sustainability, and human-centered AI. Using several statistical metrics such as Area Under Curve (AUC) and Shapley values, we explain the application of Lorenz curves to measure risk and inequality across the different principles and a taxonomy/scoring rubric to identify and mitigate identified risks. <br/><br/>To ensure a Safe, Secure, and Trustworthy Development and Use of AI, we believe the above principles are necessary for adoption during the ML lifecycle and development of AI systems. These principles are applicable to all AI modalities including generative AI.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0153
comment,2024-02-03T14:26:25Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0088,,,S,Comment on FR Doc # 2023-28232,"Generative AI should be heavily regulated. It has no place in that Arts; whither that is writing, acting, voice acting, or any form of art. It is unethically trained on art that artists did not give their consent to. It has been used to recently make a deep fake pornographic film of Taylor Swift. This sort of thing will just keep happening as the technology gets more and more sophisticated. Deep fakes of that quality could be used as &quot;revenge porn&quot; against people leaving abusive relationships. It could be used to bully children in middle and high school, which could open up a whole can of worms in regard to that sort of situation. <br/><br/>Ultimately generative AI is only suited for mathematical applications. The best place for it is in replacing CEOs and stock market type information. Keep it away from the general public for their own safety. Keep it out of the arts, which is the one bastion of human creativity we have that truly brings joy to the populous. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0088
comment,2024-02-03T14:23:01Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0079,,,Turkewitz,Comment on FR Doc # 2023-28232,"As you move forward in this process, I urge you to avoid examination of issues presented by AI in a silo, and resist decontextualization that would undermine the ability to address the broader risks posed by inadequately regulated machine learning/artificial intelligence. Governance is anything but a technical issue, and &ldquo;alignment&rdquo; in far too much of the discussion of AI ethics is entirely circular and self-referential. As such, please don&rsquo;t limit yourselves to discussions with technical experts. Take the path less traveled. Pay attention to the voices of those marginalized in current discussions, and who consequently are those for whom risks are greatest. Center the voices of women, of people of color, of poets &amp; other artists. Artificial intelligence promises to remake a new society. We owe it to ourselves to build a better one than the one we currently inhabit. Technical architectures &amp; systems must avoid reifying current biases &amp; injustices. We must focus on establishing radical transparency and effective accountability in an environment that empowers rather than diminishes the role of consent. We have an opportunity and an obligation to establish governance in the public interest. I hope you will seize it.<br/><br/>Some specific thoughts on copyright/artists&rsquo; rights below:<br/><br/>An even more fundamental starting point for consideration of AI and copyright should be to unpack some of the words used in this exercise. Artificial intelligence. Machine learning. There is no artificial intelligence &mdash; only intelligence, and computers are not capable of possessing it &mdash; at least not in the way that we generally associate intelligence with reference to judgment. Computational efficiency &mdash; the ability to discern patterns through mountains of data so as to be able to predict outcomes, or to generate new patterns based on the past, is not an exercise in intelligence, and the anthropomorphism employed here affects the way we think about these issues. We need to exercise great care at the outset. Intelligence isn&rsquo;t reducible to stimulus-response, regardless of the sophistication and complexity of the experiment. Aggregations of data can tell us where we&rsquo;ve been. Perhaps also where, based on the past, we are likely to go. They can find patterns that may escape human perception that can inform future decision-making, but they necessarily are engaged in measuring the past, not dreaming about the future. They&rsquo;re incapable of being inspired. So let&rsquo;s be really careful here and not entrust our future to the guardians of the past. I will leave it to philosophers to debate the metaphysics of the meaning of intelligence &mdash; I think it will suffice here to observe that it involves more than mere computation, thus we employ the adjective &ldquo;artificial.&rdquo; But here, the adjective actually negates the thing it&rsquo;s supposed to be modifying and therefore philosophically defines a null set. That must be borne in mind as we proceed.<br/><br/>https://medium.com/@nturkewitz_56674/copyright-and-artificial-intelligence-an-exceptional-tale-60bdd77a8f13",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0079
comment,2024-02-03T14:50:36Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0203,Unlearn.AI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Unlearn Comment on NIST RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0203/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0203
comment,2024-02-03T14:42:44Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0162,"Meta Platforms, Inc.",,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Metas-Final-Submission-NIST-RFI.docx-Google-Docs (2)', 'https://downloads.regulations.gov/NIST-2023-0009-0162/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0162
comment,2024-02-03T14:38:21Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0139,Business Roundtable,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('FINAL BRT Comment Letter on NIST AI RFI.2024.02.02', 'https://downloads.regulations.gov/NIST-2023-0009-0139/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0139
comment,2024-02-03T14:40:05Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0148,creative.ai,,,Comment on FR Doc # 2023-28232,= On Datasets and their Regulation =<br/><br/>Full text attached or available here:<br/>https://gist.github.com/alexjc/c403bfdf8a14f7bb309795d36dd6544e<br/>,"[('Feedback_NIST', 'https://downloads.regulations.gov/NIST-2023-0009-0148/attachment_1.txt')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0148
comment,2024-02-03T14:44:38Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0171,,,Steed,Comment on FR Doc # 2023-28232,See attached file(s),"[('nist-comment_fr-2023-28232', 'https://downloads.regulations.gov/NIST-2023-0009-0171/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0171
comment,2024-02-03T14:38:32Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0141,CrowdStrike,,,Comment on FR Doc # 2023-28232,Please see attachment. ,"[('CrowdStrike NIST AI EO Comments', 'https://downloads.regulations.gov/NIST-2023-0009-0141/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0141
comment,2024-02-03T14:27:29Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0092,The Alliance for Trust in AI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Alliance for Trust in AI - NIST RFI response - Feb 2 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0092/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0092
comment,2024-02-03T14:26:14Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0087,American Psychological Association,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('APA Response to NIST AI RFI 2.02.24', 'https://downloads.regulations.gov/NIST-2023-0009-0087/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0087
comment,2024-02-03T14:43:18Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0166,UL Solutions,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('2024.02.02 ULS Comments NIST AI RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0166/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0166
comment,2024-02-03T14:40:10Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0149,Hugging Face,,,Comment on FR Doc # 2023-28232,"See attached file(s).<br/>We provide information on the following:<br/><br/>1. Developing Guidelines, Standards, and Best Practices for AI Safety and Security<br/>(1) Developing a companion resource to the AI Risk Management Framework (AI RMF)...<br/>Information on &ldquo;practices for implementing AI RMF core functions&rdquo;:<br/>  Govern: Data Governance, Data Governance Roles, Platform Governance<br/>  Map: Rights and Stakeholders<br/>  Measure: Data Measurement, Model Evaluation<br/>  Manage: Rigorous Evaluation Report, User Feedback<br/>Information on &ldquo;roles&rdquo;<br/>  ML Lifecycle Roles<br/>  Data Development Roles<br/>  Model Roles<br/>Information on &ldquo;current techniques and implementations&rdquo;<br/>  Identifying impacts and developing mitigations<br/>  Assessments<br/>  Content authentication, provenance tracking, and synthetic content labeling and detection<br/>    Models and systems<br/>    Verifying the connection between data and models<br/>  Measurable and repeatable mechanisms to assess or verify the effectiveness of such techniques and implementations.<br/>Information on &ldquo;forms of transparency and documentation&rdquo;<br/>  Model Documentation<br/>  Dataset Documentation<br/>  Assessment and Evaluation<br/>    Benchmarking<br/>    Social Impact<br/>Information on &ldquo;watermarking&rdquo;<br/>Information on &ldquo;disclosing errors&rdquo;<br/><br/>(2) Creating guidance and benchmarks for evaluating and auditing AI capabilities, with a focus on capabilities and limitations through which AI could be used to cause harm.<br/>Information on &ldquo;auditing AI&rdquo;<br/>Information on &ldquo;AI Evaluations&rdquo;<br/>Information on &ldquo;AI Red-Teaming&rdquo;<br/><br/>2. Reducing the Risk of Synthetic Content<br/>Information on &ldquo;synthetic content&rdquo;<br/>Information on &ldquo;non-consensual intimate imagery&rdquo;<br/>  Technical Solutions<br/>    Text-to-image systems<br/>    Image-only systems<br/>  Organizational Solutions<br/><br/>3. Advancing Responsible Global Technical Standards for AI Development<br/>Information on &ldquo;AI nomenclature and terminology&rdquo;<br/>  NFAA<br/>  Open Source, Open Science, and the Gradient of Openness<br/>Information on &ldquo;collection and use of data&rdquo;<br/>  Privacy<br/>Information on &ldquo;Human-computer interface design for AI systems&rdquo;<br/>  Gates<br/>  Modals<br/>Information on &ldquo;AI-related standards development activities&rdquo;<br/>Appendix<br/>  Information on Watermarking for the tracking of generated content from Imatag<br/>  New Key Terminology Introduced in this Document<br/>","[('Hugging Face - NIST RFI on EO', 'https://downloads.regulations.gov/NIST-2023-0009-0149/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0149
comment,2024-02-03T14:45:40Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0176,,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Public Comment - NIST Deepfake AI Tech Regulation (1)', 'https://downloads.regulations.gov/NIST-2023-0009-0176/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0176
comment,2024-02-03T14:45:09Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0174,Stability AI,,,Comment on FR Doc # 2023-28232,Please find attached comments from Stability AI.<br/><br/>Ben Brooks<br/>Head of Public Policy,"[('Stability AI response to NIST RFI on AI', 'https://downloads.regulations.gov/NIST-2023-0009-0174/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0174
comment,2024-02-03T14:49:07Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0195,Google,,,Comment on FR Doc # 2023-28232,Please see attached for Google&#39;s comments,"[('Google NIST AI EO RFI Comments', 'https://downloads.regulations.gov/NIST-2023-0009-0195/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0195
comment,2024-02-03T14:28:45Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0097,IMATAG,,,Comment on FR Doc # 2023-28232,"Please find attached IMATAG&#39;s formal comment to Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11)<br/>","[('Imatag Comment NIST AI Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0097/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0097
comment,2024-02-03T14:32:57Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0112,Center for American Progress,,,Comment on FR Doc # 2023-28232,"February 2, 2024<br/><br/>Laurie E. Locascio<br/>Director of the National Institute of Standards and Technology (NIST) and <br/>Under Secretary of Commerce for Standards and Technology<br/>100 Bureau Drive<br/>Gaithersburg, MD 20899<br/><br/>Submitted electronically via www.regulations.gov<br/><br/>Re: Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11); Docket Number: 231218&ndash;0309, December 21, 2023<br/><br/>The development of artificial intelligence (AI), and in particular the public availability of generative AI and the widespread ability to automatically generate images, audio, and video, is already having a significant impact on society. The development and deployment of generative AI is happening at a speed and scale that is likely to exceed previous technological deployments.  Unfortunately, this rapid deployment has meant the risk management and trust and safety features that would traditionally have time to develop do not currently exist.  <br/><br/>On October 30, 2023, President Biden signed the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence  (AI EO) which directed NIST to undertake several tasks including &ldquo;developing a companion resource to the AI Risk Management Framework, NIST AI 100-1, for generative AI.&rdquo; <br/><br/>As CAP has previously stated, voluntary risk management frameworks are not a sufficient substitute for needed AI regulations and legislation.  Scholars have noted the shortcoming of a risk management framing for AI. <br/><br/>However, as AI legislation or regulation faces an uphill battle in the United States in the immediate future, voluntary frameworks like the NIST AI Risk Management Framework (NIST AI RMF)  can be a first step in helping to identify and potentially mitigate harms from Generative AI. As NIST carries out the mission assigned to it by the AI EO, the NIST AI RMF generative AI companion and any updated NIST AI RMF should: <br/>- Incorporate the White House Blueprint for an AI Bill of Rights  (AI Bill of Rights).<br/>-Define and include requirements for the responsibilities and risk management for developers of AI models and first- and third-party deployers of those AI models.  <br/>- Adopt the categories from the draft OMB AI guidance where AI use is presumed to be Safety-Impacting or Rights-Impacting  and craft risk mitigations for these categories.<br/>- Prioritize recommendations to address generative AI&rsquo;s risks to the integrity of elections and democratic processes given the historic number of elections taking place in 2024. <br/><br/>Below CAP provides the following response to the Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11); Docket Number: 231218&ndash;0309, December 21, 2023. Please contact Adam Conner (aconner@americanprogress.org: 202-669-5671) with any questions.<br/><br/>Sincerely,<br/><br/>The Center for American Progress<br/><br/>Adam Conner<br/>Vice President, Technology Policy, The Center for American Progress<br/><br/>Megan Shahi<br/>Director, Technology Policy, The Center for American Progress <br/>","[('CAP_NIST_GenerativeAIRFIComment_02.02.2024', 'https://downloads.regulations.gov/NIST-2023-0009-0112/attachment_1.pdf'), ('GenerativeAI-report', 'https://downloads.regulations.gov/NIST-2023-0009-0112/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0112
comment,2024-02-01T17:11:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0020,CIfAI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('CIfAI_Response _to_RFI_Related_to_NISTs_Assignmernts_01312024', 'https://downloads.regulations.gov/NIST-2023-0009-0020/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0020
comment,2024-02-01T17:03:49Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0008,SAE International,,,Comment on FR Doc # 2023-28232,Please find attached the response from SAE International,"[('SAE response to NIST RFI January 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0008/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0008
comment,2024-02-01T17:12:19Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0022,Federation of American Scientists,,,Comment on FR Doc # 2023-28232,See attached file(s).,"[('Public Comment NIST-2023-0009-0001', 'https://downloads.regulations.gov/NIST-2023-0009-0022/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0022
comment,2024-02-01T17:13:38Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0025,,,Byrd,Comment on FR Doc # 2023-28232,See attached file(s),"[('Making Artificial Intelligence Transparent and Trustworthy_NIST Feb 2024 RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0025/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0025
comment,2024-02-01T17:13:57Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0026,Institute for AI Policy and Strategy,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('2024.02-NIST-2023-0009-0001', 'https://downloads.regulations.gov/NIST-2023-0009-0026/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0026
comment,2024-02-01T17:06:15Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0015,,,Noack,Comment on FR Doc # 2023-28232,I think Bayesian uncertainty quantification has not gotten the attention it deserves in Machine Learning and AI. One can simply not make reliable and safe decisions without realistic uncertainties. It is a mistake to rely on ensemble methods to create distributions. I am happy to explain more if this is of interest.  ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0015
comment,2024-02-01T17:00:34Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0003,,,Anonymous,Comment on FR Doc # 2023-28232,"Developing a companion resource to the AI Risk Management Framework (AI RMF) for generative AI involves addressing several critical areas. Based on the non-exhaustive list of possible topics provided, here are key points and recommendations for each area:<br/><br/>Risks and Harms of Generative AI<br/>Mapping and Managing Trustworthiness: Establish methodologies for assessing trustworthiness in generative AI, including transparency, reliability, and fairness metrics.<br/>Addressing Specific Harms: Develop guidelines to mitigate risks such as repression, interference with democratic processes, gender-based violence, and human rights abuses. This may include strict protocols for content generation and distribution, as well as monitoring mechanisms.<br/>Standards, Norms, and Practices<br/>Existing Standards and Gaps: Evaluate current standards and identify gaps, particularly in governance, risk mapping, measurement, and management specific to generative AI.<br/>Industry Best Practices: Document and promote industry best practices, encouraging standardization across different sectors utilizing generative AI.<br/>Governance Practices<br/>Recommendations for AI Actors: Suggest updates in governance models to include risk assessment procedures specific to generative AI, emphasizing ethical considerations and stakeholder impacts.<br/>Inclusive Governance Models: Encourage inclusive models that incorporate feedback from diverse stakeholders, including marginalized and impacted communities.<br/>Professions, Skills, and Disciplinary Expertise<br/>Interdisciplinary Teams: Advocate for the formation of interdisciplinary teams comprising experts in AI, ethics, law, sociology, and other relevant fields.<br/>Roles and Responsibilities: Define clear roles and responsibilities for various team members in managing and governing generative AI systems.<br/>Roles of Different AI Actors<br/>Developers, Deployers, and End Users: Outline distinct roles and responsibilities for developers, deployers, and end users, ensuring each group contributes to risk management.<br/>Collaborative Frameworks: Encourage collaborative frameworks where different actors work together to identify and mitigate risks.<br/>Techniques and Implementations<br/>Model Validation and Verification: Develop robust model validation and verification protocols, including red-teaming exercises tailored to generative AI.<br/>Impact Assessments: Implement comprehensive human rights and ethical impact assessments as standard practice in generative AI development.<br/>Content Authentication and Provenance Tracking: Promote the development and use of technologies for content authentication, provenance tracking, and synthetic content detection.<br/>Effectiveness Assessment Mechanisms: Establish mechanisms to assess and verify the effectiveness of implemented techniques, ensuring they are scalable and fit for purpose.<br/>Additional Considerations<br/>Continuous Monitoring and Adaptation: Emphasize the importance of ongoing monitoring and adaptation of strategies as generative AI technologies evolve.<br/>Stakeholder Engagement: Foster a culture of continuous engagement with various stakeholders, including policymakers, civil society, and the general public.<br/>Public Awareness and Education: Develop public awareness campaigns and educational programs to inform about the potentials and risks of generative AI.<br/>By addressing these areas comprehensively, the companion resource to the AI RMF for generative AI can provide valuable guidance in managing the unique challenges posed by these technologies, ensuring their development and deployment are aligned with ethical principles and societal values.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0003
comment,2024-02-01T17:05:10Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0011,,,Johnson-Glenberg,Comment on FR Doc # 2023-28232,"Hello,<br/>I am a professor and Cognitive Scientist who has built NLP models in the past. I am excited that the White House is getting serious about governace for AI. I understand what is needed and want to say that I strongly  support watermarking on text and visual content. Also, I support stricter adherence to and compliance with data scraping rules. Training sets for these GenAI models shoud only use content that they are allowed to use. There is a way for companies to adhere to these restrictions,  though it is cumbersome. Additionally, the pubic should have easy to access searchable lists of the training sources (websites, libraries, etc.) that were used in the models&#39; creation. As a female scientist, I am sensitive to the biases in these systems and think there should also be separate commitees dedicated to oversight on how the large companies are trying to combat the biases and prejudices that will occur any system that uses unfiltered web content for its training. I am the co-Chair of the GenAI Committee at Arizona State University - and I would be willing to serve on a government committee dedicated to AI oversight. This is a critical time for setting AI policy in America. Please see contact information below.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0011
comment,2024-02-01T17:11:52Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0021,,,Tiwari,Comment on FR Doc # 2023-28232,"Quantifying compliance with the AI RMF for each of the four identified categories is essential. This will help organizations achieve regulatory compliance, identify loopholes, and predict the state of their current data management using AI models.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0021
comment,2024-02-01T22:28:56Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0037,Chamber of Progress,,,Comment on FR Doc # 2023-28232,Please see the attached comments of the Chamber of Progress on Section 11 of the Executive Order Concerning Artificial Intelligence.,"[('Chamber of Progress comments NIST RFI - EO-AI 2_2', 'https://downloads.regulations.gov/NIST-2023-0009-0037/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0037
comment,2024-02-01T22:29:23Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0039,,,LeFluer,Comment on FR Doc # 2023-28232,I  Support   Everything   On  These  Issues ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0039
comment,2024-02-02T13:59:27Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0046,Confidential Computing Consortium,,,Comment on FR Doc # 2023-28232,"We are submitting comments on behalf of the Confidential Computing Consortium, Linux Foundation. Please review and consider, thank you!","[('NIST20230309commentsCCC', 'https://downloads.regulations.gov/NIST-2023-0009-0046/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0046
comment,2024-02-02T14:09:55Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0072,,,Santana,Comment on FR Doc # 2023-28232,"Generative &quot;A.I&quot;, and I use that term loosely because the current iteration of it is not intelligent by any sort of actual standards, should highly regulated. Generative &quot;a.i.&quot; is little more than auto photomanipulation. It requires images to mash together. Otherwise it can create nothing. <br/>Furthermore, the advent of this technology has only helped deepfakes thrive and has ruined the lives many artists, and innocent &quot;bystanders&quot; with generated images of them in sexually explicit acts (from the famous Taylor swift to the person you don&#39;t know at a local school.) The reins must be pulled on this technology and fast. <br/>Artificial intelligence as it applies to things like finding cancer, searching through data, sequencing dna and other such applications that will actually HELP humanity, society flourish should be the goal. Not stealing jobs from hard working artists to then turn around and use it to harass, intimidate, and otherwise terrorize society with the threat of fake porn. <br/>I don&#39;t know that I covered everything, or that I even said it well enough to get the message across on the kind of threat this now poses, but this finds someone who can see these generative &quot;a.i&quot; models for what they are and will put a stop to such technologies. It is not innovative, its exploitative. It cannot create without input, therefore what it regurgitates is not what the developers say it is. <br/>I hope you will what&#39;s right and shut these applications down. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0072
comment,2024-02-02T14:05:43Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0057,,,Plant,Comment on FR Doc # 2023-28232,"These tools are using data they have no right to.<br/>It is theft.<br/>It will severely impact artists and designers and other workers.<br/>It must be tightly regulated and artists should be well compensated.<br/><br/>There must be total transparency what data they are using.<br/><br/>This has huge implications for the concept of copyright full stop.<br/>If individuals do not have copyright of their creations/writings,<br/>why will people bother creating anything.<br/><br/>Without the concept of ownership, society will break down.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0057
comment,2024-02-02T14:04:27Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0053,,,Bessette,Comment on FR Doc # 2023-28232,"There has been much outcry from artists, photographers, and internet users in general over the fact that most generative AI models were trained using data such as artwork and writing scraped from the internet without the permissions of the original creators, which in some cases would constitute as misuse of copyrighted materials. While some databases allow certain creators to &quot;opt-out&quot; their work from previous and further training, almost all of them, myself included, believe that the more ethical option would be for AI companies to require explicit permission from authors and individuals before their data is used in AI model training, in addition to being able to opt-out if any of their work was used previously.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0053
comment,2024-02-02T14:06:18Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0060,,,Anonymous,Comment on FR Doc # 2023-28232,Ai is theft and the whole purpose is to steal from the people it wishes to replace. The deepfake situation with it is horrendous it is also a cp generator due to having that content in its data sets. You will find the working class is against this destructive thing. And only these Ai companies who are trying to get away with theft are keen for Ai as it would benefit them but the law is coming,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0060
comment,2024-02-02T14:06:41Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0062,,,Montgomery,Comment on FR Doc # 2023-28232,i think that the use of a dataset that includes copyright  data should be forbidden for any commercial use without the licensing of that copyrighted data that was used in the data set. ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0062
comment,2024-02-02T14:07:56Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0065,,,Dauber,Comment on FR Doc # 2023-28232,"Hi, I think that AI as it is existing in the world right now is not a safe system for our world as it for one has relied on stealing from artists in a way that cannot be undone and the damage cause by this is not estimable.  <br/>And secondly, the effects on the economy are also too dangerous to ignore.  The amount of jobs potentially at risk are beyond the thresh hold that that risk can be taken.  <br/>There as well are the other factors like the child pornographic materials that may be in the system that present it&#39;s own issues.  <br/>That this AI was created in a bad way cannot be taken back and thus as long as it is existing off of stolen works it will be too harmful to continue.  In addition as long as there are such a significant number of jobs at risk, that shake up to the economy is too risky also.  As the welfare of the everyday people is what we as a society ought to be measuring our choices on.  And this is not good for everyday people and thus should have significant things done to protect the people from this technology.<br/>Thank you for reading this.  ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0065
comment,2024-02-02T14:08:58Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0069,,,Steen,Comment on FR Doc # 2023-28232,"I believe it to be absolutely necessary to provide legal restrictions in regards to the usage of images, especially those copyrighted, including, but not limited to, artwork done by individuals. Without proper methods of regulating what used in datasets of generative AI models, there will be a constant issue in regards to employment in the entertainment industry, whereby individuals not only have their work used without compensation, consent, or even knowledge, but will be unable to find proper employment in their respective fields because of it, there by potentially causing a spike in unemployment and dealing a severe blow to the economy.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0069
comment,2024-02-02T14:10:13Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0073,,,Anonymous,Comment on FR Doc # 2023-28232,"Transparency of datasets is very much needed. All parties must be able to and allowed to identify such data with a full disclosure of the data used.<br/><br/>Data must be licensed with consent from the original creator/ license holder. If data is used without permission, then adequate compensation should be necessary, or have the data destroyed. It is unfair for rightsholder data to be used without permission and/ or compensation to gain financial rewards with use commercially.<br/><br/>There should be better regulations and a good framework so that the original owners of said data are not caused any suffering or inconvenienced in any way.<br/><br/>There are incredibly disturbing and problematic uses of AI where laws and regulations need to be made as soon as possible to protect the public. These are not limited to but include the use of AI to create deepfake porn of minors and unconsenting individuals, toxic and harmful spreading of misinformation, using artwork without consent as training data and harming the original artists/ copyright holders.<br/><br/>Better regulations are very much needed as soon as possible.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0073
comment,2024-02-02T14:05:02Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0055,,,Gee,Comment on FR Doc # 2023-28232,"I am an artist working in the video games industry. The advent of generative AI as it currently stands is rife with legal and moral quandaries that absolutely must be addressed on a systemic level. Datasets these models are trained on must be made transparent, parties must be allowed to identify their data, and ask for licensing payment or destruction of data if used without permission or compensation. In addition, a stronger framework should exist to not put the burden on the owners of said data to meticulously comb each AI dataset for their own work. Additionally, this does not even touch upon the ability of generative AI to create images depicting real world individuals (including minors) in morally compromised or pornographic situations that are difficult to discern from reality. In essence, unregulated generative AI is a potentially infinite wellspring of misinformation and harm. <br/><br/>Under the current framework, AI companies are free to scrape the internet for any and all images regardless of copyright or ownership concern and incorporate them into their databases. Without any kind of regulation, I believe generative AI will have a deleterious effect on creative industries, which are responsible for all the media we engage with and enjoy. Beyond its effect on creative fields, the effect it will have on individuals who have any photos of themselves posted on the internet cannot be ignored. In this modern age it is increasingly difficult to not have some kind of digital footprint. I fear a world in which malicious parties can generate and manipulate images to depict any individual in any conceivable situation without recourse or consequence. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0055
comment,2024-02-02T20:07:29Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0074,MITRE,,,Comment on FR Doc # 2023-28232,MITRE&#39;s response to the subject RFI is attached.  Please let me know if you have any questions or if we can help you in any other way.,"[('MITRE NIST AI Implementation Response', 'https://downloads.regulations.gov/NIST-2023-0009-0074/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0074
comment,2024-02-03T14:30:48Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0107,"Two Six Labs, LLC dba Two Six Technologies",,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST EO14110 RFI response', 'https://downloads.regulations.gov/NIST-2023-0009-0107/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0107
comment,2024-02-03T14:39:36Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0146,Institute for Trustworthy AI in Law and Society,,,Comment on FR Doc # 2023-28232,"The Institute for Trustworthy AI in Law and Society, a partnership between the University of Maryland, George Washington University, Morgan State University, and Cornell University, is pleased to submit the attached response to the Request for Information (RFI) Related to NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11). ","[('NIST RFI - TRAILS Response 2024-02-02', 'https://downloads.regulations.gov/NIST-2023-0009-0146/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0146
comment,2024-02-03T14:44:54Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0172,SaferAI,,,Comment on FR Doc # 2023-28232,"This is SaferAI&#39;s response the NIST RFI 88 FR 88368. <br/><br/>This document outlines a comprehensive framework designed to manage the risks of<br/>general-purpose artificial intelligence systems (GPAIS) effectively. The architecture<br/>is inspired by high-risk industrial practices and aims to encompass a wide range<br/>of concerns, from safety culture to infosecurity to governance to safety by design.<br/>","[('2024_02_02_NIST_RFI_SaferAI', 'https://downloads.regulations.gov/NIST-2023-0009-0172/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0172
comment,2024-02-03T14:48:00Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0185,Computer and Communications Industry Association,,,Comment on FR Doc # 2023-28232,Attached please find the comments of the Computer and Communications Industry Association.,"[('2024-02-02 ARTIFICIAL INTELLIGENCE - CCIA Comments to NIST on AI RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0185/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0185
comment,2024-02-03T14:31:14Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0109,USTelecom,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('USTelecom NIST AI RFI 2.2.24', 'https://downloads.regulations.gov/NIST-2023-0009-0109/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0109
comment,2024-02-03T14:47:40Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0183,Verance Corporation,,,Comment on FR Doc # 2023-28232,See attached file.,"[('Verance NIST AI EO RFI 240202 LH final', 'https://downloads.regulations.gov/NIST-2023-0009-0183/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0183
comment,2024-02-03T14:37:23Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0132,Software & Information Industry Association,,,Comment on FR Doc # 2023-28232,Please find attached comments of the Software &amp; Information Industry Association.,"[('SIIA Submission on NIST AI RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0132/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0132
comment,2024-02-03T14:28:02Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0095,Holistic AI,,,Comment on FR Doc # 2023-28232,"Please find attached Holistic AI&#39;s Response on NIST&rsquo;s execution of its responsibilities under Executive Order 14110 on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. <br/><br/>Regards, <br/>Siddhant Chatterjee<br/>Policy and Governance Strategist, Holistic AI<br/><br/>publicpolicy@holisticai.com/ siddhant.chatterjee@holisticai.com","[('Holistic AI Response to NIST RFI on the Biden Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0095/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0095
comment,2024-02-03T14:36:05Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0126,Hacking Policy Council,,,Comment on FR Doc # 2023-28232,"Please find attached the comments of the Hacking Policy Council. Thank you. <br/><br/>Harley Geiger<br/>Counsel, Venable LLP<br/>Coordinator, Hacking Policy Council<br/><br/>","[('Hacking Policy Council - comments to NIST re AI red teaming - 20240202', 'https://downloads.regulations.gov/NIST-2023-0009-0126/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0126
comment,2024-02-03T14:29:18Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0099,Center for Data Innovation,,,Comment on FR Doc # 2023-28232,"Thank you for the opportunity to provide comments. Please find attached comments submitted on behalf of the Center for Data Innovation.<br/><br/>Best regards,<br/>Hodan Omaar","[('NIST AI EO RFI Center for Data Innovation', 'https://downloads.regulations.gov/NIST-2023-0009-0099/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0099
comment,2024-02-03T14:45:59Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0178,Center for AI Safety,,,Comment on FR Doc # 2023-28232,See attached file with our response to the RFI.,"[('Center for AI Safety - Response to NIST RFI Related to the AI Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0178/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0178
comment,2024-02-03T14:23:11Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0080,Center for Democracy & Technology,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('CDT Comments to NIST', 'https://downloads.regulations.gov/NIST-2023-0009-0080/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0080
comment,2024-02-03T14:45:04Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0173,,,Wasil,Comment on FR Doc # 2023-28232,Please see the attached files.,"[('NIST RFI Response_Affirmative Safety', 'https://downloads.regulations.gov/NIST-2023-0009-0173/attachment_1.pdf'), ('Affirmative Evidence of Safety_Draft Report', 'https://downloads.regulations.gov/NIST-2023-0009-0173/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0173
comment,2024-02-03T14:26:49Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0090,Federation of American Hospitals,,,Comment on FR Doc # 2023-28232,The Federation of American Hospitals appreciates the opportunity to provide comments to NIST on the AI RFI.,"[('NIST AI RFI 2124 FINAL', 'https://downloads.regulations.gov/NIST-2023-0009-0090/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0090
comment,2024-02-03T14:35:12Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0120,MLCommons,,,Comment on FR Doc # 2023-28232,Attached comments submitted on behalf of MLCommons ,"[('MLCommons submission to NIST RFI Feb-24', 'https://downloads.regulations.gov/NIST-2023-0009-0120/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0120
comment,2024-02-03T14:35:36Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0123,NVIDIA,,,Comment on FR Doc # 2023-28232,"Thank you for the opportunity to provide comments on behalf of NVIDIA Corporation pursuant to Request for Information #NIST&ndash;2023&ndash;0009 Related to NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence. We appreciate the Department of Commerce&rsquo;s work in promoting innovation and equitable standards that foster a competitive industrial landscape. NVIDIA is committed to safe and trustworthy AI, in line with the White House Voluntary Commitments and other global AI Safety initiatives and are committed to helping to drive standards around the development and deployment of safe and trustworthy AI.","[('NVIDIA Response to RFI NIST-2023-0009', 'https://downloads.regulations.gov/NIST-2023-0009-0123/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0123
comment,2024-02-03T14:49:01Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0194,WITNESS,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('WITNESS_NIST Request for Information Executive Order Artificial Intelligence', 'https://downloads.regulations.gov/NIST-2023-0009-0194/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0194
comment,2024-02-03T14:40:20Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0150,ModelCard.ai,,,Comment on FR Doc # 2023-28232,Attached please find comments from ModelCard.ai.,"[('Attachment 1_ModelCardai Response', 'https://downloads.regulations.gov/NIST-2023-0009-0150/attachment_1.pdf'), ('Attachment 2_ModelCardai Academic Paper 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0150/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0150
comment,2024-02-03T14:48:43Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0191,Responsible Innovation Labs,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST RFI 231218-0309_Responsible Innovation Labs', 'https://downloads.regulations.gov/NIST-2023-0009-0191/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0191
comment,2024-02-03T14:36:11Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0127,Scale AI,,,Comment on FR Doc # 2023-28232,Scale AI&#39;s comment can be found in the attached file.,"[('Scale-NIST EO RFI Comment_final', 'https://downloads.regulations.gov/NIST-2023-0009-0127/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0127
comment,2024-02-03T14:41:49Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0158,Mirza AI,,,Comment on FR Doc # 2023-28232,Our comments are in the enclosed file.,"[('Request for Information (RFI) related to Executive Order 14110', 'https://downloads.regulations.gov/NIST-2023-0009-0158/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0158
comment,2024-02-03T14:43:12Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0165,Center for AI and Digitla Policy,,,Comment on FR Doc # 2023-28232,"From the Center for AI and Digital Policy (&ldquo;CAIDP&rdquo;) in response to the request for information  (&ldquo;RFI&rdquo;) issued by the National Institute of Standards and Technology (&ldquo;NIST&rdquo;) relating to NIST&rsquo;s assignments under section 4.1, 4.5, and 11 of President Biden&rsquo;s Executive Order concerning Artificial Intelligence (hereafter &ldquo;EO 14110&rdquo;).  Our overarching recommendations to NIST to fulfill its mandate under EO 14110 are as follows: <br/><br/>1. Use a human-rights based approach to AI governance <br/>2. Clearly recommend and establish the obligation to terminate <br/>3. Recommend human rights impact assessments <br/>4. Recommend continual and open monitoring and contestability of outcomes<br/>5. Recommend &ldquo;no-go&rdquo; decisions for pseudo-scientific and human rights violating systems<br/>6. Recommend transparency mechanisms including &lsquo;white-box&rsquo; and &lsquo;out-of-the box&rsquo; auditor access, use of public model cards, training set documentation<br/>7. Intended use cases and exclusion criteria should be aligned with OMB&rsquo;s guidance on &ldquo;rights-impacting&rdquo; and &ldquo;safety-impacting&rdquo; AI systems<br/><br/>In the Comments below, we provide the following recommendations in response to the specific issues identified in the RFI:<br/><br/>1.<span style='padding-left: 30px'></span>On the &ldquo;Best practices regarding data capture, processing, protection, quality, privacy, transparency, confidentiality, handling, and analysis, as well as inclusivity, fairness, accountability, and representativeness (including non-discrimination, representation of lower resourced languages, and the need for data to reflect freedom of expression) in the collection and use of data;&rdquo; [3.a]: NIST must account for the lack of underrepresented groups in AI development and how their absence is inconsistent with existing laws and international best practices on inclusivity and representation in AI. Best-practices and standards should guide development of AI tools/systems which are traceable, contestable, and contribute to accountability by their design.<br/><br/>2.<span style='padding-left: 30px'></span>On &ldquo;[how to develop standards for] AI risk management and governance, including managing potential risk and harms to people, organizations, and ecosystems&rdquo; [3.a]: NIST should account for the growing environmental implications of the AI supply chain as a part of its standard development process and consider existing obligations towards reducing carbon emissions and other environmental harms arising out of the development of AI systems. <br/><br/>3.<span style='padding-left: 30px'></span>On the &ldquo;Risks and harms of generative AI, including challenges in mapping, measuring, and managing trustworthiness characteristics as defined in the AI RMF, as well as harms related to repression, interference with democratic processes and institutions, gender-based violence, and human rights abuses&rdquo; [1.a.1]: we present a catalog of risks, the urgency in addressing them, and supply recommendations of standards that advance a rights-based approach to AI governance.<br/><br/>4.<span style='padding-left: 30px'></span>On the &ldquo;Forms of transparency and documentation (e.g., model cards, data cards, system cards, benchmarking results, impact assessments, or other kinds of transparency reports) that are more or less helpful for various risk management purposes&rdquo; [1.a.1]: we urge NIST to require ex-ante human rights impact assessments, advance a transparency and disclosure framework, and continue an open public comment process.<br/><br/>We are conscious that NIST&rsquo;s framework is not mandatory and does not create enforceable rights and obligations for private actors. However, NIST must be mindful that the approach and elements of its risk management framework will inform ongoing discussions on regulation and should be human-centered, designed to proactively set a robust floor for transparency, accountability, safety, and fairness obligations for the industry. <br/><br/>Our attached file provides details of these recommendations in response to the specific questions/items in the RFI.","[('CAIDP Comments_NIST RFI_88 FR 88368_02022024', 'https://downloads.regulations.gov/NIST-2023-0009-0165/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0165
comment,2024-02-03T14:34:25Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0115,U.S. Chamber of Commerce,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('240202_Comments_RFIArtificialIntelligence_NIST', 'https://downloads.regulations.gov/NIST-2023-0009-0115/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0115
comment,2024-02-03T14:38:51Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0143,TechNet,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('FINAL TechNet Response to NIST RFI on AI Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0143/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0143
comment,2024-02-03T14:37:07Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0130,"Palo Alto Networks, Inc.",,,Comment on FR Doc # 2023-28232,"Palo Alto Networks, Inc. appreciates the opportunity to provide comments in response to the NIST Request for Information related to the NIST Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence, NIST&ndash;2023&ndash;0309.","[('Palo Alto Networks Final Comments - NIST RFI on AI Security Feb 2', 'https://downloads.regulations.gov/NIST-2023-0009-0130/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0130
comment,2024-02-03T14:43:33Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0168,BigBear.ai,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('BigBear. NIST Public Comment Letter. FINAL_2Feb24', 'https://downloads.regulations.gov/NIST-2023-0009-0168/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0168
comment,2024-02-03T14:34:46Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0117,Amazon,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Amazon NIST RFI Response 02022024', 'https://downloads.regulations.gov/NIST-2023-0009-0117/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0117
comment,2024-02-03T14:47:27Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0181,Intel Corporation,,,Comment on FR Doc # 2023-28232,Intel Corporation submission for NIST AI Executive Order,"[('NIST AI EO RFI_Feb 2 2024_Intel', 'https://downloads.regulations.gov/NIST-2023-0009-0181/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0181
comment,2024-02-03T14:49:11Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0196,Connected Health Initiative,,,Comment on FR Doc # 2023-28232,See attached for comments of the Connected Health Initiative,"[('CHI Comment re NIST AI EO Activities Sec 4 Sec 11 (2 Feb 2024) (w appendices)', 'https://downloads.regulations.gov/NIST-2023-0009-0196/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0196
comment,2024-02-03T14:40:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0152,Consumers' Research,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Consumers Research Comment - NIST AI', 'https://downloads.regulations.gov/NIST-2023-0009-0152/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0152
comment,2024-02-03T14:24:33Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0084,American Bankers Association,,,Comment on FR Doc # 2023-28232,"The American Bankers Association (ABA) appreciates the opportunity to comment on the request for information (RFI) related to National Institute of Standards and Technology&rsquo;s (NIST) assignments under sections 4.1, 4.5 and 11 of the October 30, 2023 Executive Order on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence as published in the Federal Register on December 21, 2023 (EO). NIST is uniquely positioned to build upon the success of its AI Risk Management Framework (AI RMF) by developing a complement to address the unique opportunities and challenges presented by generative AI (GenAI) and prompt-based large language models (LLMs). <br/>In our attached comment letter, the ABA: <br/><br/>- Encourages NIST to consider the banking industry&rsquo;s approach as a possible solution for other industries given that banks are at the forefront of the responsible AI movement due to the mature and flexible risk management framework at its core, which is subject to oversight by industry-focused regulatory agencies; <br/><br/>- Requests NIST to develop voluntary standards in harmony with regulatory requirements given that financial institutions are subject to numerous regulatory requirements with respect to the use of AI, including third party risk management, model risk management and cybersecurity;<br/><br/>- Requests NIST to clarify what outputs need to be subject to AI governance;<br/><br/>- Encourages NIST to define &ldquo;red-teaming&rdquo; given that the term is used in many ways and lack of clarity could raise concerns; <br/><br/>- Applauds efforts to reduce the risk of synthetic content given GenAI can generate synthetic content leading to the creation of deepfakes that can be used to perpetrate fraud and causing mis- and dis-information but also encourages NIST to recognize the positive uses of synthetic data to advance privacy and trade secrets protection and ensure it does not inadvertently prevent its development; and <br/><br/>- Generally supports efforts to advance global technical standards that accommodate the risk management needs (including regulatory compliance requirements), that are technology neutral, risk-based, and tailored to use cases.  <br/>","[('ABA Letter to NIST on Artificial Intelligence', 'https://downloads.regulations.gov/NIST-2023-0009-0084/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0084
comment,2024-02-03T14:50:11Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0202,ACT | The App Association,,,Comment on FR Doc # 2023-28232,See attached for comments of ACT | The App Association,"[('ACT Comment re NIST AI EO Activities Sec 4 Sec 11 Efforts (2 Feb 2024) (w appendix)', 'https://downloads.regulations.gov/NIST-2023-0009-0202/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0202
comment,2024-02-03T14:39:28Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0145,"Standards Michigan, LLC (www.standardsmichigan.com)",,,Comment on FR Doc # 2023-28232,"Standards Michigan Group<br/>2723 South State Street | Suite 150<br/>Ann Arbor, MI | 48104<br/>888-748-3670<br/><br/>To: Ms. Rachel Trello, National Institute of Standards and Technology<br/><br/>RE: NIST 2023-0309: Bias in Training Data. Recommend metrics.<br/><br/>February 2, 2024<br/><br/>Dear Ms.Trello:<br/><br/>Thank you for the opportunity to comment.  Standards Michigan has been involved in standards of all types &ndash; ad hoc, consortia, open, voluntary, de facto &ndash; for over 30 years.  See our ABOUT.<br/><br/>Our resources limit our recommendations to only one&ndash;a peak concern&ndash; which originates from our work in academic literature.  Because so much of the consulting information (training data) comes from the written word&ndash;and much of that in academic precincts&ndash;we expect that algorithmic bias we already see in AI output will accelerate.  A great deal of intelligence happens with action, not words.  <br/><br/>Perhaps NIST will need to develop new &ldquo;units of measurement&rdquo; to rise to its challenge.  AI metrics could be a simple number, for example, or a vector space reflecting the bias to the user&ndash; where appropriate &ndash;if the bias (or one-sidedness) cannot be eliminated.<br/><br/>A reference to some relatively new numerical indices will enlighten my suggestion. In my profession, for example, we have reliability indices (See: IEEE 1366 Guide for Electrical Power Distribution Reliability Indices).  These have proven helpful in the fullness of time.  <br/><br/>AI will have many applications in its journey along the classic &ldquo;Rogers Curve&rdquo;  Metrics could be formulated to indicate the sources, even if it means slowing down the output in non-critical queries  Much of the success in the American experiment in democracy is owed to constructive debate on all sides of an issue.<br/><br/>This is a worthy project and I wish you every conceivable success..<br/> <br/>Very truly yours.<br/><br/>Michael A Anthony, P.E.<br/><br/>&quot;Language is the only homeland.&quot; -- Czesław Miłosz (Nobel Laureate, 1980)<br/><br/>www.standardsmichigan.com<br/><br/>","[('NIST AI Lette', 'https://downloads.regulations.gov/NIST-2023-0009-0145/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0145
comment,2024-02-03T14:34:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0116,Entertainment Software Association,,,Comment on FR Doc # 2023-28232,Attached are the comments of the Entertainment Software Association.,"[('Comments of the Entertainment Software Association on the NIST Synthetic Comment RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0116/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0116
comment,2024-02-03T14:27:45Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0093,Institute for Security and Technology,,,Comment on FR Doc # 2023-28232,See attached file.,"[('IST Comment on NIST AI RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0093/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0093
comment,2024-02-03T14:26:35Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0089,HackerOne Inc.,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('HackerOne Comments to NIST AI RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0089/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0089
comment,2024-02-03T14:47:34Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0182,AFL-CIO Technology Institute,,,Comment on FR Doc # 2023-28232,Please see attached comment.,"[('AFL-CIO Tech Institute - NIST AI RFI Comment -Feb 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0182/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0182
comment,2024-02-03T14:31:06Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0108,"A2IM, NMPA, and RIAA",,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Music Community Comments RFI NIST Assignments under EO on AI v2-2-24', 'https://downloads.regulations.gov/NIST-2023-0009-0108/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0108
comment,2024-02-03T14:22:17Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0077,,,Yadav,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST_AI', 'https://downloads.regulations.gov/NIST-2023-0009-0077/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0077
comment,2024-02-03T14:31:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0111,Securities Industry and Financial Markets Association,,,Comment on FR Doc # 2023-28232,Please see the attached letter.,"[('SIFMA NIST RFI AI Response Feb 2 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0111/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0111
comment,2024-02-03T14:23:33Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0081,AI 2030 (a FinTech4Good Initiative),,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST RFI Response_AI2030_February 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0081/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0081
comment,2024-02-03T14:37:15Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0131,Equifax,,,Comment on FR Doc # 2023-28232,Equifax appreciates the opportunity to comment on NIST&#39;s RFI on Artificial Intelligence. Please see attached comment. ,"[('NIST RFI 2023-0009 Equifax Response Feb2024', 'https://downloads.regulations.gov/NIST-2023-0009-0131/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0131
comment,2024-02-03T14:30:24Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0105,,,Del Rosairo,Comment on FR Doc # 2023-28232,See attached file(s),"[('RDELROSARIO_Comment_on_FR_Doc_2023-28232', 'https://downloads.regulations.gov/NIST-2023-0009-0105/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0105
comment,2024-02-03T14:24:25Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0083,,,Lane,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST AI Filing 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0083/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0083
comment,2024-02-03T14:29:33Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0101,,,Mitterhöfer,Comment on FR Doc # 2023-28232,"Hello, Artists, writers, voice actors, journalists etc need protection. Training on someone&rsquo;s Data needs permission. Models need to be transparent so copyrightholders find their data. Opt out is useless because the damage is already done. Models cannot forget. Deepfakes need to be a crime. Current Models should be destroyed and trained from zero with consent, credit, compensation and transparency. Thank you kind regards.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0101
comment,2024-02-03T14:23:52Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0082,Greenpeace USA,,,Comment on FR Doc # 2023-28232,Comments on behalf of Greenpeace USA are attached.<br/><br/>,"[('Comments to NIST on AI Exec Order', 'https://downloads.regulations.gov/NIST-2023-0009-0082/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0082
comment,2024-02-03T14:31:22Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0110,,,Crane,Comment on FR Doc # 2023-28232,"As you review the issue, it is my hope that you not only consider the potential benefits of generative AI and other inevitable permutations coming in the near future, but also the potentially negative effect of generative AI on the people who produce the work that the genAI companies mine to train their AI models. <br/><br/>I also hope you do not focus your questions primarily on technical experts in the field, but also on the people most affected by the rise of genAI, the people most likely to see their livelihoods reduced and lives changed for the worse. If genAI is to flourish, consider the idea that those who created the work the AI companies have mined be fairly compensated for what the AI companies have taken from them. I can well envision a report issued from any generated image or writing that lists all sources for that created bit of content, along with what percentage of the generated work came from each source. Compensation could be fairly allocated based on those percentages. I believe there&#39;s a way artists and writers and musicians and other impacted by genAI can coexist and even thrive in a world where genAI is prevalent. But it will require your thoughtful, fair-minded consideration of the issues, and appropriate regulation.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0110
comment,2024-02-03T14:40:33Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0151,,,Reese,Comment on FR Doc # 2023-28232,"The inaction regarding gen a/i has already caused a lot of preventable damage and continues to do so everyday. To think that websites hosting voice cloning and a/i models full of copyrighted material and in some cases even worse material are still permitted is astounding. Everyday life is getting more connected and tied to the internet than ever before yet machine learning models falsely labeled as &quot;a/i&quot; continue to pollute it with useless images, voice and text, all of them is just a google search away. This is only a small section of all the problems it creates, see https://t.co/WjgDqeKMGr As to what to do has always been crystal clear, algorithmic disgorgement of current copyrighted material stealing, privacy violating, and personal information riddled models. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0151
comment,2024-02-03T14:42:27Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0161,,,Anonymous,Comment on FR Doc # 2023-28232,"Good day,<br/><br/>I&#39;d like to please make a comment regarding the regulation of generative AI.<br/><br/>In order to preserve democracy and human dignity, <br/>AI must be regulated and companies abusing AI must be aggressively and steeply penalized, anti-trust tools must be utilized to halt the onslaught of human expression.<br/><br/>AI composed messages must not be allowed to proliferate in human spaces were they risk drowning out all form of real human to human communication.<br/><br/>AI Deep-fake imagery of human beings must be criminalized and not allowed in any capacity, commercial and private.<br/><br/>Generative AI must be banned from producing content in the creative market; movies, video games, books, comics, visual arts, music etc. Both commercial and private use of AI to produce creative products should be banned. <br/><br/>Businesses using AI must be harshly punished if training on data without permission by copyright holder.<br/><br/>Generative AI is predominantly built on copyrighted material obtained without permission.  It cannot be allowed to be legally used in its current state while it&#39;s very existence requires it to learn from stolen data. <br/>These AIs and their resulting products must be destroyed and companies fined.<br/><br/>Companies must receive permission from original copyright holder first before using any data for AI training and all data used must be stored in an easily searchable online public database to make it easier for copyright holders to search for infractions against their rights. It shall be illegal for AI companies to use data obtained from piracy sites or social media sites where illegal copies of files, modified files with intentionally obfuscated origin may be present without copyright holders knowledge.<br/><br/>The government should enact a policing body to make sure no copyrighted data without permission is present in businesses AI training data and conduct regular inspections.  <br/><br/>Businesses using AI must be legally required to have an ethics team onboard.<br/><br/>Protecting intellectual property rights ensure that creatives have the freedom to explore and express their ideas without fear of them being consumed by AI datasets created to replace/ compete with them.<br/><br/>Ai-composed/ai-assisted text messages or images must carry an AI watermark to warn/inform users how it was made.<br/><br/>Thank you for reading!",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0161
comment,2024-02-01T17:04:46Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0010,,,Stone,Comment on FR Doc # 2023-28232,"Please see the attached file, which is a short position paper that is soon to appear in the Communications of the ACM, and that is relevant to this call.","[('Now_Later_Lasting_Ten_priorities_CACM_forthcoming', 'https://downloads.regulations.gov/NIST-2023-0009-0010/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0010
comment,2024-02-01T17:15:08Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0028,,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST_RFI_AI - CF', 'https://downloads.regulations.gov/NIST-2023-0009-0028/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0028
comment,2024-02-01T17:09:30Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0018,,,Ezell,Comment on FR Doc # 2023-28232,See attached file(s),"[('nist_comment', 'https://downloads.regulations.gov/NIST-2023-0009-0018/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0018
comment,2024-02-01T17:05:56Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0014,IEEE,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('IEEE Response to  NIST RFI on EO AI', 'https://downloads.regulations.gov/NIST-2023-0009-0014/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0014
comment,2024-02-01T17:11:20Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0019,Compsim LLC,,,Comment on FR Doc # 2023-28232,Artificial Intelligence: Who is responsible? Who cares?,"[('Who is responsible', 'https://downloads.regulations.gov/NIST-2023-0009-0019/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0019
comment,2024-02-01T17:04:05Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0009,Compsim LLC,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('General', 'https://downloads.regulations.gov/NIST-2023-0009-0009/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0009
comment,2024-02-01T17:15:21Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0029,BSA | The Software Alliance,,,Comment on FR Doc # 2023-28232,BSA | The Software Alliance submits the attached comments.,"[('BSA Comments on NIST EO RFI 1.31.24 Final', 'https://downloads.regulations.gov/NIST-2023-0009-0029/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0029
comment,2024-02-01T17:17:13Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0030,Control AI,,,Comment on FR Doc # 2023-28232,Please see the attached file.,"[('ControlAI_NIST RFI Response', 'https://downloads.regulations.gov/NIST-2023-0009-0030/attachment_1.pdf'), ('Draft_Deepfake report_Control AI', 'https://downloads.regulations.gov/NIST-2023-0009-0030/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0030
comment,2024-02-01T17:02:09Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0006,,,PALEM,Comment on FR Doc # 2023-28232,"This comment is pertaining to the best practices of AI Safety and Security.<br/><br/>The rapid advancement of AI brings immense potential for improving our lives, from healthcare and education to entertainment and business. However, this progress also demands careful consideration of safety and security. This is where psychologists play a critical role in shaping the future of AI, and their collaboration with AI developers is crucial for developing safe and responsible AI products. Here are some key reasons why:<br/><br/>1. Mitigating Human Biases in AI: AI algorithms are built on data, and human bias can easily creep into this data, leading to discriminatory or unfair outcomes. Psychologists have the expertise to identify these biases and develop methods to mitigate their impact. They can analyze the training data, the algorithms themselves, and the potential biases that users might bring to their interactions with the AI, ensuring fairer and more ethical algorithms.<br/><br/>2. Understanding Human-AI Interaction: Psychologists understand how humans perceive information, make decisions, and interact with technology. This expertise is essential for designing AI systems that are user-friendly, intuitive, and promote trust. They can help developers understand how people will react to different AI behaviors, anticipate potential misunderstandings or frustrations, and design interfaces that foster positive and productive interactions.<br/><br/>3. Addressing the Psychological Impact of AI: As AI becomes more sophisticated and integrated into our lives, it&#39;s crucial to understand its potential psychological impact. Psychologists can study the effects of AI on mental health, social dynamics, and human behavior. This knowledge can inform the development of AI systems that are mindful of these impacts and minimize potential harm, for example, ensuring AI companions are emotionally supportive or avoiding AI systems that exacerbate anxiety or social isolation.<br/><br/>4. Promoting Explainability and Transparency: A major concern with AI is the lack of transparency in its decision-making processes. This can lead to distrust and fear among users. Psychologists can advocate for explainable AI systems that users can understand, allowing for informed interaction and building trust. They can also help developers communicate the capabilities and limitations of AI effectively, managing expectations and preventing misunderstandings.<br/><br/>5. Developing Ethical Guidelines and Regulations: Designing ethical AI requires a thorough understanding of human values and social context. Psychologists can contribute to the development of ethical guidelines and regulations for AI development and use. They can help identify potential ethical pitfalls, guide discussions on acceptable and unacceptable applications of AI, and ensure that AI serves the greater good while respecting human rights and dignity.<br/><br/>Collaboration, not isolation, is key. By working together, psychologists and AI developers can create safer and more beneficial AI products for everyone. Psychologists bring their expertise in human behavior and ethics to the table, while developers provide the technical know-how to build the tools. This collaboration is essential for ensuring that AI continues to advance responsibly and ethically, maximizing its potential while minimizing its risks.<br/><br/>The future of AI is in our hands. By fostering effective collaboration between psychologists and AI developers, we can ensure that AI becomes a force for good, improving our lives while safeguarding our well-being and ethical principles.","[('Cyberpsychology for a Safe AI and Security', 'https://downloads.regulations.gov/NIST-2023-0009-0006/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0006
comment,2024-02-01T17:07:00Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0016,Synapse Partners,,,Comment on FR Doc # 2023-28232,"Black Duck is software that allows you to scan applications and container images, identify all open-source components, and detect any open-source security vulnerabilities, compliance issues, or code-quality risks. We need to develop a similar service for LLMs that will be able to automatically determine security vulnerabilities, connections to copyrighted material that should be used by the LLM, potentially even identify areas where various forms of bias are introduced in the model. Contrary to Black Duck software that can run on and analyze a completed software system, because the knowledge incorporated into LLMs is represented implicitly in the form of network architecture and weights among the nodes, the type of system advocated here will need to run while the LLM is being trained. The approach will impact negatively both the cost of building an LLM as well as the time it takes to create a fully-trained LLM.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0016
comment,2024-02-01T17:14:30Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0027,Nha Viet Institute,,,Comment on FR Doc # 2023-28232,"The concept of Artificial Intelligence currently functions as a linguistic ensnarement for social imagination, transcending its strict association with computer science terminology. Dr. Son Pham (2022), within the pages of his booklet &quot;On Artificial Intelligence: A Poetic Definition,&quot; currently accessible on Amazon and Kindle Prime, delves into the amalgamation of technology and linguistic creativity. In a pivotal section, he presents an alternative definition of AI as either the Automated Intent or the intent of automation. <br/><br/>&quot;Artificial Intelligence, a concept profound,<br/>Born of automation, in intent it&#39;s found.<br/>A synergy of minds, silicon and thought,<br/>In the dance of algorithms, a connection sought.<br/><br/>Automated intent or intent of automation,<br/>The essence distilled, in clear formation.<br/>AI, a creation of human design,<br/>A marvel that transcends the confines of time.<br/><br/>So let us ponder, with minds wide and free,<br/>The fusion of technology and humanity.<br/>In the tapestry of progress, where dreams are sown,<br/>Artificial Intelligence, a definition known.&quot;<br/><br/>My comment is to incorporate, right at the outset of the policy, a clear articulation of the social imagination surrounding the concept of AI through this poetic definition.<br/><br/>Given that AI is fundamentally driven by &quot;intent,&quot; a heightened focus on K-12 education becomes imperative. This educational stage is pivotal for nurturing the next generation of AI scientists across various fields, while also laying the essential groundwork for our democracy and ethical standards.<br/><br/>The establishment of safe, secure, and trustworthy AI systems hinges on the bedrock of democracy and ethical standards. It is therefore paramount to cultivate a collective understanding among policymakers and other stakeholders that AI fundamentally operates as an &quot;automated intent.&quot; This recognition will fortify our efforts to shape policies that ensure responsible AI deployment and contribute to the broader societal good.<br/><br/>Furthermore, it is essential to integrate the viewpoints of 21st-century post-foundational scholars in education when addressing matters of democracy and ethical standards. Scholars who draw upon research from the 20th century and earlier, along with ancient philosophies, should reassess and modernize their arguments to align with contemporary advancements and insights.<br/><br/>Thank you for granting me the opportunity to share my insights and address concerns regarding the Executive order on the Safe, secure, and trustworthy development and use of artificial intelligence. I am hopeful that we can effectively tackle the challenges ahead by developing a comprehensive understanding of the associated risks and taking necessary actions.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0027
comment,2024-02-01T22:29:08Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0038,,,R,Comment on FR Doc # 2023-28232,"With the diverse nature of stakeholders, terms and definitions must be developed to help ensure a shared understanding. As part of this, two to three use-case scenarios may be very helpful, as contextual perspectives are relevant, and there may be ambuguity without a couple scenarios. In addition to probability of inference outcomes, it would be prudent to bound with confidence levels. Formally documenting Roles and Responsibilities, as well as Authority is necessary for furthering trustworthiness.  Since data lakes (distributed or federated learning) may be part of the model training, so correctness is necessary. For example, if FP8 is used, how is the exponent and mantissa defined. <br/><br/>There is ambiguity in terms of what defines a life cycle and how might an agorithm/model be reused, so clear definitions on what constitutes a life cycle. A minimal requirement of metadata used to describe the data is necessary. <br/><br/>A minimal requirement of &#39;do no harm&#39; must be included, along with definitions of what &quot;no harm&quot; means. <br/><br/>Since the training data can have a dramatic impact on the operation, reprentative information of what data was used, and this must be accessible for all stakeholders. <br/><br/>Ethically aligned design is necessity, so defining terms for ethical behaviors must be included.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0038
comment,2024-02-01T22:26:42Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0031,Health IT End-Users Alliance,,,Comment on FR Doc # 2023-28232,Attached are comments from the Health IT End-Users (HITEU) Alliance.,"[('HITEU Alliance comment letter on NIST AI RFI 1 31 2024 Final', 'https://downloads.regulations.gov/NIST-2023-0009-0031/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0031
comment,2024-02-01T22:27:04Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0032,Plan International USA,,,Comment on FR Doc # 2023-28232,"See attached file(s)<br/><br/>Plan International is a global development NGO focused on girls&#39; rights. Plan partners with US Government entities including USAID, State and DOL. Plan is not an expert on the workings of AI, but rather on the impact of AI and social media on the lives of girls and women in the United States and globally. The attached research report summarizes the key findings from scientifically representative surveys that Plan conducted in more than two dozen nations regarding the impacts of social media on women and girls. The data and qualitative interviews clearly demonstrate that girls and women are seeing their educational, employment, social and political roles heavily compromised due to the hostile and unregulated environments on social media, which have only been further exacerbated and accelerated by AI. Plan was therefore dismayed to find that the impacts of AI on girls and women were largely ignored in the text of Executive Order 14110. Plan fully supports the addition of a gender lens and of gender analysis to these regulations as they are developed by the NIST for sections 4.1, 4.5 and 11. Plan fully supports the comments by Vice President Harris that underlined the importance of addressing gender-based violence and other human rights abuses through these regulations. <br/><br/>Such consideration is especially relevant to the AI Risk Management Framework, to Biases in Data, Models and AI Lifecycle Practices and to Impacts on Equity. It also must be addressed through regulations to prevent generative AI from producing child sexual abuse material or producing non-consensual intimate imagery of real individuals (to include intimate digital depictions of the body or body parts of an identifiable individual). Most girls and women do not have an army of followers to protect them (as Taylor Swift does) and are relying on the government to provide that security. <br/><br/>As an international NGO, Plan has worked with State and USAID to include the issue of technology-facilitated gender-based violence through social media and AI in their program strategies. Plan has also worked with USUN to successfully include language on the impact of online abuse on adolescent girls in the Agreed Conclusions at the UN Commission on the Status of Women. Plan has also worked with the White House Gender Policy Council on these issues and is a member of the advisory council for the Global Partnership for Action on Online Harassment and Abuse, which the United States and Denmark co-chair. Plan&#39;s work in these forums and the attached research demonstrate that girls and women around the world are facing essentially one global culture of abuse online. With the European Union, UK and Australia already years ahead of the US in regulating these social ills, we believe there is fertile ground for advancing global standards, as mandated by section 11 of the EO. ","[('PlanInternational_Online GBV Brief_Consolidated Findings ', 'https://downloads.regulations.gov/NIST-2023-0009-0032/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0032
comment,2024-02-01T22:27:42Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0034,,,Anonymous,Comment on FR Doc # 2023-28232,"We submit the attached comments in the form of a research literature review, case study analysis, and critique of existing generative AI red-teaming practices.","[('red-teaming for generative AI v2', 'https://downloads.regulations.gov/NIST-2023-0009-0034/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0034
comment,2024-02-01T22:27:19Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0033,IBM,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST AI EO RFC - IBM Comments FINAL', 'https://downloads.regulations.gov/NIST-2023-0009-0033/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0033
comment,2024-02-02T14:07:07Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0063,,,,Comment on FR Doc # 2023-28232,SWGfL response is attached as a file,"[('SWGFL Response to NIST Assignments of the Executive Order Concerning Artificial Intelligence', 'https://downloads.regulations.gov/NIST-2023-0009-0063/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0063
comment,2024-02-02T14:04:15Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0052,,,Jain,Comment on FR Doc # 2023-28232,"Recommendations on Generative AI risk management and AI evaluation<br/>Author: Gautam Jain<br/>linkedin.com/in/gautamjain<br/><br/>Thoughts expressed are original views of the author. Submission is in personal capacity and does not represent the views of the author&rsquo;s employer<br/>Preface<br/>These recommendations are created in response to National Institute of Standards and Technology&rsquo;s (NIST) RFI (Request For Information) as per executive order 14110 Sections 4.1(a)(i)(A) and (C) which directs NIST to establish guidelines and best practices in order to promote consensus industry standards in the development and deployment of safe, secure, and trustworthy AI systems. <br/>Recommendations<br/>Specific and actionable best practices that should be part of AI RMF 2.0 and were missing in AI RMF 1.0 are listed below. To promote a safe, secure and trustworthy AI system in the future, there are 6 unique recommendations namely Disclaimer, Source and Transparency, Human-led testing, Pre-launch readiness, Post-launch feedback and Independent 3rd Party auditor ecosystem. The following section describes each recommendation further:<br/>Disclaimer<br/>Every output from a generative AI based product or feature must have a disclaimer which is specific to the intent of the query or the subject within reasonable human understanding. For e.g. if a human requests for financial investment advice to an AI model, it&rsquo;s output must contain a standard alert message or disclaimer related to the risk associated and that the output is not a legal financial advice and the end user is requested to exercise caution and avail professional financial advice from a registered, professional advisor, tax consultant or accountant - as the case may be. AI developers, deployers and auditors must all adhere to the provision (to be added to AI RMF 2.0) that each response has an associated disclaimer which is relevant within reasonable understanding of the intent of the query.<br/><br/>Source &amp; Traceability<br/>In the AI RMF 1.0, it is mentioned that Accountability and Transparency will lead to Trustworthiness. However, there is no mention of Source and Traceability of the generated content or output. If auditors were to pass an AI model for a public launch and wider-availability beyond safe-environment testing, the AI models must be able to either probabilistically or deterministically share the source(s) in creating the output i.e articles read, authors quoted, web pages crawled, systems used, websites referred, books studies, data analyzed, countries associated. In case multiple generic data sets are used to generate the output, the AI model must be able to trace the top &lsquo;n&rsquo; contributing data sets which had the highest percentage role to play in algorithmic learning in the process of generating output.<br/><br/>Human-led Testing<br/>Create a community of Trusted Testers.<br/>An AI model, to be publicly available in the hands of humans, or affecting their day-to-day lives, must pass rigorous testing standards<br/>AI models or algorithms, train on data sets and have the computational ability to learn and bypass a test, thus NIST or Secretary of Commerce must actively invest in creating a thriving community of human testers representing different races, regions, religions, ages, genders, IQs, EQs etc. respecting aspects of diversity, equity, inclusion and belonging.<br/>Create a repository of basic, intermediate and advanced test cases which can be used by the human testers to get started with an AI-model audit, but do maintain &gt; 51% of the tests to be impromptu, human-created, unguided cases<br/><br/>Pre-launch readiness<br/>Checklist requirements for a public launch of an AI model should include a Safety score. Based on the human-led testing, each AI model must be given a safety score on a scale of 0-100, which should help the end users of the system to understand directionally how safe it might be to use a particular generative AI based solution<br/><br/>Post-launch feedback<br/>NIST must direct AI developers and deployers to continually accept, process, action and inform on each and every piece of feedback received from human interactions. This will help in improving the models based on human expectations of a particular result and maintain a sustainable way to keep AI models in check<br/>The end user must be entitled to receive an update or resolution within reasonable time (SLAs) as is possible for the creators and maintenance personnel of the AI solution <br/><br/>Independent 3P audits<br/>The AI RMF 2.0 should have a certification of the highest standard, issued by NIST or other leading regulators to create a 3rd party auditors ecosystem. The auditors may charge a fee for each evaluation based on the AI RMF 2.0 and render an algorithm as safe or unsafe (pass or fail) to be launched<br/>These auditors should be independent entities, not having any vested interests in the companies they are auditing for a given framework<br/>Public launch of AI solutions which do not have a valid certification must be illegal under cybersecurity laws.","[('NIST Submission to promote safe secure and trustworthy AI systems by Gautam Jain', 'https://downloads.regulations.gov/NIST-2023-0009-0052/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0052
comment,2024-02-02T14:09:37Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0071,Wiley,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Wiley Response to NIST AI Executive Order Committments 02-02-2024', 'https://downloads.regulations.gov/NIST-2023-0009-0071/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0071
comment,2024-02-02T14:03:13Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0050,,,Herrick,Comment on FR Doc # 2023-28232,I believe that generative AI models should be heavily federally regulated to protect human artists and their work from being scraped. It should be illegal to train these models on copyright material.,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0050
comment,2024-02-02T14:09:16Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0070,Chartered Software Developer Association,,,Comment on FR Doc # 2023-28232,"3. Advance Responsible Global Technical Standards for AI Development<br/><br/>Testing and assurance plays a crucial role in a trusted ecosystem. <br/><br/>On-going Maintenance of Generative AI Model is needed.<br/><br/>1. Active supervision is crucial for ensuring that Generative AI model operates as intended, identifying substantial data drift, and assessing the possible decline in model performance.<br/> <br/>2. The model testing should cover the ongoing monitoring/backtesting initiative of Generative AI Model (i.e the prompts and answers by the AI Model)<br/><br/>3. Periodic retraining of AI Model (Adjusting to evolving user behavior and the availability of content)<br/><br/>4. Human Input: Integrate user feedback and preferences into the recommendation system. Enable users to offer explicit feedback on recommendations, leveraging this input to enhance the model.<br/><br/>The comments above emphasise on the on-going maintenance of post-training &amp; launched/live Generative AI Model to ensure the AI model fits the AI governance values. On top of model testing during the model development as well as the annual audit on the launched model, on-going maintenance on best-effort basis (live/monthly/quarterly subject to the tool availability) would act as a preventive measure for early model faulty detection before the damage done is too large.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0070
comment,2024-02-02T14:06:31Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0061,,,Anonymous,Comment on FR Doc # 2023-28232,"Reproducibility<br/><br/>Ensuring the security and safety of AI is difficult or impossible when systems do not yield the same result when provided with the same stimulus a second time. <br/><br/>NIST AI 100-1 mentions reproducibility but it should be a top level concern as should explainability. AI in systems that affect public safety is not to be trusted without reproducibility and explainability. <br/><br/>For example, an autonomous vehicle swerving for no apparent reason while traveling at speed on a freeway, requires both reproducibility and explainability if its behavior is to be understood and corrected by humans. Resolving the issue using another AI system without human understanding simply iterates the problem and may compound it.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0061
comment,2024-02-02T13:57:16Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0042,Lily Innovation Advisors Limited,,,Comment on FR Doc # 2023-28232,"NIST should consider approaching AI risks using well-developed risk management frameworks, such as ISO 31000 and M_o_R. These are mature frameworks that involve assessment of risks (based upon criteria including impact, likelihood, proximity and velocity) and ongoing planning of responsive actions.<br/><br/>Lily Innovation is in early stages of developing a website Saihub.info (launched in December 2023), which applies a modified version of such risk management approaches to AI risks and harms. Saihub has created an evolving AI &ldquo;harms register&rdquo; to record potential harms from AI (both current and projected harms), based on the &ldquo;risk register&rdquo; used in risk management practice. Saihub is also developing an expanding set of analyses of individual harms in the register, providing key materials on each. These analyses focus primarily on threats/harms, but Saihub intends for its approach to be extended to opportunities/benefits of AI.<br/><br/>We plan to significantly expand Saihub, and are currently seeking UK government funding to build a network to do so. The content of Saihub is fully open to the public.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0042
comment,2024-02-02T14:02:59Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0049,,,Anonymous,Comment on FR Doc # 2023-28232,"Transparency of datasets is needed; parties must be allowed to identify their data and ask for licensing payment or destruction of data if used without permission or compensation of said data. Opt-Out is not realistic given the entirety of the internet.<br/><br/>Stronger framework that not put the burden on the owners on said data if their data is found to have been used in the making of genAI products such as Stable Diffusion. Currently, individual creatives are at the biggest disadvantage.<br/>In the case of generative ai fake porn, minors and other marginalized individuals are at the biggest disadvantage here. The onus to file a lawsuit for remedy should not be on the individual who is affected, especially minors.<br/><br/>Protection of creative industries and public privacy and safety should be an imperative.<br/><br/>Recent relevant articles:<br/>https://www.latimes.com/entertainment-arts/business/story/2024-01-30/ai-artificial-intelligence-impact-report-entertainment-industry - not a pure data report but useful to look at overall sentiments and future plans. There is room to avoid this depressing future.<br/>https://x.com/ednewtonrex/status/1750927026666766357?s=20<br/><br/>Ideas: <br/>Immediate data collection of impact of these technologies in industry, labour, job loss/economic loss and overall public safety and well-being from federal agencies.<br/><br/>Remove non profit research to commercial (potential data laundering) loopholes.<br/>An article on this specific issue regarding AI/ML models:<br/>https://waxy.org/2022/09/ai-data-laundering-how-academic-and-nonprofit-researchers-shield-tech-companies-from-accountability/<br/><br/>Inclusion of large, visible watermarks or other technologies that allow the public to know if they are encountering synthetic media, such as the inclusion of synthetic media in political ads, tv shows etc.<br/><br/>Collaboration with the FTC to to remove IP and private data from current commercial data sets and latent spaces. Future regulations will not be able to stand and grow if potentially all existing ai models are in clear violation of copyright and privacy laws.<br/><br/>Using videos, images, audios and texts not covered by a license to exploit for AI training shall be prohibited for those software that allow the upload of media contents to generate an image, a video, a text or an audio, such as image-to-image software.<br/><br/>The distinction between &ldquo;copyrighted material&rdquo; and &ldquo;public domain&quot; is no longer adequate to identify what can and cannot be used for the datasets. Learning datasets contain personal sensitive data, protected by the privacy laws, but not by copyright.Examples of material released can be found when it would not have been possible to foresee its use in a dataset to train an AI model. Any data used in training a model shall be curated and authorized by its legitimate owner and willingly inserted in the dataset by its author with full knowledge of it. AI companies shall produce internally original materials for the training or license external material following terms and contracts previously established with the authors or rightful holders of said material.<br/><br/>Employers should not be able to force employees to use genAI technologies if it&#39;s not outlined in their contract or if there is little difference in an employee&#39;s overall work performance. Work created for employers through Work For Hire should not be automatically allowed to be used for generative model training, even in the future.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0049
comment,2024-02-02T20:07:48Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0075,Friends of the Earth,,,Comment on FR Doc # 2023-28232,"This comment was submitted by Friends of the Earth, on behalf of the Climate Action Against Disinformation coalition. See attached file.","[('Climate Action Against Disinformatio NIST AI Submission Feb 2', 'https://downloads.regulations.gov/NIST-2023-0009-0075/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0075
comment,2024-02-02T20:08:03Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0076,,,Levine,Comment on FR Doc # 2023-28232,See attached file(s),"[('Levine_Doherty_NIST_2023_0009_0001', 'https://downloads.regulations.gov/NIST-2023-0009-0076/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0076
comment,2024-02-03T14:34:10Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0114,CTIA,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('240131 CTIA Comments on NIST AI RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0114/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0114
comment,2024-02-03T14:44:26Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0170,Public Knowledge,,,Comment on FR Doc # 2023-28232,Please see attachment.,"[('PK NIST Dkt 231218-0309', 'https://downloads.regulations.gov/NIST-2023-0009-0170/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0170
comment,2024-02-03T14:41:08Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0155,Partnership on AI,,,Comment on FR Doc # 2023-28232,"Please find attached Partnership on AI&#39;s&nbsp;submission in response to the Request for Information related to NIST&#39;s Assignments under sections 4.1, 4.5 and 11 of the Executive Order on Artificial Intelligence.&nbsp; The submission is 21 pages plus 2 annexes.&nbsp; <br/>Please let us know&nbsp;if you have any questions or concerns.","[('PAI Submission - NIST AI Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0155/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0155
comment,2024-02-03T14:38:03Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0136,PRISMAGuard LLC,,,Comment on FR Doc # 2023-28232,"Responded to your RFI call to certain sections of 4.1, 4.5, and 11 pertaining to <br/>1. Developing Guidelines, Standards, and Best Practices for AI Safety and Security, <br/>2. Reducing the Risk of Synthetic Content, and <br/>3. Advance Responsible Global Technical Standards for AI Development.<br/><br/>See the attachment for details. <br/>Mano Paul","[('EO 14110 - Safe Secure and Trustworthy AI computing - 2023-24283', 'https://downloads.regulations.gov/NIST-2023-0009-0136/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0136
comment,2024-02-03T14:38:27Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0140,Spatial Web Foundation,,,Comment on FR Doc # 2023-28232,"<br/>In response to NIST&#39;s RFI item 1.(2), The Spatial Web Foundation recommends that NIST adopt the IEEE 2874 Spatial Web Standard as guidance for evaluating AI Capabilities which AI could cause harm.","[('Spatial Web Foundation response to NIST RFI 20240202', 'https://downloads.regulations.gov/NIST-2023-0009-0140/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0140
comment,2024-02-03T14:35:21Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0121,Synergist Technologies LLC,,,Comment on FR Doc # 2023-28232,"Synergist Technologies LLC [www.synergist.technology] is glad to submit the attached response to the National<br/>Institute of Standards and Technology&rsquo;s (&ldquo;NIST&rdquo;) Request for Information Related to<br/>NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning<br/>Artificial Intelligence. We seek to be part of the solution of addressing trustworthy and safe AI in the form of an audit software.  This response also includes information around image tool testing, in response to<br/>the Secretary of Commerce Supplementary Information.","[('NIST RFI SYNERGIST', 'https://downloads.regulations.gov/NIST-2023-0009-0121/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0121
comment,2024-02-03T14:47:17Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0180,Queer in AI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST RFI 2023-0009', 'https://downloads.regulations.gov/NIST-2023-0009-0180/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0180
comment,2024-02-03T14:38:10Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0137,America's Credit Unions,,,Comment on FR Doc # 2023-28232,Comment from America&#39;s Credit Unions,"[(""ACU Letter to NIST - Request for InformationRelated to NIST's Assignments Under the Executive Order Concerning Artificial Intelligence_2.2.24"", 'https://downloads.regulations.gov/NIST-2023-0009-0137/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0137
comment,2024-02-03T14:42:06Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0159,RAND,,,Comment on FR Doc # 2023-28232,"RAND&rsquo;s Technology and Security Policy Center (TASP) conducts research on dual-use technologies, such as artificial intelligence (AI), and their relevance to the national security of the United States. As part of this work, TASP has investigated evaluations and red-teaming of AI systems, including conducting evaluations of AI systems&rsquo; ability to enable the execution of biological weapon attacks, and organized a workshop on red-teaming as a method to identify capabilities and risks of AI models. <br/> <br/>RAND is pleased to submit the attached paper to the National Institute of Standards and Technology (NIST)&rsquo;s December 2023 Request for Information Related to NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence. This paper provides insights on the assignments and topics described in the RFI.","[('PE-A3221-1-ANIST RFI PREPUB 2024Feb2-b', 'https://downloads.regulations.gov/NIST-2023-0009-0159/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0159
comment,2024-02-03T14:48:30Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0189,Future of Life Institute,,,Comment on FR Doc # 2023-28232,"Response to Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11)<br/><br/>Organization: Future of Life Institute <br/>Point of Contact: Hamza Tariq Chaudhry, US Policy Specialist. hamza@futureoflife.org <br/><br/>Full RFI Response attached as PDF below. ","[('RFI NIST 202300090001 Future of Life Institute', 'https://downloads.regulations.gov/NIST-2023-0009-0189/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0189
comment,2024-02-03T14:47:51Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0184,,,Kerry,Comment on FR Doc # 2023-28232,"COMMENTS OF CAMERON F. KERRY<br/>ON IMPLEMENTATION OF EXECUTIVE ORDER 14,110<br/><br/>I am submitting these comments response to the Request for Information Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11)(&ldquo;the RFI&rdquo;). I address the questions in the RFI relating to international engagement on standards development and NIST&rsquo;s mandate to lead &ldquo;a coordinated effort with key international partners and with standards development organizations&rdquo; on AI standards.<br/><br/>My comments are based on my work at The Brookings Institution, where I am the Ann R. &amp; Andrew H. Tisch Distinguished Visiting Fellow in Governance Studies, and co-lead the Forum for Cooperation on AI (&ldquo;FCAI&rdquo;).  FCAI is a joint project with the Centre for European Policy studies in which officials from NIST and other U.S. Government agencies have participated and in which standards have been a key subject.  While my comments reflect this work, they are not made on behalf of the Brookings Institution or a project of FCAI.  My comments are my own and they are attached.  <br/>Respectfully submitted,<br/>Cameron F. Kerry<br/>","[('Comments to NIST 02.02.2024', 'https://downloads.regulations.gov/NIST-2023-0009-0184/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0184
comment,2024-02-03T14:35:29Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0122,,,Harvey,Comment on FR Doc # 2023-28232,"Regan made a point of saying &quot;Trust but Verify&quot; in Russian many years ago.  I also recall university professors at that time working on neural nets and other early forms of AI, which have made enormous advances.  We struggle today with the knowledge that software - even without AI - can be harmful and potentially life-threatening.   We have convened multiple time in our effort to define practices and standardized rules to follow for developing &#39;safe&#39; code.<br/><br/>Unfortunately, AI development can be very costly.  We should predict that much like the condition of bridges in the 19th century, there will be people who cutting corners (whether deliberately or through ignorance is not stipulated) in their personal path to completing AI projects.  Perhaps there are corner cases without adequate training data.  As with financial investments, there will be bad actors.  As such, our collective contributions to recommended practices are helpful, but may be of only modest benefit for those are willful, or willfully ignorant.  Let&#39;s remember the role of software in the 737MAX crashes - and Boeing&#39;s stated intent to commercialize without the user-cost of pilot retraining.<br/><br/>For this simple too-human reason, we must have an expectation of a methodical process of both target performance and guardrails for any potentially life-threatening software application.  We must remember that people have already died - in the case of autonomous vehicles (Uber in Arizona).   That is, requirements must be explicit and coders and manager will need to authenticate their work - regardless of conformance to recommended coding methodologies.<br/><br/>Most importantly, there must be a way to audit the process of the development and certification of both low level algos and higher level functions.  Today&#39;s applications have hundreds of individual code blocks - some being open-source or software of unknown provenance.  The audit function must also satisfy release and edit processes supporting near-term and long-term software lifecycle management (See PATCH-Act).  Following a decade of internal development, the FDA has finally iterated toward a methodology of supporting software audits, AI inclusive. (See Software as a Medical Device.)  It&#39;s is defined extensively for the condition of potentially life-threatening applications.   As implemented in industry today it is both tedious and laborious for coders and front line managers to execute to this standard; however, it need not be given the very recent expansion of automated tools to both ensure requirements compliance and track versions, tests, certifications and approvals. (See Ketryx.)<br/><br/>In conclusion, while it&#39;s clear that the audit function is not explicitly part of the current NIST effort, it is likely short-sighted to ignore the need for any useful standard to be auditable by government employees who may not have the time/ability to read code blocks at a detailed level.<br/>To paraphase the nineteenth century riddle, &quot;What is it the less of it there is, the more it is dread?,&quot; if coders built bridges the way they write software, no one would cross them. <br/><br/>Ken Harvey<br/>Retired CEO/Entrepreneur ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0122
comment,2024-02-03T14:24:57Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0085,,,Oliveira,Comment on FR Doc # 2023-28232,Ais should NOT train on copyrighted material that their company don&#39;t own. Stop the scrapping of Internet users data! Take out from their systems what they scrapped without compansation and permission and pay the damaged parts. The AI companies should put out a statement available in the AI interface for users explaining all these bad pratices( scrapping copyrighted material and users data without permission and compansation) they did how it was wrong and ilegal and damaged careers and lifes and how they will correct themselfs from now on. ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0085
comment,2024-02-03T14:25:15Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0086,Anika Systems,,,Comment on FR Doc # 2023-28232,"<br/>Generative AI Risk Management<br/><br/>a. Risks and Harms of Generative AI: The potential for generative AI to cause harm is significant, especially in relation to misinformation, deepfakes, and other forms of synthetic media. The development of a companion resource to the AI RMF for generative AI should prioritize the identification of these risks, with a clear mapping of trustworthiness characteristics, to ensure robust risk mitigation strategies are in place. It is essential to consider both technical and non-technical harms, including the impacts on democracy, privacy, and human rights.<br/><br/>b. Standards and Practices: There is an apparent gap in standardized practices for managing generative AI risks. The AI RMF core functions should be expanded to include specific guidance tailored to generative AI, with a focus on governance structures that incorporate multi-stakeholder perspectives, including those from civil society and affected communities.<br/><br/>AI Red-Teaming<br/><br/>a. Guidelines for Red-Teaming: Red-teaming exercises are essential for identifying and mitigating unforeseen risks in AI systems. Guidelines for red teaming should consider the various stages of the AI lifecycle and provide a structured approach to identifying potential threat models and harmful capabilities.<br/><br/>b. Capabilities and Limitations: Understanding the capabilities and limitations of AI red-teaming is vital for its effectiveness. The guidelines should provide clarity on the scope and scale of red-teaming exercises, ensuring that they are comprehensive enough to reveal significant risks without compromising the security or intellectual property of the AI systems.<br/><br/>c. Best Practices and Information Sharing: Establishing best practices for red teaming is necessary to ensure consistency and effectiveness across different organizations and sectors. These practices should include mechanisms for information sharing that balance the need for collaboration with the protection of sensitive information.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0086
comment,2024-02-03T14:38:43Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0142,,,Jordan,Comment on FR Doc # 2023-28232,We should not be allowing AI to be used without protections for the workers it will disrupt en masse. Right now we need much more restrictive rules about the use of generative AI/LLMs and any other models that have been trained on data that contains content both copywritten and taken from users without their consent<br/><br/>Please lead the way for sensible policy that puts humans first,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0142
comment,2024-02-03T14:41:17Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0156,,,Vyawahare,Comment on FR Doc # 2023-28232,"Fully support information related to AI red-teaming, generative AI risk management, synthetic content labeling &amp; authentication, and advancing responsible global technical standards for AI development.<br/>Name: Prafulla Vyawahare<br/>Organization: Department of Transportation, State of California<br/>Note: Please note that, if enough comments are not received, then please consider extending the date by a week or so. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0156
comment,2024-02-03T14:42:59Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0163,,,Anonymous,Comment on FR Doc # 2023-28232,"Ai companies took all data didn&#39;t pay for it to cretors, didn&#39;t ask pernmissions and trained their models for profit.All this training absolutely unethical violation of copyright law that didn&#39;t give any great to as . This is stilling for profit , and people from whoom it was stilled will lost their jobs. It is skerry for me to think about future, where kids should&#39;t learn how to write and draw becouse there is no meaning on it - it all doing AI. I dont whant live in that kind world, becousse creating art is meaning of my life, without it   would be just nightmare.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0163
comment,2024-02-03T14:49:28Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0198,,,Laur,Comment on FR Doc # 2023-28232,"I&#39;ll keep it brief. Please consider regulating with the public interest in mind. This technology promises many societal improvements, but as it currently stands it seems to be doing more harm than good. It threatens many careers, spreads disinformation, plagiarizes, increases biases, and so on. It&#39;s a way for Big Tech to scrape our culture and sell it back to us as they see fit. I ask that you please allow those being scraped and used as training data to have as much (if not more) of a voice in this as AI engineers, experts, and the companies making and hosting the models. Complete transparency is a must. These companies should not receive any exemptions from current laws because their tech &quot;is just too complicated.&quot; There should be consent, compensation, and transparency on the data used to train these, and thorough explanations on their outputs to hold accountability for those releasing models. I also believe watermarking should be a top priority for generative AI. Both to combat disinformation and improve consumer protections. So a quick recap - transparency, accountability, and watermarking/identifying of all generative AI.  Thank you for allowing these comments. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0198
comment,2024-02-03T14:48:36Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0190,,,Meister,Comment on FR Doc # 2023-28232,"Generative AI is a predatory resource. It requires harvesting information in order to make the software work. It essentially takes an amount of work done by an artist, reads its patterns and reproduces it creating a derivative image. That is already proven even by the fact that the companies responsible for the tool are asking for an exception when it comes to using copyrighted material. That much is easy to understand and I imagine everyone is talking about this here. A lot of the workforce however is being replaced by the tool and the dark thing is that big companies are firing their workforce in exchange for the benefit of using such tool that draws information from the very people they fired.<br/><br/>It&rsquo;s also important to note that generative ai is not only an ethical problem in the artistic field. It&rsquo;s not a problem with the tool simply but an ethical issue related to how it can be used. Like guns and drones need regulation, so does generative ai. The tool can and will inevitably by logic be used for scams and political attacks using resemblance of voice and appearance of people relevant in the media, opinion formation, fake news and scams since the vast investment by large companies like google will inevitably bring an unstoppable polishing to an already powerful tool. <br/><br/>It has to be regulated because it affects jobs in a multibillion dollar industry in a large scale and it affects people in such ways that can rattle the cage of society worldwide. Many countries are already starting to take stance but it seems it is not a profound reaction. Such reaction is only expected when things go bad and irreversible. Before that happens: Regulate, make it ethical. I am a professional artist. Like me many pay their bills and feed their kids with this income. It is not a small number or a matter of adaptation. Some of us have 20, 30, 40 years in this business. <br/><br/>Regulate. Make it ethical",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0190
comment,2024-02-03T14:48:24Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0188,,,Pepin,Comment on FR Doc # 2023-28232,Ai were trained on stolen/copyrighted work without any conscent from artists across the world.<br/>The way those Ai are trained was unetical practices that should never be allowed ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0188
comment,2024-02-03T14:45:53Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0177,,,Thresher,Comment on FR Doc # 2023-28232,"The way that generative AI is recklessly developed and deployed currently is absolutely unacceptable. Several generative AI models have been trained on material indiscriminately scraped from the internet, much of it copyrighted material without the consent of the original authors. As if that was not bad enough, this scraping has led to these AIs being trained on leaked PII, and this PII being redistributed by these generative AIs. As reported by 404 Media, these generative AIs have also been trained on CSAM (https://www.404media.co/laion-datasets-removed-stanford-csam-child-abuse/), leading to this CSAM also being redistributed. This indiscriminate scraping of data needs to stop immediately, as it greatly harms those stolen from as well as well as pretty much every citizen who uses the internet. AI models must be opt-in only, if copyrighted material is to be used for training, then explicit permission from the original author(s) must be given first, and personal data must not be used either. Government regulation should ensure the datasets used to train are completely open, and there needs to be strong oversight during the training process. The way that generative AI in its current form is used by the public is a serious problem as well. Generative AI is currently being used to annihilate jobs rather than help create any. Social media has already been flooded by synthetic images, forcing artists to compete with their own stolen labor. Because these generative AIs were designed to imitate human made work as much as possible, it is often difficult to tell apart an AI generated image from real human art, leading to fraud. Worse, generative AI is capable of mimicking a given artist&#39;s style, as it was trained on their works without permission. In this report commissioned by the CAA and others (https://animationguild.org/wp-content/uploads/2024/01/Future-Unscripted-The-Impact-of-Generative-Artificial-Intelligence-on-Entertainment-Industry-Jobs-pages-1.pdf), around 204,000 jobs are estimated to be disrupted by generative AI in the next 3 years, and that is just in the entertainment industry alone. Deepfakes are also a serious concern, just like how synthetic images can be mistaken for actual art, they can also be mistaken for real photos. Generative voice AI is a serious threat as it can be used to fool people into thinking someone actually said something they did not, it has already been used in numerous scams. Generative AI enables misinformation to spread at an incredible rate never seen before. As all of these current harms reveal, generative AIs must be developed carefully, and released carefully, if ever. Strong government regulation needs to be put in place that ensures training data is open and non-infringing. Intellectual property rights and personal data must be protected from generative AI. All current infringing generative AIs, including but not limited to Stable Diffusion, Midjourney, ChatGPT, DALL-E, Bard, CoPilot, ElevenLabs, and Llama must be purged and made illegal to host/distribute.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0177
comment,2024-02-03T14:30:34Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0106,,,Crossan,Comment on FR Doc # 2023-28232,"Image generating technology urgently needs strong regulations on generating likeness, sexual content, and the use of copyrighted materials. There was never permission granted by the owners of this data. Web scraping to gather works for the creation of a product for commercial use without the express permission of the copyright holders is simply a way to skirt well established laws and diminish competition in every industry it is attempted.<br/><br/>Rights-holders must be able to check if their work has been trained.<br/>Copyright law must be enforced.<br/>Congress needs to pass anti-nonconsensual porn bill which would allow lawsuits against companies who create these data sets and those who created or intended to spread non consensual porn.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0106
comment,2024-02-03T14:33:08Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0113,,,Anonymous,Comment on FR Doc # 2023-28232,"Generative AI has been threatening a lot of artists livelihoods by using their own work to create unjust competition.<br/><br/>Some parts of the market have been almost completely eliminated, such as some corporate jobs and freelancer commission categories.<br/><br/>We artists would like AI to be developed ethically, using databases that are either commissioned or from the public domain. We also want to force generative image AI to back-pedal; they have purposefully accelerated the tools development just to say it&#39;s too late&nbsp;to&nbsp;go&nbsp;back&nbsp;now.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0113
comment,2024-02-03T14:49:35Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0199,OpenAI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST AI Executive Order - OpenAI response', 'https://downloads.regulations.gov/NIST-2023-0009-0199/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0199
comment,2024-02-03T14:42:18Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0160,Access Now,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Access Now Submission to NIST RFI PDF', 'https://downloads.regulations.gov/NIST-2023-0009-0160/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0160
comment,2024-02-03T14:48:50Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0192,Center for AI Policy,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('CAIP_NIST_RFI_Response', 'https://downloads.regulations.gov/NIST-2023-0009-0192/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0192
comment,2024-02-03T14:36:27Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0128,,,Gonzalez,Comment on FR Doc # 2023-28232,"Please see the attached research study from the University of British Columbia concerning the anthropocentric bias against Ai generative artworks. In my opinion, the findings of this study help to absolutely show how irrational bias and an unnecessarily fearful mindset to technology is fueling much of the negative criticism against Ai generated artwork.","[('Ai art research bias', 'https://downloads.regulations.gov/NIST-2023-0009-0128/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0128
comment,2024-02-03T14:46:05Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0179,,,Ortiz,Comment on FR Doc # 2023-28232,Hello. Attached to this submission is a document which could be seen as a longer form comment. Thank  you all for  your time<br/>-Karla Ortiz,"[('NIST AI EXECUTIVE ORDER_Karla Ortiz', 'https://downloads.regulations.gov/NIST-2023-0009-0179/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0179
comment,2024-02-03T14:39:43Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0147,Adobe Inc,,,Comment on FR Doc # 2023-28232,"Good day, please find the attached comments submitted on behalf of Adobe Inc.","[('Adobe NIST RFI Response_FInal', 'https://downloads.regulations.gov/NIST-2023-0009-0147/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0147
comment,2024-02-03T14:38:15Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0138,Johns Hopkins Center for Health Security,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Johns Hopkins Center for Health Security Response to RFI on NIST AI Executive Order 2 Feb 24', 'https://downloads.regulations.gov/NIST-2023-0009-0138/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0138
comment,2024-02-03T14:37:53Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0135,University of Colorado Boulder,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('CU Boulder Response NIST RFI AI Executive Order 2-2-24', 'https://downloads.regulations.gov/NIST-2023-0009-0135/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0135
comment,2024-02-03T14:28:52Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0098,Cleveland Clinic,,,Comment on FR Doc # 2023-28232,Please see the attached letter. ,"[('2_02_2024 NIST AI RFI_Cleveland Clinic', 'https://downloads.regulations.gov/NIST-2023-0009-0098/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0098
comment,2024-02-03T14:45:21Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0175,BABL AI Inc.,,,Comment on FR Doc # 2023-28232,Please find attached comments submitted on behalf of BABL AI. ,"[('NIST RFI Response - BABL AI', 'https://downloads.regulations.gov/NIST-2023-0009-0175/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0175
comment,2024-02-03T14:22:32Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0078,Truepic,,,Comment on FR Doc # 2023-28232,"Please find Truepic&#39;s attached submission to the Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence. <br/><br/>Thank you for your consideration. <br/><br/>Mounir Ibrahim <br/>EVP, Public Affairs &amp; Impact<br/>Truepic <br/><br/>","[('2-2 Signed Truepic NIST RFI response (1)', 'https://downloads.regulations.gov/NIST-2023-0009-0078/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0078
comment,2024-02-03T14:50:02Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0201,Panorama Global,,,Comment on FR Doc # 2023-28232,"Dear NIST:<br/><br/>You will find here a submitted comment that is from The Reclaim Coalition focused on AI generated sexual violence harms - aka nonconsensual &quot;deepfake&quot; pornography. This multi-stakeholder response is written in part by survivors.<br/><br/>You may reach us at:<br/><br/>Andrea Powell, Director<br/>The Reclaim Coalition<br/>Panorama Global<br/>Andrea.powell@panoramaglobal.org<br/><br/>","[(""Written Comment_ Request for Information (RFI) Related to NIST's Assignments Under Sections 4-2"", 'https://downloads.regulations.gov/NIST-2023-0009-0201/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0201
comment,2024-02-03T14:48:06Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0186,Cranium AI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST_RFI_20240202_CraniumAI', 'https://downloads.regulations.gov/NIST-2023-0009-0186/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0186
comment,2024-02-03T14:40:59Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0154,,,Doshi-Velez,Comment on FR Doc # 2023-28232,Comment of Finale Doshi-Velez and Elena L. Glassman,"[('FDV EG NIST Comment FINAL FOR SUBMISSION Feb 2 2024 w EXHIBIT', 'https://downloads.regulations.gov/NIST-2023-0009-0154/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0154
comment,2024-02-03T14:41:24Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0157,Digimarc Corporation,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST RFI Digimarc (02-02-2024) - filed', 'https://downloads.regulations.gov/NIST-2023-0009-0157/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0157
comment,2024-02-03T14:44:09Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0169,,,Clymer,Comment on FR Doc # 2023-28232,"The attached documents investigate how developers might argue that AI systems are unlikely to pose catastrophic risks (e.g. CBRN risks). This question directly informs the design of specific standards.<br/><br/>In particular, we describe the following:<br/>- How developers could make a case that AI systems are incapable of causing a catastrophe (either autonomously or due to misuse) and what assumptions are involved in these arguments.<br/>- How developers could make a case that AI systems do not have the propensity to cause a catastrophe (i.e. that they are &#39;aligned&#39;)<br/><br/>The first document is within the 25 page limit and summarizes the key assumptions of these arguments.<br/><br/>The second document includes substantially more detail and examples and is provided as a reference.","[('Safety_arguments (1)', 'https://downloads.regulations.gov/NIST-2023-0009-0169/attachment_1.pdf'), ('RFI - cases for extreme AI risks', 'https://downloads.regulations.gov/NIST-2023-0009-0169/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0169
comment,2024-02-03T14:35:51Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0124,Data & Society Research Institute,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('RFI Related to NIST AI Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0124/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0124
comment,2024-02-03T14:43:07Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0164,National Nurses United,,,Comment on FR Doc # 2023-28232,"On behalf of National Nurses United, please see the attachment.","[('2024 NNU RFI Response_ NIST-2023-0009 AI Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0164/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0164
comment,2024-02-03T14:48:13Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0187,,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('2024-02-02 Comment response letter on NIST RFI re AI FINAL', 'https://downloads.regulations.gov/NIST-2023-0009-0187/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0187
comment,2024-02-03T14:30:06Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0104,Electronic Privacy Information Center (EPIC),,,Comment on FR Doc # 2023-28232,"Please find attached public comments filed by the Electronic Privacy Information Center (EPIC) in response to NIST&#39;s RFI related to its assignments under Sections 4.1, 4.5 and 11 of Executive Order 14110.","[('EPIC Comment on NIST AI Executive Order Mandates RFI 02.02.24', 'https://downloads.regulations.gov/NIST-2023-0009-0104/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0104
comment,2024-02-03T14:49:16Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0197,,,,Comment on FR Doc # 2023-28232,"Please find an enclosed response from the University of California, Davis Office of Research.","[('NIST AI RFI Response - UC Davis', 'https://downloads.regulations.gov/NIST-2023-0009-0197/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0197
