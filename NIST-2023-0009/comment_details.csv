commentOrDocument,modifyDate,docketId,commentOnDocumentId,id,organization,firstName,lastName,title,comment,attachments,link
document,2024-02-03T02:00:32Z,NIST-2023-0009,,NIST-2023-0009-0001,,,,"Request for Information (RFI) Related to NIST’s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11)",,"[('Request for Information (RFI) Related to NIST’s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11)', 'https://downloads.regulations.gov/NIST-2023-0009-0001/content.pdf'), ('2023-28232', 'https://downloads.regulations.gov/NIST-2023-0009-0001/attachment_1.pdf')]",https://api.regulations.gov/v4/documents/NIST-2023-0009-0001
comment,2024-02-01T22:28:21Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0035,,,Minerva,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST AI Executive Order', 'https://downloads.regulations.gov/NIST-2023-0009-0035/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0035
comment,2024-02-01T22:29:34Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0040,"Duality Technologies, Inc.",,,Comment on FR Doc # 2023-28232,"Attached please find comments from Duality Technologies, a provider of privacy technologies used to secure AI and enable AI collaborations. Thank you very much. ","[('Duality Technologies Response to Request for Information vSUBMITTED', 'https://downloads.regulations.gov/NIST-2023-0009-0040/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0040
comment,2024-02-01T22:30:12Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0041,,,Scheller,Comment on FR Doc # 2023-28232,See attached file(s),"[('Final Draft MAIC NIST AI EXECUTIVE ORDER', 'https://downloads.regulations.gov/NIST-2023-0009-0041/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0041
comment,2024-02-01T22:28:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0036,Coalition for Content Provenance and Authenticity (C2PA),,,Comment on FR Doc # 2023-28232,"Dear Director Locascio, <br/><br/>My name is Mounir Ibrahim and I am submitting comments to the Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence on behalf of the Coalition for Content Provenance and Authenticity (C2PA). <br/><br/>Please see the attached letter signed by the C2PA Chairman, Andrew Jenks. <br/><br/>Do let us know if any problems opening the document or if there are any questions. <br/><br/>Thank you for your consideration. <br/><br/>","[('Submission - C2PA NIST AI RFI Signed', 'https://downloads.regulations.gov/NIST-2023-0009-0036/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0036
comment,2024-02-01T17:13:05Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0024,StakeOut.AI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('StakeOut.AI NIST Comment_1Feb2024', 'https://downloads.regulations.gov/NIST-2023-0009-0024/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0024
comment,2024-02-01T17:03:36Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0007,"National Federation of Independent Business, Inc.",,,Comment on FR Doc # 2023-28232,"NFIB (National Federation of Independent Business) comment letter of January 10, 2024, in response to Department of Commerce/NIST Notice Titled &quot;Request for Information (RFI) Related to NIST&#39;s Assignments Under Sections 4.1, 4.5, and 11 of Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11),&quot; Docket No. 231218-0309, NIST-2023-0309, 88 Fed. Reg. 88368 (December 21,2023), is attached.","[('NFIBcommentlettertoDeptofCommerceNISTonArtificialIntellligenceJanuary10of2024', 'https://downloads.regulations.gov/NIST-2023-0009-0007/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0007
comment,2024-02-01T17:05:23Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0012,,,Delaney,Comment on FR Doc # 2023-28232,"Hello, I&#39;ve attached my response to the RFI. I&#39;m writing as an individual outside the industry. My educational background is in mechanical engineering. Thanks, Joseph","[('NIST RFI Response', 'https://downloads.regulations.gov/NIST-2023-0009-0012/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0012
comment,2024-02-01T17:12:29Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0023,,,,Comment on FR Doc # 2023-28232,"The City of New York (&ldquo;City&rdquo;), through its Office of Technology and Innovation (&ldquo;OTI&rdquo;), submits the attached response to the National Institute of Standards and Technology&rsquo;s (&ldquo;NIST&rdquo;) Request for Information Related to NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence.","[('OTI Response NIST RFC FINAL', 'https://downloads.regulations.gov/NIST-2023-0009-0023/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0023
comment,2024-02-01T17:01:10Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0004,,,Swope,Comment on FR Doc # 2023-28232,"This submission presents a focused strategy for aligning with Executive Order (EO) 14110, enhancing Governance, Risk, and Compliance (GRC) by leveraging NIST&#39;s Artificial Intelligence (AI) Risk Management Framework (RMF). The response focuses on concrete, practical enhancements in AI utilization, specifically targeting systemic issues like organizational tribalism, departmental silos, and the management of Generative AI (GenAI) and Large Language Models (LLMs). The strategy boosts performance, leverages AI advancements for growth, and improves operational efficiency. Central to this plan is extensive AI practice enhancements and the introduction of innovative GRC tools, including a specialized Microsoft Excel Tool that can be scaled across public and private GRC AI Executive decision-making systems.","[('Attachment1_SWOPE EO 14110 NIST RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0004/attachment_1.pdf'), ('Attachment2_SWOPE EO 14110 NIST RFI SUPP EXCEL TOOL', 'https://downloads.regulations.gov/NIST-2023-0009-0004/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0004
comment,2024-02-01T17:08:44Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0017,Swear,,,Comment on FR Doc # 2023-28232,"To the National Institute of Standards and Technology (NIST),<br/><br/>Thank you for the opportunity to respond to the Request for Information (RFI) concerning the management of risks and harms of generative AI, as outlined in Sections 4.1, 4.5, and 11 of the Executive Order on Artificial Intelligence. We at Swear would like to provide insights on the roles different AI actors can play in this landscape and discuss the current techniques for content authentication, provenance tracking, and synthetic content detection.<br/><br/>Roles of AI Actors in Managing Risks and Harms of Generative AI:<br/>-AI Developers: The primary responsibility of AI developers is to incorporate ethical considerations and risk mitigation strategies during the development phase of AI technologies. This includes embedding mechanisms for content authentication and traceability within the AI systems, especially those capable of generating synthetic content.<br/>-AI Deployers: Deployers should ensure that AI systems are used within ethical boundaries and comply with existing regulations. They are responsible for implementing content labeling and detection mechanisms to distinguish between authentic and synthetic content.<br/>-End Users: The end user&rsquo;s role involves being vigilant and informed about the potential risks associated with AI-generated content. They should be equipped with tools to verify content authenticity and be aware of the implications of sharing unverified information.<br/><br/>Current Techniques for Content Authentication and Provenance Tracking:<br/>-Swear&#39;s approach to content authentication and provenance tracking involves real-time digital watermarking and cryptographic signatures. This method ensures that any piece of digital content is authenticated at its point of creation and its journey is traceable throughout its lifecycle. (Please see Attachment 1_Swear Explainer.pdf)<br/>-The feasibility of this approach lies in its seamless integration into existing content creation and sharing platforms, requiring minimal behavioral change from end users.<br/>-Regarding validity and fitness for purpose, real-time watermarking and cryptographic techniques offer a robust solution against sophisticated AI-generated forgeries, ensuring the integrity of digital media.<br/><br/>Techniques for Synthetic Content Labeling and Detection:<br/>-Current techniques involve AI-driven algorithms that analyze visual and audio content for signs of manipulation. However, these techniques often lag behind the rapidly advancing synthetic content creation technologies.<br/>-Swear proposes a preventive approach where content is authenticated at the source, thus reducing the reliance on post-hoc detection methods. This method is more efficient in preventing the spread of synthetic content.<br/><br/>Reducing the Risk of Synthetic Content:<br/>-The key to reducing the risk lies in a proactive strategy where content is authenticated at the source rather than relying solely on detection after the content has been disseminated.<br/>-This approach involves a trade-off: while it is highly effective in preventing the spread of deepfakes, it requires direct consumer opt-in or integration into content creation tools and platforms.<br/><br/>Opposing Views:<br/>-Some may argue that real-time authentication could potentially infringe on privacy or creative freedom. However, Swear&#39;s method prioritizes user privacy and data security, ensuring that the content remains in the user&#39;s control.<br/>-Another opposing view is the reliance on AI for detection of synthetic content. While AI detection is a valuable tool, it should be complemented with proactive measures to ensure a comprehensive approach to content integrity.<br/><br/>Swear&#39;s approach to managing risks associated with generative AI involves a combination of real-time content authentication, ethical AI development practices, and informed use by end users. This approach not only addresses current challenges but is scalable and adaptable to future advancements in AI technology.","[('Attachment 1_Swear Explainer', 'https://downloads.regulations.gov/NIST-2023-0009-0017/attachment_1.pdf'), ('Attachment 2_Swear Workflow', 'https://downloads.regulations.gov/NIST-2023-0009-0017/attachment_2.pdf'), ('Attachment 3_Swear Screens', 'https://downloads.regulations.gov/NIST-2023-0009-0017/attachment_3.png')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0017
comment,2024-02-01T17:05:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0013,,,Anonymous,Comment on FR Doc # 2023-28232,"Submitted Comment:<br/>My response relates to responsible AI risk management for accounting and finance industry. For responsible AI risk management, the implementation of specific control activities is essential to ensure that AI systems are developed and used in a safe, ethical, and compliant manner. Here&#39;s a list of recommended control activities tailored for managing risks associated with Artificial Intelligence:<br/>1. Ethical and Legal Compliance Reviews:<br/>   - Conduct regular reviews to ensure AI systems comply with ethical standards and legal regulations.<br/>   - Implement control mechanisms to monitor changes in laws and ethical guidelines relevant to AI.<br/>2. Data Quality and Privacy Controls:<br/>   - Establish protocols for data accuracy, completeness, and relevance.<br/>   - Implement robust data privacy controls, including data anonymization and encryption<br/>3. Bias Detection and Mitigation:<br/>   - Regularly test AI models for biases (gender, racial, or otherwise).<br/>   - Implement procedures to mitigate and correct detected biases<br/>4. Model Validation and Verification:<br/>   - Conduct thorough testing and validation of AI models for reliability and accuracy.<br/>   - Implement version control and approval processes for model updates.<br/>5. Security Measures for AI Systems:<br/>   - Implement cybersecurity measures to protect AI systems from unauthorized access and tampering.<br/>   - Conduct regular security audits and vulnerability assessments.<br/>6. Transparency and Explainability Measures:<br/>   - Ensure AI decisions and processes are transparent and understandable to relevant stakeholders.<br/>   - Implement control activities like logging and reporting mechanisms to track AI decision-making processes.<br/>7. Human Oversight and Intervention:<br/>   - Establish processes for human oversight of AI decision-making.<br/>   - Develop mechanisms for manual intervention and override of AI decisions where necessary.<br/><br/>","[('Submission Comment', 'https://downloads.regulations.gov/NIST-2023-0009-0013/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0013
comment,2024-02-01T17:00:01Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0002,Robotic Technology Inc.,,,Comment on FR Doc # 2023-28232,"Robotic Technology Inc. (RTI) has been involved with autonomous AI/robot systems since the previous millennium, including performing many projects for NIST. Here we are submitting a White Paper that suggests and defines metrics for trusted autonomy. ","[('White Paper Metrics For Trusted Autonomy 3 Jan 24', 'https://downloads.regulations.gov/NIST-2023-0009-0002/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0002
comment,2024-02-01T17:01:28Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0005,OutSecure Inc,,,Comment on FR Doc # 2023-28232,The current state of AI Cybersecurity awareness and preparedness is inadequate and poses a high risk to society at large.  <br/>We require a Cybersecurity Framework focused on AI specifically.  <br/>I have been defining strategic risk based Cybersecurity &amp; Privacy programs at large Fortune 50 companies for last 25 years so know first hand that security community and leaders at most organization&#39;s have less time for complex strategy and proactive approach as is required to solve for this gap effectively.<br/>I also gave this view as a comment at a virtual NIST AI hearing.<br/>,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0005
comment,2024-02-02T13:58:04Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0044,Centre for Information Policy Leadership (CIPL),,,Comment on FR Doc # 2023-28232,"Dear Sir or Madam,<br/>Thank you for the opportunity to comment on NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Sections 4.1, 4.5, and 11). Please find attached comments from the Centre for Information Policy Leadership (CIPL). ","[('CIPL - Response to NIST-2023-0309', 'https://downloads.regulations.gov/NIST-2023-0009-0044/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0044
comment,2024-02-02T13:58:43Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0045,"Salesforce, Inc.",,,Comment on FR Doc # 2023-28232,"See attached file(s) for Salesforce, Inc. comments","[('Salesforce Submission - NIST RFI 2.2.24 ', 'https://downloads.regulations.gov/NIST-2023-0009-0045/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0045
comment,2024-02-02T13:57:35Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0043,Kaiser Permanente,,,Comment on FR Doc # 2023-28232,"ATTN: AI E.O. RFI Comments<br/><br/>Submitted electronically to: www.regulations.gov<br/><br/>RE: Request for Information (RFI) Related to NIST&rsquo;s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence [NIST&ndash;2023&ndash;0009&ndash;0001]<br/><br/>Kaiser Permanente offers the following comments on the above-captioned RFI.","[('KP Comments NIST AI 20240202', 'https://downloads.regulations.gov/NIST-2023-0009-0043/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0043
comment,2024-02-02T14:08:33Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0067,,,Balebako,Comment on FR Doc # 2023-28232,Please find in the uploaded file my comments regarding AI red-teaming.,"[('NIST Red Team comments from Balebako', 'https://downloads.regulations.gov/NIST-2023-0009-0067/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0067
comment,2024-02-02T14:05:29Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0056,,,Broyde,Comment on FR Doc # 2023-28232,"Generative Ai is predominantly built on copyrighted material. It cannot be allowed to be legally used in its current state while it&#39;s very existence requires it to learn from stolen data.<br/>As far as its use in a commercial sense, I wish for them to only be allowed use legally obtained data.<br/><br/>Beyond that it can be and has been used to create a significant amount of misinformation. It can be used to create misinformation that can ruin a massive amount of lives:<br/>-Mia Janine, a 14 year old girl whom committed suicide because AI generated nudes of her were spread<br/>-Ai generated clip of an official claiming they plan to sabotage the election<br/><br/>The reality of GAi is that it can speed up some jobs, while it&#39;s downsides can lead to a literal destruction of democracy through falsified information and propaganda.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0056
comment,2024-02-02T14:01:19Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0047,,,Billy,Comment on FR Doc # 2023-28232,"I am an individual commenting on the security risk of AI. I think robots can be very beneficial if used properly just like any other items. Basic robots for a few basic duties  are  probably safe. When it comes to programming adaptive personality and upgraded capabilities. I think we need several layers of security features from 3 different generations of human to secure the capabilities installment features. What&#39;s ethical for a 21 year, a 35 year old, and a 50 year old varies. We need panels from all corners of the content, all age groups and all races to work together on the creation, the adaptability, the boundaries, and the use.  There is no do over on this. Some features should require 3 layers of mankind affirmation from 3 generations.  The adaptive capabilities must be highly controlled from the very start. I hope this comment helps.<br/><br/>A Billy<br/>[EMAIL REDACTED]",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0047
comment,2024-02-02T14:03:47Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0051,,,Yago,Comment on FR Doc # 2023-28232,"AI in its current form is inherently harmful to humanity. Datasets have been created unscrupulously, taking data without consent. Datasets contain images of child sex abuse and other horrific and illegal material. AI models have been used to create pornography, which has led to at least one suicide. It is my strong conviction that all current models and datasets be abandoned, and for AI to be pursued in an ethical manner, with strong oversight.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0051
comment,2024-02-02T14:04:49Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0054,,,Anonymous,Comment on FR Doc # 2023-28232,"The most barebones regulation of Generative AI MUST be, for the future of Art and Culture, a complete lack of copyright protections and discouragement of use in general.  The relative savings of foregoing artists will lead to a rise in unemployment and a stark drop of the quality of any media made with it.  This doesn&rsquo;t touch on, however, the creation of Generative AI media directly requires a flagrant disobedience of copyright law as it currently stands.  No Generative AI model has been shown as possible without the theft of hundreds, if not thousands of copyright protected media that was not granted consent of use.  Thus, Generative AI must be seen for what it is, a theft machine built on the backs of art workers that seeks to unemploy and disenfranchise them.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0054
comment,2024-02-02T14:07:42Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0064,,,Deng,Comment on FR Doc # 2023-28232,"I am a student artist who has worked professionally in the illustration and concept art field. Generative AI as it is is unethical in using our labor and harmful towards our industry. <br/><br/>Generative AI is not equitable because it exists solely on the merits of its data, which are our data - our copyrighted images. Without this data, generative AI would not be able to compete with our labor. An ethical model would be a GenAI that only trains off of public domain images. This is well within the possibilities, but this would simply mean that AI companies make less money, and thus they do not do this.<br/><br/>It is extremely harmful to the creative ecosystem for AI companies to be able to take our labor and data for free to build a product that directly competes with us.  <br/><br/>GenAI companies should be required to build their technologies transparently and ethically. As it is, Generative Image AI is a violation of human creative rights, the right to own and protect our intellectual and creative works, and will contribute to a world in which humans are not able to control and profit off of their own work, as it is immediately cannibalized by AI systems to directly compete with itself. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0064
comment,2024-02-02T14:08:49Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0068,,,Campos Costanzo,Comment on FR Doc # 2023-28232,Here only to second the opinion of the commenter who submitted the stakeout.ai document on strict AI regulation.,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0068
comment,2024-02-02T14:05:57Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0058,,,Brorson,Comment on FR Doc # 2023-28232,"Good day<br/><br/>I&#39;d like to please make a comment regarding the regulation of generative AI.<br/><br/>Imagine a world where there are no new creative ideas brought to life through the arts because aspiring creatives cannot protect their work from being taken by AI corporations without consent, credit or reparations. <br/><br/>Protecting intellectual property rights ensure that creatives have the freedom to explore and express their ideas without fear of them being consumed by AI datasets created to replace/ compete with them.<br/><br/>Thank you for allowing me the opportunity to make a comment.<br/><br/>Warm Regards<br/>Dean Brorson<br/>Fine Artist and Illustrator ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0058
comment,2024-02-02T14:08:09Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0066,,,M,Comment on FR Doc # 2023-28232,"For a safe future with AI we need all datasets to be public. Large datasets are hosted in cloud storage, they need to be publicly accessible and searchable without any paywall or logins so that independent verification of the content can be done by anyone at any time.<br/><br/>A standardized system of disclosure is also essential. Most commercial projects rely on underlying larger AI models. These smaller companies must disclose which models and companies they are using in an up front and easy to access way. Not standardizing these disclosures creates a legal minefield for any potential customers.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0066
comment,2024-02-02T14:02:37Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0048,,,Anonymous,Comment on FR Doc # 2023-28232,"Generative AI is a humanitarian rights and public safety issue above all else. The potential for public harm is possibly catastrophic should it be left unsupervised in it&#39;s current state. It (generative AI) has already cost the livelihoods of several person&#39;s who have before now relied on their craft for making a living and are now being replaced by a mechanism that processes results based on the theft of these very same peoples works. Generative AI cannot continue to exist in its current theft based, unregulated form.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0048
comment,2024-02-02T14:06:06Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0059,,,Melley,Comment on FR Doc # 2023-28232,Generative AI should not be permitted to use any works/assets/media/etc. without the creator&#39;s/owner&#39;s explicit consent. <br/><br/>Generative AI should not be able to be copywritten. <br/><br/>The environmental cost of Generative AI should be accounted for when creating legislation or guidelines pertaining to it. ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0059
comment,2024-02-01T17:11:39Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0020,CIfAI,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('CIfAI_Response _to_RFI_Related_to_NISTs_Assignmernts_01312024', 'https://downloads.regulations.gov/NIST-2023-0009-0020/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0020
comment,2024-02-01T17:03:49Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0008,SAE International,,,Comment on FR Doc # 2023-28232,Please find attached the response from SAE International,"[('SAE response to NIST RFI January 2024', 'https://downloads.regulations.gov/NIST-2023-0009-0008/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0008
comment,2024-02-01T17:12:19Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0022,Federation of American Scientists,,,Comment on FR Doc # 2023-28232,See attached file(s).,"[('Public Comment NIST-2023-0009-0001', 'https://downloads.regulations.gov/NIST-2023-0009-0022/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0022
comment,2024-02-01T17:13:38Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0025,,,Byrd,Comment on FR Doc # 2023-28232,See attached file(s),"[('Making Artificial Intelligence Transparent and Trustworthy_NIST Feb 2024 RFI', 'https://downloads.regulations.gov/NIST-2023-0009-0025/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0025
comment,2024-02-01T17:13:57Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0026,Institute for AI Policy and Strategy,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('2024.02-NIST-2023-0009-0001', 'https://downloads.regulations.gov/NIST-2023-0009-0026/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0026
comment,2024-02-01T17:06:15Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0015,,,Noack,Comment on FR Doc # 2023-28232,I think Bayesian uncertainty quantification has not gotten the attention it deserves in Machine Learning and AI. One can simply not make reliable and safe decisions without realistic uncertainties. It is a mistake to rely on ensemble methods to create distributions. I am happy to explain more if this is of interest.  ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0015
comment,2024-02-01T17:00:34Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0003,,,Anonymous,Comment on FR Doc # 2023-28232,"Developing a companion resource to the AI Risk Management Framework (AI RMF) for generative AI involves addressing several critical areas. Based on the non-exhaustive list of possible topics provided, here are key points and recommendations for each area:<br/><br/>Risks and Harms of Generative AI<br/>Mapping and Managing Trustworthiness: Establish methodologies for assessing trustworthiness in generative AI, including transparency, reliability, and fairness metrics.<br/>Addressing Specific Harms: Develop guidelines to mitigate risks such as repression, interference with democratic processes, gender-based violence, and human rights abuses. This may include strict protocols for content generation and distribution, as well as monitoring mechanisms.<br/>Standards, Norms, and Practices<br/>Existing Standards and Gaps: Evaluate current standards and identify gaps, particularly in governance, risk mapping, measurement, and management specific to generative AI.<br/>Industry Best Practices: Document and promote industry best practices, encouraging standardization across different sectors utilizing generative AI.<br/>Governance Practices<br/>Recommendations for AI Actors: Suggest updates in governance models to include risk assessment procedures specific to generative AI, emphasizing ethical considerations and stakeholder impacts.<br/>Inclusive Governance Models: Encourage inclusive models that incorporate feedback from diverse stakeholders, including marginalized and impacted communities.<br/>Professions, Skills, and Disciplinary Expertise<br/>Interdisciplinary Teams: Advocate for the formation of interdisciplinary teams comprising experts in AI, ethics, law, sociology, and other relevant fields.<br/>Roles and Responsibilities: Define clear roles and responsibilities for various team members in managing and governing generative AI systems.<br/>Roles of Different AI Actors<br/>Developers, Deployers, and End Users: Outline distinct roles and responsibilities for developers, deployers, and end users, ensuring each group contributes to risk management.<br/>Collaborative Frameworks: Encourage collaborative frameworks where different actors work together to identify and mitigate risks.<br/>Techniques and Implementations<br/>Model Validation and Verification: Develop robust model validation and verification protocols, including red-teaming exercises tailored to generative AI.<br/>Impact Assessments: Implement comprehensive human rights and ethical impact assessments as standard practice in generative AI development.<br/>Content Authentication and Provenance Tracking: Promote the development and use of technologies for content authentication, provenance tracking, and synthetic content detection.<br/>Effectiveness Assessment Mechanisms: Establish mechanisms to assess and verify the effectiveness of implemented techniques, ensuring they are scalable and fit for purpose.<br/>Additional Considerations<br/>Continuous Monitoring and Adaptation: Emphasize the importance of ongoing monitoring and adaptation of strategies as generative AI technologies evolve.<br/>Stakeholder Engagement: Foster a culture of continuous engagement with various stakeholders, including policymakers, civil society, and the general public.<br/>Public Awareness and Education: Develop public awareness campaigns and educational programs to inform about the potentials and risks of generative AI.<br/>By addressing these areas comprehensively, the companion resource to the AI RMF for generative AI can provide valuable guidance in managing the unique challenges posed by these technologies, ensuring their development and deployment are aligned with ethical principles and societal values.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0003
comment,2024-02-01T17:05:10Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0011,,,Johnson-Glenberg,Comment on FR Doc # 2023-28232,"Hello,<br/>I am a professor and Cognitive Scientist who has built NLP models in the past. I am excited that the White House is getting serious about governace for AI. I understand what is needed and want to say that I strongly  support watermarking on text and visual content. Also, I support stricter adherence to and compliance with data scraping rules. Training sets for these GenAI models shoud only use content that they are allowed to use. There is a way for companies to adhere to these restrictions,  though it is cumbersome. Additionally, the pubic should have easy to access searchable lists of the training sources (websites, libraries, etc.) that were used in the models&#39; creation. As a female scientist, I am sensitive to the biases in these systems and think there should also be separate commitees dedicated to oversight on how the large companies are trying to combat the biases and prejudices that will occur any system that uses unfiltered web content for its training. I am the co-Chair of the GenAI Committee at Arizona State University - and I would be willing to serve on a government committee dedicated to AI oversight. This is a critical time for setting AI policy in America. Please see contact information below.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0011
comment,2024-02-01T17:11:52Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0021,,,Tiwari,Comment on FR Doc # 2023-28232,"Quantifying compliance with the AI RMF for each of the four identified categories is essential. This will help organizations achieve regulatory compliance, identify loopholes, and predict the state of their current data management using AI models.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0021
comment,2024-02-01T22:28:56Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0037,Chamber of Progress,,,Comment on FR Doc # 2023-28232,Please see the attached comments of the Chamber of Progress on Section 11 of the Executive Order Concerning Artificial Intelligence.,"[('Chamber of Progress comments NIST RFI - EO-AI 2_2', 'https://downloads.regulations.gov/NIST-2023-0009-0037/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0037
comment,2024-02-01T22:29:23Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0039,,,LeFluer,Comment on FR Doc # 2023-28232,I  Support   Everything   On  These  Issues ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0039
comment,2024-02-02T13:59:27Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0046,Confidential Computing Consortium,,,Comment on FR Doc # 2023-28232,"We are submitting comments on behalf of the Confidential Computing Consortium, Linux Foundation. Please review and consider, thank you!","[('NIST20230309commentsCCC', 'https://downloads.regulations.gov/NIST-2023-0009-0046/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0046
comment,2024-02-02T14:09:55Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0072,,,Santana,Comment on FR Doc # 2023-28232,"Generative &quot;A.I&quot;, and I use that term loosely because the current iteration of it is not intelligent by any sort of actual standards, should highly regulated. Generative &quot;a.i.&quot; is little more than auto photomanipulation. It requires images to mash together. Otherwise it can create nothing. <br/>Furthermore, the advent of this technology has only helped deepfakes thrive and has ruined the lives many artists, and innocent &quot;bystanders&quot; with generated images of them in sexually explicit acts (from the famous Taylor swift to the person you don&#39;t know at a local school.) The reins must be pulled on this technology and fast. <br/>Artificial intelligence as it applies to things like finding cancer, searching through data, sequencing dna and other such applications that will actually HELP humanity, society flourish should be the goal. Not stealing jobs from hard working artists to then turn around and use it to harass, intimidate, and otherwise terrorize society with the threat of fake porn. <br/>I don&#39;t know that I covered everything, or that I even said it well enough to get the message across on the kind of threat this now poses, but this finds someone who can see these generative &quot;a.i&quot; models for what they are and will put a stop to such technologies. It is not innovative, its exploitative. It cannot create without input, therefore what it regurgitates is not what the developers say it is. <br/>I hope you will what&#39;s right and shut these applications down. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0072
comment,2024-02-02T14:05:43Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0057,,,Plant,Comment on FR Doc # 2023-28232,"These tools are using data they have no right to.<br/>It is theft.<br/>It will severely impact artists and designers and other workers.<br/>It must be tightly regulated and artists should be well compensated.<br/><br/>There must be total transparency what data they are using.<br/><br/>This has huge implications for the concept of copyright full stop.<br/>If individuals do not have copyright of their creations/writings,<br/>why will people bother creating anything.<br/><br/>Without the concept of ownership, society will break down.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0057
comment,2024-02-02T14:04:27Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0053,,,Bessette,Comment on FR Doc # 2023-28232,"There has been much outcry from artists, photographers, and internet users in general over the fact that most generative AI models were trained using data such as artwork and writing scraped from the internet without the permissions of the original creators, which in some cases would constitute as misuse of copyrighted materials. While some databases allow certain creators to &quot;opt-out&quot; their work from previous and further training, almost all of them, myself included, believe that the more ethical option would be for AI companies to require explicit permission from authors and individuals before their data is used in AI model training, in addition to being able to opt-out if any of their work was used previously.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0053
comment,2024-02-02T14:06:18Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0060,,,Anonymous,Comment on FR Doc # 2023-28232,Ai is theft and the whole purpose is to steal from the people it wishes to replace. The deepfake situation with it is horrendous it is also a cp generator due to having that content in its data sets. You will find the working class is against this destructive thing. And only these Ai companies who are trying to get away with theft are keen for Ai as it would benefit them but the law is coming,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0060
comment,2024-02-02T14:06:41Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0062,,,Montgomery,Comment on FR Doc # 2023-28232,i think that the use of a dataset that includes copyright  data should be forbidden for any commercial use without the licensing of that copyrighted data that was used in the data set. ,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0062
comment,2024-02-02T14:07:56Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0065,,,Dauber,Comment on FR Doc # 2023-28232,"Hi, I think that AI as it is existing in the world right now is not a safe system for our world as it for one has relied on stealing from artists in a way that cannot be undone and the damage cause by this is not estimable.  <br/>And secondly, the effects on the economy are also too dangerous to ignore.  The amount of jobs potentially at risk are beyond the thresh hold that that risk can be taken.  <br/>There as well are the other factors like the child pornographic materials that may be in the system that present it&#39;s own issues.  <br/>That this AI was created in a bad way cannot be taken back and thus as long as it is existing off of stolen works it will be too harmful to continue.  In addition as long as there are such a significant number of jobs at risk, that shake up to the economy is too risky also.  As the welfare of the everyday people is what we as a society ought to be measuring our choices on.  And this is not good for everyday people and thus should have significant things done to protect the people from this technology.<br/>Thank you for reading this.  ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0065
comment,2024-02-02T14:08:58Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0069,,,Steen,Comment on FR Doc # 2023-28232,"I believe it to be absolutely necessary to provide legal restrictions in regards to the usage of images, especially those copyrighted, including, but not limited to, artwork done by individuals. Without proper methods of regulating what used in datasets of generative AI models, there will be a constant issue in regards to employment in the entertainment industry, whereby individuals not only have their work used without compensation, consent, or even knowledge, but will be unable to find proper employment in their respective fields because of it, there by potentially causing a spike in unemployment and dealing a severe blow to the economy.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0069
comment,2024-02-02T14:10:13Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0073,,,Anonymous,Comment on FR Doc # 2023-28232,"Transparency of datasets is very much needed. All parties must be able to and allowed to identify such data with a full disclosure of the data used.<br/><br/>Data must be licensed with consent from the original creator/ license holder. If data is used without permission, then adequate compensation should be necessary, or have the data destroyed. It is unfair for rightsholder data to be used without permission and/ or compensation to gain financial rewards with use commercially.<br/><br/>There should be better regulations and a good framework so that the original owners of said data are not caused any suffering or inconvenienced in any way.<br/><br/>There are incredibly disturbing and problematic uses of AI where laws and regulations need to be made as soon as possible to protect the public. These are not limited to but include the use of AI to create deepfake porn of minors and unconsenting individuals, toxic and harmful spreading of misinformation, using artwork without consent as training data and harming the original artists/ copyright holders.<br/><br/>Better regulations are very much needed as soon as possible.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0073
comment,2024-02-02T14:05:02Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0055,,,Gee,Comment on FR Doc # 2023-28232,"I am an artist working in the video games industry. The advent of generative AI as it currently stands is rife with legal and moral quandaries that absolutely must be addressed on a systemic level. Datasets these models are trained on must be made transparent, parties must be allowed to identify their data, and ask for licensing payment or destruction of data if used without permission or compensation. In addition, a stronger framework should exist to not put the burden on the owners of said data to meticulously comb each AI dataset for their own work. Additionally, this does not even touch upon the ability of generative AI to create images depicting real world individuals (including minors) in morally compromised or pornographic situations that are difficult to discern from reality. In essence, unregulated generative AI is a potentially infinite wellspring of misinformation and harm. <br/><br/>Under the current framework, AI companies are free to scrape the internet for any and all images regardless of copyright or ownership concern and incorporate them into their databases. Without any kind of regulation, I believe generative AI will have a deleterious effect on creative industries, which are responsible for all the media we engage with and enjoy. Beyond its effect on creative fields, the effect it will have on individuals who have any photos of themselves posted on the internet cannot be ignored. In this modern age it is increasingly difficult to not have some kind of digital footprint. I fear a world in which malicious parties can generate and manipulate images to depict any individual in any conceivable situation without recourse or consequence. ",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0055
comment,2024-02-02T20:07:29Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0074,MITRE,,,Comment on FR Doc # 2023-28232,MITRE&#39;s response to the subject RFI is attached.  Please let me know if you have any questions or if we can help you in any other way.,"[('MITRE NIST AI Implementation Response', 'https://downloads.regulations.gov/NIST-2023-0009-0074/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0074
comment,2024-02-01T17:04:46Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0010,,,Stone,Comment on FR Doc # 2023-28232,"Please see the attached file, which is a short position paper that is soon to appear in the Communications of the ACM, and that is relevant to this call.","[('Now_Later_Lasting_Ten_priorities_CACM_forthcoming', 'https://downloads.regulations.gov/NIST-2023-0009-0010/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0010
comment,2024-02-01T17:15:08Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0028,,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST_RFI_AI - CF', 'https://downloads.regulations.gov/NIST-2023-0009-0028/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0028
comment,2024-02-01T17:09:30Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0018,,,Ezell,Comment on FR Doc # 2023-28232,See attached file(s),"[('nist_comment', 'https://downloads.regulations.gov/NIST-2023-0009-0018/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0018
comment,2024-02-01T17:05:56Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0014,IEEE,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('IEEE Response to  NIST RFI on EO AI', 'https://downloads.regulations.gov/NIST-2023-0009-0014/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0014
comment,2024-02-01T17:11:20Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0019,Compsim LLC,,,Comment on FR Doc # 2023-28232,Artificial Intelligence: Who is responsible? Who cares?,"[('Who is responsible', 'https://downloads.regulations.gov/NIST-2023-0009-0019/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0019
comment,2024-02-01T17:04:05Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0009,Compsim LLC,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('General', 'https://downloads.regulations.gov/NIST-2023-0009-0009/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0009
comment,2024-02-01T17:15:21Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0029,BSA | The Software Alliance,,,Comment on FR Doc # 2023-28232,BSA | The Software Alliance submits the attached comments.,"[('BSA Comments on NIST EO RFI 1.31.24 Final', 'https://downloads.regulations.gov/NIST-2023-0009-0029/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0029
comment,2024-02-01T17:17:13Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0030,Control AI,,,Comment on FR Doc # 2023-28232,Please see the attached file.,"[('ControlAI_NIST RFI Response', 'https://downloads.regulations.gov/NIST-2023-0009-0030/attachment_1.pdf'), ('Draft_Deepfake report_Control AI', 'https://downloads.regulations.gov/NIST-2023-0009-0030/attachment_2.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0030
comment,2024-02-01T17:02:09Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0006,,,PALEM,Comment on FR Doc # 2023-28232,"This comment is pertaining to the best practices of AI Safety and Security.<br/><br/>The rapid advancement of AI brings immense potential for improving our lives, from healthcare and education to entertainment and business. However, this progress also demands careful consideration of safety and security. This is where psychologists play a critical role in shaping the future of AI, and their collaboration with AI developers is crucial for developing safe and responsible AI products. Here are some key reasons why:<br/><br/>1. Mitigating Human Biases in AI: AI algorithms are built on data, and human bias can easily creep into this data, leading to discriminatory or unfair outcomes. Psychologists have the expertise to identify these biases and develop methods to mitigate their impact. They can analyze the training data, the algorithms themselves, and the potential biases that users might bring to their interactions with the AI, ensuring fairer and more ethical algorithms.<br/><br/>2. Understanding Human-AI Interaction: Psychologists understand how humans perceive information, make decisions, and interact with technology. This expertise is essential for designing AI systems that are user-friendly, intuitive, and promote trust. They can help developers understand how people will react to different AI behaviors, anticipate potential misunderstandings or frustrations, and design interfaces that foster positive and productive interactions.<br/><br/>3. Addressing the Psychological Impact of AI: As AI becomes more sophisticated and integrated into our lives, it&#39;s crucial to understand its potential psychological impact. Psychologists can study the effects of AI on mental health, social dynamics, and human behavior. This knowledge can inform the development of AI systems that are mindful of these impacts and minimize potential harm, for example, ensuring AI companions are emotionally supportive or avoiding AI systems that exacerbate anxiety or social isolation.<br/><br/>4. Promoting Explainability and Transparency: A major concern with AI is the lack of transparency in its decision-making processes. This can lead to distrust and fear among users. Psychologists can advocate for explainable AI systems that users can understand, allowing for informed interaction and building trust. They can also help developers communicate the capabilities and limitations of AI effectively, managing expectations and preventing misunderstandings.<br/><br/>5. Developing Ethical Guidelines and Regulations: Designing ethical AI requires a thorough understanding of human values and social context. Psychologists can contribute to the development of ethical guidelines and regulations for AI development and use. They can help identify potential ethical pitfalls, guide discussions on acceptable and unacceptable applications of AI, and ensure that AI serves the greater good while respecting human rights and dignity.<br/><br/>Collaboration, not isolation, is key. By working together, psychologists and AI developers can create safer and more beneficial AI products for everyone. Psychologists bring their expertise in human behavior and ethics to the table, while developers provide the technical know-how to build the tools. This collaboration is essential for ensuring that AI continues to advance responsibly and ethically, maximizing its potential while minimizing its risks.<br/><br/>The future of AI is in our hands. By fostering effective collaboration between psychologists and AI developers, we can ensure that AI becomes a force for good, improving our lives while safeguarding our well-being and ethical principles.","[('Cyberpsychology for a Safe AI and Security', 'https://downloads.regulations.gov/NIST-2023-0009-0006/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0006
comment,2024-02-01T17:07:00Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0016,Synapse Partners,,,Comment on FR Doc # 2023-28232,"Black Duck is software that allows you to scan applications and container images, identify all open-source components, and detect any open-source security vulnerabilities, compliance issues, or code-quality risks. We need to develop a similar service for LLMs that will be able to automatically determine security vulnerabilities, connections to copyrighted material that should be used by the LLM, potentially even identify areas where various forms of bias are introduced in the model. Contrary to Black Duck software that can run on and analyze a completed software system, because the knowledge incorporated into LLMs is represented implicitly in the form of network architecture and weights among the nodes, the type of system advocated here will need to run while the LLM is being trained. The approach will impact negatively both the cost of building an LLM as well as the time it takes to create a fully-trained LLM.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0016
comment,2024-02-01T17:14:30Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0027,Nha Viet Institute,,,Comment on FR Doc # 2023-28232,"The concept of Artificial Intelligence currently functions as a linguistic ensnarement for social imagination, transcending its strict association with computer science terminology. Dr. Son Pham (2022), within the pages of his booklet &quot;On Artificial Intelligence: A Poetic Definition,&quot; currently accessible on Amazon and Kindle Prime, delves into the amalgamation of technology and linguistic creativity. In a pivotal section, he presents an alternative definition of AI as either the Automated Intent or the intent of automation. <br/><br/>&quot;Artificial Intelligence, a concept profound,<br/>Born of automation, in intent it&#39;s found.<br/>A synergy of minds, silicon and thought,<br/>In the dance of algorithms, a connection sought.<br/><br/>Automated intent or intent of automation,<br/>The essence distilled, in clear formation.<br/>AI, a creation of human design,<br/>A marvel that transcends the confines of time.<br/><br/>So let us ponder, with minds wide and free,<br/>The fusion of technology and humanity.<br/>In the tapestry of progress, where dreams are sown,<br/>Artificial Intelligence, a definition known.&quot;<br/><br/>My comment is to incorporate, right at the outset of the policy, a clear articulation of the social imagination surrounding the concept of AI through this poetic definition.<br/><br/>Given that AI is fundamentally driven by &quot;intent,&quot; a heightened focus on K-12 education becomes imperative. This educational stage is pivotal for nurturing the next generation of AI scientists across various fields, while also laying the essential groundwork for our democracy and ethical standards.<br/><br/>The establishment of safe, secure, and trustworthy AI systems hinges on the bedrock of democracy and ethical standards. It is therefore paramount to cultivate a collective understanding among policymakers and other stakeholders that AI fundamentally operates as an &quot;automated intent.&quot; This recognition will fortify our efforts to shape policies that ensure responsible AI deployment and contribute to the broader societal good.<br/><br/>Furthermore, it is essential to integrate the viewpoints of 21st-century post-foundational scholars in education when addressing matters of democracy and ethical standards. Scholars who draw upon research from the 20th century and earlier, along with ancient philosophies, should reassess and modernize their arguments to align with contemporary advancements and insights.<br/><br/>Thank you for granting me the opportunity to share my insights and address concerns regarding the Executive order on the Safe, secure, and trustworthy development and use of artificial intelligence. I am hopeful that we can effectively tackle the challenges ahead by developing a comprehensive understanding of the associated risks and taking necessary actions.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0027
comment,2024-02-01T22:29:08Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0038,,,R,Comment on FR Doc # 2023-28232,"With the diverse nature of stakeholders, terms and definitions must be developed to help ensure a shared understanding. As part of this, two to three use-case scenarios may be very helpful, as contextual perspectives are relevant, and there may be ambuguity without a couple scenarios. In addition to probability of inference outcomes, it would be prudent to bound with confidence levels. Formally documenting Roles and Responsibilities, as well as Authority is necessary for furthering trustworthiness.  Since data lakes (distributed or federated learning) may be part of the model training, so correctness is necessary. For example, if FP8 is used, how is the exponent and mantissa defined. <br/><br/>There is ambiguity in terms of what defines a life cycle and how might an agorithm/model be reused, so clear definitions on what constitutes a life cycle. A minimal requirement of metadata used to describe the data is necessary. <br/><br/>A minimal requirement of &#39;do no harm&#39; must be included, along with definitions of what &quot;no harm&quot; means. <br/><br/>Since the training data can have a dramatic impact on the operation, reprentative information of what data was used, and this must be accessible for all stakeholders. <br/><br/>Ethically aligned design is necessity, so defining terms for ethical behaviors must be included.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0038
comment,2024-02-01T22:26:42Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0031,Health IT End-Users Alliance,,,Comment on FR Doc # 2023-28232,Attached are comments from the Health IT End-Users (HITEU) Alliance.,"[('HITEU Alliance comment letter on NIST AI RFI 1 31 2024 Final', 'https://downloads.regulations.gov/NIST-2023-0009-0031/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0031
comment,2024-02-01T22:27:04Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0032,Plan International USA,,,Comment on FR Doc # 2023-28232,"See attached file(s)<br/><br/>Plan International is a global development NGO focused on girls&#39; rights. Plan partners with US Government entities including USAID, State and DOL. Plan is not an expert on the workings of AI, but rather on the impact of AI and social media on the lives of girls and women in the United States and globally. The attached research report summarizes the key findings from scientifically representative surveys that Plan conducted in more than two dozen nations regarding the impacts of social media on women and girls. The data and qualitative interviews clearly demonstrate that girls and women are seeing their educational, employment, social and political roles heavily compromised due to the hostile and unregulated environments on social media, which have only been further exacerbated and accelerated by AI. Plan was therefore dismayed to find that the impacts of AI on girls and women were largely ignored in the text of Executive Order 14110. Plan fully supports the addition of a gender lens and of gender analysis to these regulations as they are developed by the NIST for sections 4.1, 4.5 and 11. Plan fully supports the comments by Vice President Harris that underlined the importance of addressing gender-based violence and other human rights abuses through these regulations. <br/><br/>Such consideration is especially relevant to the AI Risk Management Framework, to Biases in Data, Models and AI Lifecycle Practices and to Impacts on Equity. It also must be addressed through regulations to prevent generative AI from producing child sexual abuse material or producing non-consensual intimate imagery of real individuals (to include intimate digital depictions of the body or body parts of an identifiable individual). Most girls and women do not have an army of followers to protect them (as Taylor Swift does) and are relying on the government to provide that security. <br/><br/>As an international NGO, Plan has worked with State and USAID to include the issue of technology-facilitated gender-based violence through social media and AI in their program strategies. Plan has also worked with USUN to successfully include language on the impact of online abuse on adolescent girls in the Agreed Conclusions at the UN Commission on the Status of Women. Plan has also worked with the White House Gender Policy Council on these issues and is a member of the advisory council for the Global Partnership for Action on Online Harassment and Abuse, which the United States and Denmark co-chair. Plan&#39;s work in these forums and the attached research demonstrate that girls and women around the world are facing essentially one global culture of abuse online. With the European Union, UK and Australia already years ahead of the US in regulating these social ills, we believe there is fertile ground for advancing global standards, as mandated by section 11 of the EO. ","[('PlanInternational_Online GBV Brief_Consolidated Findings ', 'https://downloads.regulations.gov/NIST-2023-0009-0032/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0032
comment,2024-02-01T22:27:42Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0034,,,Anonymous,Comment on FR Doc # 2023-28232,"We submit the attached comments in the form of a research literature review, case study analysis, and critique of existing generative AI red-teaming practices.","[('red-teaming for generative AI v2', 'https://downloads.regulations.gov/NIST-2023-0009-0034/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0034
comment,2024-02-01T22:27:19Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0033,IBM,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('NIST AI EO RFC - IBM Comments FINAL', 'https://downloads.regulations.gov/NIST-2023-0009-0033/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0033
comment,2024-02-02T14:07:07Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0063,,,,Comment on FR Doc # 2023-28232,SWGfL response is attached as a file,"[('SWGFL Response to NIST Assignments of the Executive Order Concerning Artificial Intelligence', 'https://downloads.regulations.gov/NIST-2023-0009-0063/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0063
comment,2024-02-02T14:04:15Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0052,,,Jain,Comment on FR Doc # 2023-28232,"Recommendations on Generative AI risk management and AI evaluation<br/>Author: Gautam Jain<br/>linkedin.com/in/gautamjain<br/><br/>Thoughts expressed are original views of the author. Submission is in personal capacity and does not represent the views of the author&rsquo;s employer<br/>Preface<br/>These recommendations are created in response to National Institute of Standards and Technology&rsquo;s (NIST) RFI (Request For Information) as per executive order 14110 Sections 4.1(a)(i)(A) and (C) which directs NIST to establish guidelines and best practices in order to promote consensus industry standards in the development and deployment of safe, secure, and trustworthy AI systems. <br/>Recommendations<br/>Specific and actionable best practices that should be part of AI RMF 2.0 and were missing in AI RMF 1.0 are listed below. To promote a safe, secure and trustworthy AI system in the future, there are 6 unique recommendations namely Disclaimer, Source and Transparency, Human-led testing, Pre-launch readiness, Post-launch feedback and Independent 3rd Party auditor ecosystem. The following section describes each recommendation further:<br/>Disclaimer<br/>Every output from a generative AI based product or feature must have a disclaimer which is specific to the intent of the query or the subject within reasonable human understanding. For e.g. if a human requests for financial investment advice to an AI model, it&rsquo;s output must contain a standard alert message or disclaimer related to the risk associated and that the output is not a legal financial advice and the end user is requested to exercise caution and avail professional financial advice from a registered, professional advisor, tax consultant or accountant - as the case may be. AI developers, deployers and auditors must all adhere to the provision (to be added to AI RMF 2.0) that each response has an associated disclaimer which is relevant within reasonable understanding of the intent of the query.<br/><br/>Source &amp; Traceability<br/>In the AI RMF 1.0, it is mentioned that Accountability and Transparency will lead to Trustworthiness. However, there is no mention of Source and Traceability of the generated content or output. If auditors were to pass an AI model for a public launch and wider-availability beyond safe-environment testing, the AI models must be able to either probabilistically or deterministically share the source(s) in creating the output i.e articles read, authors quoted, web pages crawled, systems used, websites referred, books studies, data analyzed, countries associated. In case multiple generic data sets are used to generate the output, the AI model must be able to trace the top &lsquo;n&rsquo; contributing data sets which had the highest percentage role to play in algorithmic learning in the process of generating output.<br/><br/>Human-led Testing<br/>Create a community of Trusted Testers.<br/>An AI model, to be publicly available in the hands of humans, or affecting their day-to-day lives, must pass rigorous testing standards<br/>AI models or algorithms, train on data sets and have the computational ability to learn and bypass a test, thus NIST or Secretary of Commerce must actively invest in creating a thriving community of human testers representing different races, regions, religions, ages, genders, IQs, EQs etc. respecting aspects of diversity, equity, inclusion and belonging.<br/>Create a repository of basic, intermediate and advanced test cases which can be used by the human testers to get started with an AI-model audit, but do maintain &gt; 51% of the tests to be impromptu, human-created, unguided cases<br/><br/>Pre-launch readiness<br/>Checklist requirements for a public launch of an AI model should include a Safety score. Based on the human-led testing, each AI model must be given a safety score on a scale of 0-100, which should help the end users of the system to understand directionally how safe it might be to use a particular generative AI based solution<br/><br/>Post-launch feedback<br/>NIST must direct AI developers and deployers to continually accept, process, action and inform on each and every piece of feedback received from human interactions. This will help in improving the models based on human expectations of a particular result and maintain a sustainable way to keep AI models in check<br/>The end user must be entitled to receive an update or resolution within reasonable time (SLAs) as is possible for the creators and maintenance personnel of the AI solution <br/><br/>Independent 3P audits<br/>The AI RMF 2.0 should have a certification of the highest standard, issued by NIST or other leading regulators to create a 3rd party auditors ecosystem. The auditors may charge a fee for each evaluation based on the AI RMF 2.0 and render an algorithm as safe or unsafe (pass or fail) to be launched<br/>These auditors should be independent entities, not having any vested interests in the companies they are auditing for a given framework<br/>Public launch of AI solutions which do not have a valid certification must be illegal under cybersecurity laws.","[('NIST Submission to promote safe secure and trustworthy AI systems by Gautam Jain', 'https://downloads.regulations.gov/NIST-2023-0009-0052/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0052
comment,2024-02-02T14:09:37Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0071,Wiley,,,Comment on FR Doc # 2023-28232,See attached file(s),"[('Wiley Response to NIST AI Executive Order Committments 02-02-2024', 'https://downloads.regulations.gov/NIST-2023-0009-0071/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0071
comment,2024-02-02T14:03:13Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0050,,,Herrick,Comment on FR Doc # 2023-28232,I believe that generative AI models should be heavily federally regulated to protect human artists and their work from being scraped. It should be illegal to train these models on copyright material.,[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0050
comment,2024-02-02T14:09:16Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0070,Chartered Software Developer Association,,,Comment on FR Doc # 2023-28232,"3. Advance Responsible Global Technical Standards for AI Development<br/><br/>Testing and assurance plays a crucial role in a trusted ecosystem. <br/><br/>On-going Maintenance of Generative AI Model is needed.<br/><br/>1. Active supervision is crucial for ensuring that Generative AI model operates as intended, identifying substantial data drift, and assessing the possible decline in model performance.<br/> <br/>2. The model testing should cover the ongoing monitoring/backtesting initiative of Generative AI Model (i.e the prompts and answers by the AI Model)<br/><br/>3. Periodic retraining of AI Model (Adjusting to evolving user behavior and the availability of content)<br/><br/>4. Human Input: Integrate user feedback and preferences into the recommendation system. Enable users to offer explicit feedback on recommendations, leveraging this input to enhance the model.<br/><br/>The comments above emphasise on the on-going maintenance of post-training &amp; launched/live Generative AI Model to ensure the AI model fits the AI governance values. On top of model testing during the model development as well as the annual audit on the launched model, on-going maintenance on best-effort basis (live/monthly/quarterly subject to the tool availability) would act as a preventive measure for early model faulty detection before the damage done is too large.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0070
comment,2024-02-02T14:06:31Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0061,,,Anonymous,Comment on FR Doc # 2023-28232,"Reproducibility<br/><br/>Ensuring the security and safety of AI is difficult or impossible when systems do not yield the same result when provided with the same stimulus a second time. <br/><br/>NIST AI 100-1 mentions reproducibility but it should be a top level concern as should explainability. AI in systems that affect public safety is not to be trusted without reproducibility and explainability. <br/><br/>For example, an autonomous vehicle swerving for no apparent reason while traveling at speed on a freeway, requires both reproducibility and explainability if its behavior is to be understood and corrected by humans. Resolving the issue using another AI system without human understanding simply iterates the problem and may compound it.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0061
comment,2024-02-02T13:57:16Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0042,Lily Innovation Advisors Limited,,,Comment on FR Doc # 2023-28232,"NIST should consider approaching AI risks using well-developed risk management frameworks, such as ISO 31000 and M_o_R. These are mature frameworks that involve assessment of risks (based upon criteria including impact, likelihood, proximity and velocity) and ongoing planning of responsive actions.<br/><br/>Lily Innovation is in early stages of developing a website Saihub.info (launched in December 2023), which applies a modified version of such risk management approaches to AI risks and harms. Saihub has created an evolving AI &ldquo;harms register&rdquo; to record potential harms from AI (both current and projected harms), based on the &ldquo;risk register&rdquo; used in risk management practice. Saihub is also developing an expanding set of analyses of individual harms in the register, providing key materials on each. These analyses focus primarily on threats/harms, but Saihub intends for its approach to be extended to opportunities/benefits of AI.<br/><br/>We plan to significantly expand Saihub, and are currently seeking UK government funding to build a network to do so. The content of Saihub is fully open to the public.",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0042
comment,2024-02-02T14:02:59Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0049,,,Anonymous,Comment on FR Doc # 2023-28232,"Transparency of datasets is needed; parties must be allowed to identify their data and ask for licensing payment or destruction of data if used without permission or compensation of said data. Opt-Out is not realistic given the entirety of the internet.<br/><br/>Stronger framework that not put the burden on the owners on said data if their data is found to have been used in the making of genAI products such as Stable Diffusion. Currently, individual creatives are at the biggest disadvantage.<br/>In the case of generative ai fake porn, minors and other marginalized individuals are at the biggest disadvantage here. The onus to file a lawsuit for remedy should not be on the individual who is affected, especially minors.<br/><br/>Protection of creative industries and public privacy and safety should be an imperative.<br/><br/>Recent relevant articles:<br/>https://www.latimes.com/entertainment-arts/business/story/2024-01-30/ai-artificial-intelligence-impact-report-entertainment-industry - not a pure data report but useful to look at overall sentiments and future plans. There is room to avoid this depressing future.<br/>https://x.com/ednewtonrex/status/1750927026666766357?s=20<br/><br/>Ideas: <br/>Immediate data collection of impact of these technologies in industry, labour, job loss/economic loss and overall public safety and well-being from federal agencies.<br/><br/>Remove non profit research to commercial (potential data laundering) loopholes.<br/>An article on this specific issue regarding AI/ML models:<br/>https://waxy.org/2022/09/ai-data-laundering-how-academic-and-nonprofit-researchers-shield-tech-companies-from-accountability/<br/><br/>Inclusion of large, visible watermarks or other technologies that allow the public to know if they are encountering synthetic media, such as the inclusion of synthetic media in political ads, tv shows etc.<br/><br/>Collaboration with the FTC to to remove IP and private data from current commercial data sets and latent spaces. Future regulations will not be able to stand and grow if potentially all existing ai models are in clear violation of copyright and privacy laws.<br/><br/>Using videos, images, audios and texts not covered by a license to exploit for AI training shall be prohibited for those software that allow the upload of media contents to generate an image, a video, a text or an audio, such as image-to-image software.<br/><br/>The distinction between &ldquo;copyrighted material&rdquo; and &ldquo;public domain&quot; is no longer adequate to identify what can and cannot be used for the datasets. Learning datasets contain personal sensitive data, protected by the privacy laws, but not by copyright.Examples of material released can be found when it would not have been possible to foresee its use in a dataset to train an AI model. Any data used in training a model shall be curated and authorized by its legitimate owner and willingly inserted in the dataset by its author with full knowledge of it. AI companies shall produce internally original materials for the training or license external material following terms and contracts previously established with the authors or rightful holders of said material.<br/><br/>Employers should not be able to force employees to use genAI technologies if it&#39;s not outlined in their contract or if there is little difference in an employee&#39;s overall work performance. Work created for employers through Work For Hire should not be automatically allowed to be used for generative model training, even in the future.<br/>",[],https://api.regulations.gov/v4/comments/NIST-2023-0009-0049
comment,2024-02-02T20:07:48Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0075,Friends of the Earth,,,Comment on FR Doc # 2023-28232,"This comment was submitted by Friends of the Earth, on behalf of the Climate Action Against Disinformation coalition. See attached file.","[('Climate Action Against Disinformatio NIST AI Submission Feb 2', 'https://downloads.regulations.gov/NIST-2023-0009-0075/attachment_1.docx')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0075
comment,2024-02-02T20:08:03Z,NIST-2023-0009,NIST-2023-0009-0001,NIST-2023-0009-0076,,,Levine,Comment on FR Doc # 2023-28232,See attached file(s),"[('Levine_Doherty_NIST_2023_0009_0001', 'https://downloads.regulations.gov/NIST-2023-0009-0076/attachment_1.pdf')]",https://api.regulations.gov/v4/comments/NIST-2023-0009-0076
